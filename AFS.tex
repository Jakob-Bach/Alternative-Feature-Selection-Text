\documentclass{article}

\title{
	Finding Optimal Diverse Feature Sets\texorpdfstring{\\}{ }with Alternative Feature Selection
}
\author{
	Jakob Bach~\orcidlink{0000-0003-0301-2798}\\
	\small Karlsruhe Institute of Technology (KIT), Germany\\
	\small \href{mailto:jakob.bach@kit.edu}{jakob.bach@kit.edu}
}
\date{} % don't display a date

\usepackage[style=numeric, backend=bibtex]{biblatex}
\usepackage[ruled,linesnumbered,vlined]{algorithm2e} % pseudo-code
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % theorems, definitions etc.
\usepackage{booktabs} % nicely formatted tables (with top, mid, and bottom rule)
\usepackage{graphicx} % plots
\usepackage{multirow} % cells spanning multiple rows in tables
\usepackage{orcidlink} % ORCID icon
\usepackage{subcaption} % figures with multiple sub-figures and sub-captions
\usepackage{hyperref} % links and URLs

\addbibresource{references.bib}

\newtheorem{proposition}{Proposition}
\theoremstyle{definition}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

\newcommand{\stirling}[2]{\genfrac\{\}{0pt}{}{#1}{#2}} % used for Stirling numbers of the second kind

\begin{document}

\maketitle

\begin{abstract}
Feature selection is popular for obtaining small, interpretable, yet highly accurate prediction models.
Conventional feature-selection methods typically yield one feature set only, which might not suffice in some scenarios.
For example, users might be interested in finding alternative feature sets with similar prediction quality, offering different explanations of the data.
In this article, we introduce alternative feature selection and formalize it as an optimization problem.
In particular, we define alternatives via constraints and enable users to control the number and dissimilarity of alternatives.
We consider sequential as well as simultaneous search for alternatives.
Next, we discuss how to integrate conventional feature-selection methods as objectives.
In particular, we propose solver-based methods search to tackle the optimization problem.
Further, we analyze the complexity of this optimization problem and proof $\mathcal{NP}$-hardness.
Additionally, we show that a constant-factor approximation exists under certain conditions and propose corresponding heuristic search methods.
Finally, we evaluate alternative feature selection in comprehensive experiments with 30 binary-classification datasets.
We observe that alternative feature sets may indeed have high prediction quality, and we analyze factors influencing this outcome.
\end{abstract}
%
\textbf{Keywords:} feature selection, alternatives, constraints, mixed-integer programming, explainability, interpretability, XAI

\section{Introduction}
\label{sec:afs:introduction}

\paragraph{Motivation}

Feature-selection methods are ubiquitous for a variety of reasons.
By reducing dataset dimensionality, they lower the computational cost and memory requirements of prediction models.
Next, prediction models may generalize better without irrelevant and spurious predictors.
While some model types can implicitly select relevant features, others cannot.
Finally, prediction models may become simpler~\cite{li2017feature}, improving interpretability.

Most conventional feature-selection methods only return one feature set~\cite{borboudakis2021extending}.
These methods optimize a criterion of feature-set quality, e.g., prediction performance.
However, besides the optimal feature set, there might be other, differently composed feature sets with similar quality.
Such alternative feature sets are interesting for users, e.g., to obtain several diverse explanations.
Alternative explanations can provide additional insights into predictions, enable users to develop and test different hypotheses, appeal to different kinds of users, and foster trust in the predictions~\cite{kim2021multi, wang2019designing}.

For example, in a dataset describing physical experiments, feature selection may help to discover relationships between physical quantities.
In particular, highly predictive feature sets indicate which input quantities are strongly related to the output quantity.
Domain experts may use these feature sets to formulate hypotheses on physical laws.
However, if multiple alternative sets of similar quality exist, further analyses and experiments may be necessary to reveal the true underlying physical mechanism.
Only knowing one predictive feature set and using it as the only explanation is misleading in such a situation.

\paragraph{Problem statement}

This article addresses the problem of alternative feature selection, which we informally define as follows:
Find multiple, sufficiently different feature sets that optimize feature-set quality.
We provide formal definitions in Section~\ref{sec:afs:approach:constraints}.
This problem entails an interesting trade-off:
Depending on how different the alternatives should be, one might have to compromise on quality.
In particular, a stronger dissimilarity requirement might require selecting more low-quality features in the alternatives.

Two points are essential for alternative feature selection, which we both address in this article.
First, one needs to formalize and quantify what an alternative feature set is.
In particular, users should be able to control the dissimilarity of alternatives and hence the aforementioned quality trade-off.
Second, one needs an approach to find alternative feature sets efficiently.
Ideally, the approach should be general, i.e., cover a broad range of conventional feature-selection methods, given the variety of the latter~\cite{chandrashekar2014survey, li2017feature}.

\paragraph{Related work}

While finding alternative solutions has already been addressed extensively in the field of clustering~\cite{bailey2014alternative}, there is a lack of such approaches for feature selection.
Only a few feature-selection methods target at obtaining multiple, diverse feature sets~\cite{borboudakis2021extending}.
In particular, techniques for ensemble feature selection~\cite{saeys2008robust, seijo2017ensemble} and statistically equivalent feature subsets~\cite{lagani2017feature} produce multiple feature sets but not optimal alternatives.
These approaches do not guarantee the diversity of the feature sets, nor do they let users control diversity.
In fields related to feature selection, the goal of obtaining multiple, diverse solutions has been studied as well, e.g., for subspace clustering~\cite{hu2018subspace, mueller2009relevant}, subgroup discovery~\cite{leeuwen2012diverse}, subspace search~\cite{trittenbach2019dimension}, or explainable-AI techniques~\cite{artelt2022even, kim2016examples, mothilal2020explaining, russell2019efficient} like counterfactuals.
These approaches are not directly applicable or easily adaptable to feature selection, and most of them provide limited or no user control over alternatives, as we will elaborate in Section~\ref{sec:afs:related-work}.

\paragraph{Contributions}

Our contribution is five-fold.

First, we formalize alternative feature selection as an optimization problem.
In particular, we define alternatives via constraints on feature sets.
This approach is orthogonal to the feature-selection method so that users can choose the latter according to their needs.
This approach also allows integrating other constraints on feature sets, e.g., to capture domain knowledge \cite{bach2022empirical, groves2015toward}.
Finally, this approach lets users control the search for alternatives with two parameters, i.e., the number of alternatives and a dissimilarity threshold.
For multiple alternatives, we consider sequential as well as simultaneous search.

Second, we discuss how to solve this optimization problem.
To that end, we describe how to integrate different categories of conventional feature-selection methods in the objective function of the optimization problem.
In particular, we propose solver-based search methods for white-box and black-box optimization.

Third, we analyze the computational complexity of the optimization problem.
We show $\mathcal{NP}$-hardness, even for a simple notion of feature-set quality, i.e., univariate feature qualities, as used in filter feature selection.

Fourth, we propose heuristic search methods for univariate feature qualities.
We show that, under certain conditions, the optimization problem resides in complexity class $\mathcal{APX}$, i.e., a constant-factor approximation exists.

Fifth, we evaluate alternative feature selection with comprehensive experiments.
In particular, we use 30 binary-classification datasets from the Penn Machine Learning Benchmarks (PMLB)~\cite{olson2017pmlb, romano2021pmlb} and five feature-selection methods.
We focus our evaluation on the feature-set quality of the alternatives relative to our user parameters.
We publish all our code\footnote{\url{https://github.com/Jakob-Bach/Alternative-Feature-Selection}} and experimental data\footnote{\url{https://doi.org/10.35097/1623}} online.
%TODO new link to experimental data

\paragraph{Experimental results}

We observe that several factors influence the quality of alternatives, i.e., the dataset, feature-selection method, notion of feature-set quality, and parameters for searching alternatives.
As expectable, feature-set quality tends to decrease with an increasing number of alternatives and an increasing dissimilarity threshold for alternatives.
Thus, these parameters allow users to control the trade-off between dissimilarity and quality of alternatives.
Also, even no valid alternative may exist if the parameter values are too strict.
Computationally, a solver-based sequential search for multiple alternatives was significantly faster than a simultaneous one while yielding a similar quality.
Additionally, our heuristic search methods for univariate feature qualities achieved a high quality with negligible runtime.
Finally, we observe that the prediction performance of feature sets may only weakly correlate with the quality assigned by feature-selection methods.
In particular, seemingly bad alternatives regarding the latter might still be good regarding the former.

\paragraph{Outline}

Section~\ref{sec:afs:fundamentals} introduces notation and fundamentals.
Section~\ref{sec:afs:approach} describes and analyzes alternative feature selection.
Section~\ref{sec:afs:related-work} reviews related work.
Section~\ref{sec:afs:experimental-design} outlines our experimental design, while Section~\ref{sec:afs:evaluation} presents the experimental results.
Section~\ref{sec:afs:conclusion} concludes.
Appendix~\ref{sec:afs:appendix} contains supplementary materials.

\section{Fundamentals}
\label{sec:afs:fundamentals}

In this section, we introduce basic notation (cf.~Section~\ref{sec:afs:fundamentals:notation}) and review different methods to measure the quality of feature sets (cf.~Section~\ref{sec:afs:fundamentals:quality}).

\subsection{Notation}
\label{sec:afs:fundamentals:notation}

$X \in \mathbb{R}^{m \times n}$ stands for a dataset in the form of a matrix.
Each row is a data object, and each column is a feature.
$\tilde{F} = \{f_1, \dots, f_n\}$ is the corresponding set of feature names.
We assume that categorical features have already been made numeric, e.g., via one-hot encoding.
$X_{\cdot{}j} \in \mathbb{R}^m$ denotes the vector representation of the $j$-th feature.
$y \in Y^m$ represents the prediction target with domain $Y$, e.g., $Y=\{0,1\}$ for binary classification or $Y=\mathbb{R}$ for regression.

In feature selection, one makes a binary decision $s_j \in \{0,1\}$ for each feature, i.e., either selects it or not.
The vector $s \in \{0,1\}^n$ combines all these selection decisions and yields the selected feature set $F_s = \{f_j \mid s_j=1\} \subseteq \tilde{F}$.
To simplify notation, we drop the subscript~$s$ in definitions where we do not explicitly refer to the value of~$s$ but only the set~$F$.
The function $Q(s,X,y)$ returns the quality of such a feature set.
Without loss of generality, we assume that this function should be maximized.

\subsection{Measuring Feature (Set) Quality}
\label{sec:afs:fundamentals:quality}

There are different ways to evaluate feature-set quality $Q(s,X,y)$.
We only give a short overview here; see~\cite{chandrashekar2014survey, li2017feature, njoku2023wrapper} for comprehensive studies and surveys of feature selection.
Also, note that we assume a supervised feature-selection scenario, i.e., feature-set quality depending on a prediction target~$y$.
In principle, our definitions of alternatives also apply to an unsupervised scenario.
Since the prediction target only appears in the function~$Q(s,X,y)$, one could replace~$Q(s,X,y)$ with $Q(s,X)$, i.e., an unsupervised notion of quality.

A conventional categorization of feature-selection methods distinguishes between filter, wrapper, and embedded methods~\cite{guyon2003introduction}.

\paragraph{Filter methods}

Filter methods evaluate feature sets without training a prediction model.
Univariate filters assess each feature independently.
They often assign a score to each feature, e.g., the absolute Pearson correlation or the mutual information between a feature and the prediction target.
Such methods ignore potential interactions between features, e.g., redundancies.
In contrast, multivariate filters evaluate feature sets as a whole.
Such methods often combine a measure of feature relevance with a measure of feature redundancy.
Examples include CFS~\cite{hall1999correlation, hall2000correlation}, FCBF~\cite{yu2003feature}, and mRMR~\cite{peng2005feature}.

\paragraph{Wrapper methods}

Wrapper methods~\cite{kohavi1997wrappers} evaluate feature sets by training prediction models with them and measuring prediction quality.
They employ a generic search strategy to iterate over candidate feature sets, e.g., genetic algorithms.
Feature-set quality is a black-box function in this search.

\paragraph{Embedded methods}

Embedded methods train prediction models with built-in feature selection, e.g., decision trees~\cite{breiman1984classification} or random forests~\cite{breiman2001random}.
Thus, the criterion for feature-set quality is model-specific.
For example, tree-based models often use information gain or the Gini index to select features during training.

\paragraph{Post-hoc feature-importance methods}

Apart from conventional feature selection, there are various methods that assess feature importance after training a model.
These methods range from local explanation methods like LIME~\cite{ribeiro2016should} or SHAP~\cite{lundberg2017unified} to global importance methods like permutation importance~\cite{breiman2001random} or SAGE~\cite{covert2020understanding}.
In particular, assessing feature importance plays a crucial role in the field of machine-learning interpretability~\cite{carvalho2019machine, molnar2020interpretable}.

\section{Alternative Feature Selection}
\label{sec:afs:approach}

In this section, we present the problem and approaches for alternative feature selection.
First, we define the overall structure of the optimization problem, i.e., objective and constraints (cf.~Section~\ref{sec:afs:approach:problem}).
Second, we formalize the notion of alternatives via constraints (cf.~Section~\ref{sec:afs:approach:constraints}).
Third, we discuss different objective functions corresponding to different feature-set quality measures from Section~\ref{sec:afs:fundamentals:quality}.
In particular, we describe how to solve the resulting optimization problem (cf.~Section~\ref{sec:afs:approach:objectives}).
Fourth, we analyze the computational complexity of the optimization problem (cf.~Section~\ref{sec:afs:approach:complexity}).
Fifth, we propose and analyze heuristic search methods for the optimization problem (cf.~Section~\ref{sec:afs:approach:univariate-heuristics}).

\subsection{Optimization Problem}
\label{sec:afs:approach:problem}

Alternative feature selection has two goals.
First, the quality of an alternative feature set should be high.
Second, an alternative feature set should differ from one or more other feature set(s).
There are several ways to combine these two goals in an optimization problem:

First, one can consider both goals as objectives, obtaining an unconstrained multi-objective problem.
Second, one can treat feature-set quality as objective and enforce alternatives with constraints.
Third, one can consider being alternative as objective and constrain feature-set quality, e.g., with a lower bound.
Fourth, one can define constraints for both, feature-set quality and being alternative, searching for feasible solutions instead of optimizing.

We stick to the second formulation, i.e., optimizing feature-set quality subject to being alternative.
This formulation has the advantage of keeping the original objective function of feature selection.
Thus, users do not need to specify a range or a threshold on feature-set quality but can control how alternative the feature sets must be instead.
We obtain the following optimization problem for a single alternative feature set~$F_s$:
%
\begin{equation}
	\begin{aligned}
		\max_s &\quad Q(s,X,y) \\
		\text{subject to:} &\quad F_s~\text{being alternative}
	\end{aligned}
	\label{eq:afs:afs-general}
\end{equation}
%
In the following, we discuss different objective functions $Q(s,X,y)$ and suitable constraints for \emph{being alternative}.
Additionally, many feature-selection methods also limit the feature-set size $|F_s|$ to a user-defined value~$k \in \mathbb{N}$, which adds a further, simple constraint to the optimization problem.

\subsection{Constraints -- Defining Alternatives}
\label{sec:afs:approach:constraints}

In this section, we formalize alternative feature sets.
First, we discuss the base case where an individual feature set is an alternative to another one (cf.~Section~\ref{sec:afs:approach:constraints:single}).
Second, we extend this notion to multiple alternatives, considering sequential and simultaneous search as two different search problems (cf.~Section~\ref{sec:afs:approach:constraints:multiple}).

Our notion of alternatives is independent of the feature-selection method.
We provide two parameters, i.e., a dissimilarity threshold~$\tau$ and the number of alternatives~$a$, allowing users to control the search for alternatives.

\subsubsection{Single Alternative}
\label{sec:afs:approach:constraints:single}

We consider a feature set an alternative to another feature set if it differs sufficiently.
Mathematically, we express this notion with a set-dissimilarity measure~\cite{choi2010survey, egghe2009new}.
These measures typically assess how strongly two sets overlap and relate this to their sizes.
E.g., a well-known set-dissimilarity measure is the Jaccard distance, which is defined as follows for the feature sets $F'$ and $F''$:
%
\begin{equation}
	d_{\text{Jacc}}(F',F'') = 1 - \frac{|F' \cap F''|}{|F' \cup F''|} = 1 - \frac{|F' \cap F''|}{|F'| + |F''| - |F' \cap F''|}
	\label{eq:afs:jaccard}
\end{equation}
%
In this article, we use a dissimilarity measure based on the Dice coefficient:
%
\begin{equation}
	d_{\text{Dice}}(F',F'') = 1 - \frac{2 \cdot |F' \cap F''|}{|F'| + |F''|}
	\label{eq:afs:dice}
\end{equation}
%
Generally, we do not have strong requirements on the set-dissimilarity measure~$d(\cdot)$.
Our definitions of alternatives only assume symmetry, i.e., $d(F',F'')=d(F'',F')$, and non-negativity, i.e., $d(F',F'') \geq 0$, though one could adapt them to other conditions as well.
In particular, the dissimilarity measure does not need to be a metric but can also be a semi-metric~\cite{wilson1931semi} like~$d_{\text{Dice}}(\cdot)$.

We leverage the set-dissimilarity measure for the following definition:
%
\begin{definition}[Single alternative]
	Given a symmetric, non-negative set-dissimi\-larity measure~$d(\cdot)$ and a dissimilarity threshold~$\tau \in \mathbb{R}_{\geq 0}$, a feature set $F'$ is an alternative to a feature set~$F''$ (and vice versa) if $d(F',F'') \geq \tau$.
	\label{def:afs:single-alternative}
\end{definition}
%
The threshold~$\tau$ controls how alternative the feature sets must be and depends on the dataset as well as user preferences.
In particular, requiring strong dissimilarity may cause a significant drop in feature-set quality.
Some datasets may contain many features of similar utility, thereby enabling many alternatives of similar quality, while predictions on other datasets may depend on a few key features.
Only users can decide which drop in feature-set quality is acceptable as a trade-off for obtaining alternatives.
Thus, we leave $\tau$ as a user parameter.
In case the set-dissimilarity measure $d(\cdot)$ is normalized to $[0,1]$, like the Dice dissimilarity (cf.~Equation~\ref{eq:afs:dice}) or Jaccard distance (cf.~Equation~\ref{eq:afs:jaccard}), the interpretation of $\tau$ is user-friendly:
Setting $\tau=0$ allows identical alternatives, while $\tau=1$ implies zero overlap.

If the choice of $\tau$ is unclear a priori, users can try out different values and compare the resulting feature-set quality.
One systematic approach is a binary search:
Start with the mid-range value of $\tau=0$, i.e., 0.5 for $\tau \in [0,1]$.
If the quality of the resulting alternative is too low, decrease $\tau$ to 0.25, i.e., allow more similarity.
If the quality of the resulting alternative is acceptably high, increase $\tau$ to 0.75, i.e., check a more dissimilar feature set.
Continue this procedure till an alternative with an acceptable quality-dissimilarity trade-off is found.

When implementing Definition~\ref{def:afs:single-alternative}, the following proposition gives way to using a broad range of solvers to tackle the related optimization problem:
%
\begin{proposition}[Linearity of constraints for alternatives]
	Using the Dice dissimilarity (cf.~Equation~\ref{eq:afs:dice}), one can express alternative feature sets (cf.~Definition~\ref{def:afs:single-alternative}) with 0-1 integer linear constraints.
	\label{prop:afs:linear-constraints}
\end{proposition}
%
\begin{proof}
We re-arrange terms in the Dice dissimilarity (cf.~Equation~\ref{eq:afs:dice}) to get rid of the quotient of feature-set sizes:
%
\begin{equation}
	\begin{aligned}
		d_{\text{Dice}}(F',F'') = 1 - \frac{2 \cdot |F' \cap F''|}{|F'| + |F''|} &\geq \tau \\
		\Leftrightarrow |F' \cap F''| &\leq \frac{1 - \tau}{2} \cdot (|F'| + |F''|)
	\end{aligned}
	\label{eq:afs:dice-rearranged}
\end{equation}
%
Next, we express set sizes in terms of the feature-selection vector $s$:
%
\begin{equation}
	\begin{aligned}
		|F_s| =& \sum_{j=1}^n s_j \\
		|F_{s'} \cap F_{s''}| =& \sum_{j=1}^n s'_j \cdot s''_j
	\end{aligned}
	\label{eq:afs:feature-set-size}
\end{equation}
%
Finally, we replace each product $s'_j \cdot s''_j$ with an auxiliary variable~$t_j$, bound by additional constraints, to linearize it~\cite{mosek2022modeling}:
%
\begin{equation}
	\begin{aligned}
		t_j \leq& s'_j \\
		t_j \leq& s''_j \\
		1 + t_j \geq& s'_j + s''_j \\
		t_j \in& \{0,1\}
	\end{aligned}
	\label{eq:afs:product-linear}
\end{equation}
%
Combining Equations~\ref{eq:afs:dice-rearranged},~\ref{eq:afs:feature-set-size}, and~\ref{eq:afs:product-linear}, we obtain a set of constraints that only involve linear expressions of binary decision variables.
In particular, there are only sum expressions and multiplications with constants but no products between variables.
If one feature set is known, i.e., either $s'$ or $s''$ is fixed, Equation~\ref{eq:afs:feature-set-size} only multiplies variables with constants and is already linear without Equation~\ref{eq:afs:product-linear}.
\end{proof}
%
Given a suitable objective function, which we discuss later, linear constraints allow using a broad range of solvers.
As an alternative formulation, one could also encode such constraints into propositional logic (\textsc{SAT})~\cite{ulrich2022selecting}.

If the set sizes $|F'|$ and $|F''|$ are constant, e.g., user-defined, Equation~\ref{eq:afs:dice-rearranged} implies that the threshold~$\tau$ has a linear relationship to the maximum number of overlapping features~$|F' \cap F''|$.
This correspondence eases the interpretation of~$\tau$ and makes us use the Dice dissimilarity in the following.
In contrast, the Jaccard distance exhibits a non-linear relationship between $\tau$ and the overlap size, which follows from re-arranging Equation~\ref{eq:afs:jaccard} in combination with Definition~\ref{def:afs:single-alternative}:
%
\begin{equation}
	\begin{aligned}
		d_{\text{Jacc}}(F',F'') = 1 - \frac{|F' \cap F''|}{|F'| + |F''| - |F' \cap F''|} &\geq \tau \\
		\Leftrightarrow |F' \cap F''| &\leq \frac{1 - \tau}{2 - \tau} \cdot (|F'| + |F''|)
		\end{aligned}
	\label{eq:afs:jaccard-rearranged}
\end{equation}
%
Further, if $|F'| = |F''|$, as in our experiments, the Dice dissimilarity (cf.~Equation~\ref{eq:afs:dice-rearranged}) becomes identical to several other set-dissimilarity measures~\cite{egghe2009new}.
The parameter~$\tau$ then directly expresses which fraction of features in one set needs to differ from the other set and vice versa, which further eases interpretability:
%
\begin{equation}
	d_{\text{Dice}}(F',F'') \geq \tau \Leftrightarrow |F' \cap F''| \leq (1 - \tau) \cdot |F'| = (1 - \tau) \cdot |F''|
	\label{eq:afs:dice-rearranged-equal-size}
\end{equation}
%
Thus, if users are uncertain how to choose $\tau$ and $|F'|$ is reasonably small, they can try out all values of $\tau \in \{i / |F'|\}$ with $i \in \{1, \dots, |F'|\}$.
In particular, these $|F'|$~unique values of $\tau$ suffice to produce all distinct solutions that one could obtain with an arbitrary $\tau \in (0,1]$.

\subsubsection{Multiple Alternatives}
\label{sec:afs:approach:constraints:multiple}

If users desire multiple alternative feature sets rather than only one, we can determine these alternatives sequentially or simultaneously.
The number of alternatives~$a \in \mathbb{N}_0$ is a parameter to be set by the user.
The overall number of feature sets is $a + 1$ since we deem one feature set the `original' one.
Table~\ref{tab:afs:seq-sim-comparison} compares the sizes of the optimization problems for these two search problems.

\begin{table}[t]
	\centering
	\renewcommand*{\arraystretch}{1.3}
	\begin{tabular}{lccc}
		\toprule
		& \multicolumn{2}{c}{Sequential search} & \multirow{2}{*}{Simultaneous search} \\
		\cmidrule(r){2-3}
		& Alternative $i$ & Summed & \\
		\midrule
		Decision variables~$s$ & $n$ & $ (a+1) \cdot n$ & $(a+1) \cdot n$ \\
		Linearization variables~$t$ & $0$ & $0$ & $\frac{a \cdot (a+1) \cdot n}{2}$ \\
		Alternative constraints & $i$ & $\frac{a \cdot (a+1)}{2}$ & $\frac{a \cdot (a+1)}{2}$ \\
		Linearization constraints & $0$ & $0$ & $\frac{3 \cdot a \cdot (a+1) \cdot n}{2}$ \\
		\bottomrule
	\end{tabular}
	\caption{Size of the optimization problem by search problem, for $a$~alternatives ($a + 1$~feature sets overall) and $n$ features.}
	\label{tab:afs:seq-sim-comparison}
\end{table}

\paragraph{Sequential-search problem}

In the sequential-search problem, users obtain several alternatives iteratively, with one feature set per iteration.
We constrain this new set to be an alternative to all previously found ones, which are given in the set~$\mathbb{F}$:
%
\begin{definition}[Sequential alternative]
	A feature set~$F''$ is an alternative to a set of feature sets~$\mathbb{F}$ (and vice versa) if $F''$ is a single alternative (cf.~Definition~\ref{def:afs:single-alternative}) to each $F' \in \mathbb{F}$.
	\label{def:afs:sequential-alternative}
\end{definition}
%
One could also think of less strict constraints, e.g., requiring only the average dissimilarity to all previously found feature sets to pass a threshold~$\tau$.
However, definitions like the latter may allow some feature sets to overlap heavily or even be identical if other feature sets are very dissimilar.
Thus, we require pairwise dissimilarity in Definition~\ref{def:afs:sequential-alternative}.
Combining Equation~\ref{eq:afs:afs-general} with Definition~\ref{def:afs:sequential-alternative}, we obtain the following optimization problem for each iteration of the search:
%
\begin{equation}
	\begin{aligned}
		\max_s &\quad Q(s,X,y) \\
		\text{subject to:} &\quad \forall F' \in \mathbb{F}:~d(F_s,F') \geq \tau
	\end{aligned}
	\label{eq:afs:afs-sequential}
\end{equation}
%
The objective function remains the same as for a single alternative ($|\mathbb{F}| = 1$), i.e., we only optimize the quality of one feature set at once.
In particular, with $\mathbb{F} = \emptyset$ in the first iteration, we optimize for the `original' feature set, which is the same as in conventional feature selection without constraints for alternatives.
Thus, the number of variables in the optimization problem is independent of the number of alternatives~$a$.
Instead, we solve the optimization problem repeatedly; each alternative only adds one constraint to the problem.
As we always compare only one variable feature set to existing, constant feature sets, we also do not need to introduce auxiliary variables as in Equation~\ref{eq:afs:product-linear}.
Thus, we expect the runtime of exact, e.g., solver-based, sequential search to scale well with the number of alternatives.
Further runtime gains may arise if the solver keeps a state between iterations and can warm-start.

However, as the solution space becomes narrower over iterations, feature-set quality can deteriorate with each further alternative.
In particular, multiple alternatives from the same sequential search might differ significantly in their quality.
As a remedy, users can decide after each iteration if the feature-set quality is already unacceptably low or if another alternative should be found.
In particular, users do not need to define the number of alternatives~$a$ a priori.

\paragraph{Simultaneous-search problem}

In the simultaneous-search problem, users obtain multiple alternatives at once, so they need to decide on the number of alternatives~$a$ beforehand.
We use pairwise dissimilarity constraints for alternatives again:
%
\begin{definition}[Simultaneous alternatives]
	A set of feature sets~$\mathbb{F}$ contains simultaneous alternatives if each feature set~$F' \in \mathbb{F}$ is a single alternative (cf.~Definition~\ref{def:afs:single-alternative}) to each other set~$F'' \in \mathbb{F}$, $F' \neq F''$.
	\label{def:afs:simultaneous-alternative}
\end{definition}
%
Combining Equation~\ref{eq:afs:afs-general} with Definition~\ref{def:afs:simultaneous-alternative}, we obtain the following optimization problem for $a+1$ feature sets:
%
\begin{equation}
	\begin{aligned}
		\max_{s^{(0)}, \dots, s^{(a)}} &\quad \operatorname*{agg}_{i \in \{0, \dots, a\}} Q(s^{(i)},X,y) \\
		\text{subject to:} &\quad \forall i_1, i_2 \in \{0, \dots, a\},~i_1 \neq i_2:~d(F_{s^{(i_1)}},F_{s^{(i_2)}}) \geq \tau
	\end{aligned}
	\label{eq:afs:afs-simultaneous}
\end{equation}
%
In contrast to the sequential case (cf.~Equation~\ref{eq:afs:afs-sequential}), the problem requires $a+1$ instead one decision vector~$s$, and a modified objective function.
The operator~$\text{agg}(\cdot)$ defines how to aggregate the feature-set qualities of the alternatives.
In our experiments, we consider the sum as well as the minimum to instantiate~$\text{agg}(\cdot)$, which we refer to as \emph{sum-aggregation} and \emph{min-aggregation}.
The latter explicitly fosters balanced feature-set qualities.
Appendix~\ref{sec:afs:appendix:simultaneous-objective-aggregation} discusses these two aggregation operators and additional ideas for balancing qualities in detail.

Runtime-wise, we expect exact simultaneous search to scale worse with the number of alternatives than exact sequential search, as it tackles one large optimization problem instead of multiple smaller ones.
In particular, the number of decision variables increases linearly with the number of alternatives~$a$.
Also, for each feature and each pair of alternatives, we need to introduce an auxiliary variable if we want to obtain linear constraints (cf.~Equation~\ref{eq:afs:product-linear} and Table~\ref{tab:afs:seq-sim-comparison}).

In contrast to the greedy definition of sequential search, simultaneous search optimizes alternatives globally.
Thus, the simultaneous procedure should yield the same or higher average feature-set quality for the same number of alternatives.
Also, the quality can be more evenly distributed over the alternatives, as opposed to the dropping quality over the course of the sequential procedure.
However, increasing the number of alternatives still has a negative effect on the average feature-set quality.
Further, as opposed to the sequential procedure, there are no intermediate steps where users could interrupt the search.

\subsection{Objective Functions -- Finding Alternatives}
\label{sec:afs:approach:objectives}

In this section, we discuss how to find alternative feature sets.
In particular, we describe how to solve the optimization problem from Section~\ref{sec:afs:approach:problem} for the different categories of feature-set quality measures from Section~\ref{sec:afs:fundamentals:quality}.
We distinguish between white-box optimization (cf.~Section~\ref{sec:afs:approach:objectives:white-box}), black-box optimization (cf.~Section~\ref{sec:afs:approach:objectives:black-box}), and embedding alternatives (cf.~Section~\ref{sec:afs:approach:objectives:embedding}).

\subsubsection{White-Box Optimization}
\label{sec:afs:approach:objectives:white-box}

If the feature-set quality function~$Q(s,X,y)$ is sufficiently simple, one can tackle alternative feature selection with a suitable solver for white-box optimization problems.
We already showed that our notion of alternative feature sets results in 0-1 integer linear constraints (cf.~Proposition~\ref{prop:afs:linear-constraints}).
We now discuss several feature-selection methods with objectives that admit formulating a 0-1 integer linear problem.
Appendix~\ref{sec:afs:appendix:multivariate-filter-objectives}~describes feature-selection methods we did not include in our experiments.

\paragraph{Univariate filter feature selection}

For univariate filter feature selection, the objective function is linear by default.
In particular, these methods decompose the quality of a feature set into the qualities of the individual features:
%
\begin{equation}
	\max_s \quad Q_{\text{uni}}(s,X,y) = \sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j
	\label{eq:afs:univariate-filter}
\end{equation}
%
Here, $q(\cdot)$ typically is a bivariate dependency measure, e.g., mutual information~\cite{kraskov2004estimating} or the absolute value of Pearson correlation, to quantify the relationship between one feature and the prediction target.

For this objective, Appendix~\ref{sec:afs:appendix:univariate-complete-optimization-problem} specifies the complete optimization problem, including the constraints for alternatives.
Appendix~\ref{sec:afs:appendix:univariate-pre-selection} describes how to potentially speed up optimization by leveraging the monotonicity of the objective.
Section~\ref{sec:afs:approach:univariate-heuristics} proposes heuristic search methods for this objective.

Instead of an integer problem, one could formulate a weighted partial maximum satisfiability (\textsc{MaxSAT}) problem~\cite{bacchus2021maximum, li2021maxsat}, i.e., a weighted \textsc{Max One} problem~\cite{khanna1997complete}.
In particular, Equation~\ref{eq:afs:univariate-filter} is a sum of weighted binary variables, and the constraints for alternatives can be turned into SAT formulas with a cardinality encoding~\cite{sinz2005towards} for the sum expressions.

\paragraph{Post-hoc feature importance}

Technically, one can also insert values of post-hoc feature-importance scores into Equation~\ref{eq:afs:univariate-filter}.
For example, one can pre-compute permutation importance~\cite{breiman2001random} or SAGE scores~\cite{covert2020understanding} for each feature and use them as univariate feature qualities~$q(X_{\cdot{}j},y)$.
However, such post-hoc importance scores typically evaluate the quality of each feature in the presence of other features. 
For example, a feature may only be important in subsets where another feature is present, due to feature interaction, but unimportant otherwise, and a post-hoc importance method like SHAP~\cite{lundberg2017unified} may reflect both these aspects.
In contrast, Equation~\ref{eq:afs:univariate-filter} implicitly assumes feature independence and cannot adapt importance scores depending on whether other features are selected.
Thus, treating pre-computed post-hoc importance scores as univariate feature qualities in the optimization objective can serve as a heuristic but may not faithfully represent the actual feature qualities in a particular selected set.

\paragraph{FCBF}

The Fast Correlation-Based Filter (FCBF)~\cite{yu2003feature} bases on the notion of predominance:
Each selected feature's correlation with the prediction target must exceed a user-defined threshold as well as the correlation of each other selected feature with the given one.
While the original FCBF uses a heuristic search to find predominant features, we propose a formulation as a constrained optimization problem to enable a white-box optimization for alternatives:
%
\begin{equation}
	\begin{aligned}
		\max_s &\quad Q_{\text{FCBF}}(s,X,y) = \sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j \\
		\text{subject to:} &\quad \forall j_1, j_2 \in \{1, \dots, n\},~j_1 \neq j_2,~(*): s_{j_1} + s_{j_2} \leq 1 \\
		\text{with } (*) \text{:} &\quad q(X_{\cdot{}j_1},y) \leq q(X_{\cdot{}j_2}, X_{\cdot{}j_1}) \\
	\end{aligned}
	\label{eq:afs:fcbf}
\end{equation}
%
We drop the original FCBF's threshold on feature-target correlation and maximize the latter instead, as in the univariate-filter case.
This change could produce large feature sets that contain many low-quality features.
As a countermeasure, one can constrain the feature-set sizes, as we do in our experiments.
Additionally, one could also filter out the features with low target correlation before optimization.
Further, we keep FCBF's constraints on feature-feature correlation.
In particular, we prevent the simultaneous selection of two features if the correlation between them is at least as high as one of the features' correlation to the target.
Since the condition~$q(X_{\cdot{}j_1},y) \leq q(X_{\cdot{}j_2}, X_{\cdot{}j_1})$ in Equation~\ref{eq:afs:fcbf} does not depend on the decision variables~$s$, one can check whether it holds before formulating the optimization problem and add the corresponding linear constraint $s_{j_1} + s_{j_2} \leq 1$ only for feature pairs where it is needed.

\paragraph{mRMR}

Minimal Redundancy Maximum Relevance (mRMR)~\cite{peng2005feature} combines two criteria, i.e., feature relevance and feature redundancy.
Relevance corresponds to the dependency between features and prediction target, which should be maximized, as for univariate filters.
Redundancy, in turn, corresponds to the dependency between features, which should be minimized.
Both terms are averaged over the selected features.
Using a bivariate dependency measure~$q(\cdot)$, the objective is maximizing the following difference between relevance and redundancy:
%
\begin{equation}
	\begin{aligned}
		\max_s \quad Q_{\text{mRMR}}(s,X,y) &= \frac{\sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j}{\sum_{j=1}^{n} s_j} \\
		&- \frac{\sum_{j_1=1}^{n} \sum_{j_2=1}^{n} q(X_{\cdot{}j_1}, X_{\cdot{}j_2}) \cdot s_{j_1} \cdot s_{j_2}}{(\sum_{j=1}^{n} s_j)^2}
	\end{aligned}
	\label{eq:afs:mrmr}
\end{equation}
%
If one knows the feature-set size $\sum_{j=1}^{n} s_j$ to be a constant~$k$, the denominators of both fractions are constant, so the objective leads to a quadratic-programming problem~\cite{nguyen2014effective, rodriguez2010quadratic}.
If one additionally replaces each product terms $s_{j_1} \cdot s_{j_2}$ according to Equation~\ref{eq:afs:product-linear}, the problem becomes linear.
However, there is a more efficient linearization~\cite{nguyen2009optimizing, nguyen2010towards}, which we use in our experiments:
%
\begin{equation}
	\begin{aligned}
		\max_s &\quad & Q_{\text{mRMR}}(s,X,y) &= \frac{\sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j}{k} - \frac{\sum_{j=1}^{n} z_j}{k \cdot (k-1)} \\
		\text{subject to:} &\quad \forall j_1: & A_{j_1} &= \sum_{j_2 \neq j_1} q(X_{\cdot{}j_1}, X_{\cdot{}j_2}) \cdot s_{j_2} \\
		&\quad \forall j: & z_j &\geq M \cdot (s_j - 1) + A_j \\
		&\quad \forall j: & z_j &\in \mathbb{R}_{\geq 0} \\
		\text{with indices:} &\quad & j, j_1, j_2 &\in \{1, \dots, n\}
	\end{aligned}
	\label{eq:afs:mrmr-linear}
\end{equation}
%
Here, $A_{j_1}$ is the sum of all redundancy terms related to the feature with index~$j_1$, i.e., the summed dependency value between this feature and all other selected features.
Thus, one can use one real-valued auxiliary variable $z_j$ for each feature instead of one new binary variable for each pair of features.
Since redundancy should be minimized, $z_j$ assumes the value of $A_j$ with equality if the feature with index~$j$ is selected~($s_j=1$) and is zero otherwise ($s_j=0$).
To this end, $M$ is a large positive value that deactivates the constraint $z_j \geq A_j$ if $s_j=0$.

Since Equation~\ref{eq:afs:mrmr-linear} assumes the feature-set size~$k \in \mathbb{N}$ to be user-defined before optimization, it requires fewer auxiliary variables and constraints than the more general formulation in~\cite{nguyen2009optimizing, nguyen2010towards}.
Additionally, in accordance with~\cite{nguyen2014effective}, we assign a value of zero to the self-redundancy terms $q(X_{\cdot{}j},X_{\cdot{}j})$, effectively excluding them from the objective function.
Thus, the redundancy term uses $k \cdot (k-1)$ instead of $k^2$ for averaging.

\subsubsection{Black-Box Optimization}
\label{sec:afs:approach:objectives:black-box}

If feature-set quality does not have an expression suitable for white-box optimization, one has to treat it as a black-box function when searching for alternatives.
This situation applies to wrapper feature-selection methods, which use prediction models to assess feature-set quality.
One can optimize such black-box functions with search heuristics that systematically iterate over candidate feature sets.
However, search heuristics often assume an unconstrained search space and may propose candidate feature sets that are not alternative enough.
We see four ways to address this issue:

\paragraph{Enumerating feature sets}

Instead of using a search heuristic, one may enumerate all feature sets that are alternative enough.
E.g., one can iterate over all feature sets and sort out those violating the constraints or use a solver to enumerate all valid alternatives directly.
Both approaches are usually very inefficient, as there can be a vast number of alternatives.

\paragraph{Sampling feature sets}

Instead of considering all possible alternatives, one can also sample a limited number.
E.g., one could sample from all feature sets but remove samples that are not alternative enough.
However, if the number of valid alternatives is small, this approach might need many samples.
One could also sample with the help of a solver.
However, uniform sampling from a constrained space is a computationally hard problem, possibly harder than determining if a valid solution exists or not~\cite{ermon2012uniform}.

\paragraph{Multi-objective optimization}

If one phrases alternative feature selection as a multi-objective problem (cf.~Section~\ref{sec:afs:approach:problem}), there are no hard constraints anymore, and one could apply a standard multi-objective black-box search procedure.
However, as explained in Section~\ref{sec:afs:approach:problem}, we decided to pursue a single-objective formulation with constraints.

\paragraph{Adapting search}

One can adapt an existing search heuristic to consider the constraints for alternatives.
One idea is to prevent the search from producing feature sets that violate the constraints or at least make the latter less likely, e.g., with a penalty in the objective function.
Another idea is to `repair' feature sets in the search that violate constraints, e.g., replacing them with the most similar feature sets satisfying the constraints.
Such solver-assisted search approaches are common in search procedures for software feature models~\cite{guo2018preserve, henard2015combining, white2010automated}.
One could also apply solver-based repair to sampled feature sets.

\begin{algorithm}[t]
	\DontPrintSemicolon
	\KwIn{Dataset $X$ with $n$ features, \newline
		Prediction target $y$, \newline
		Feature-set quality function $Q(\cdot)$, \newline
		Constraints for alternatives $Cons$, \newline
		Maximum number of iterations $max\_iters$}
	\KwOut{Set of feature-selection decision vectors $S = \{s^{(0)}, \dots, s^{(a)}\}$}
	\BlankLine
	$S \leftarrow \text{Solve}(Cons)$ \tcp*{Initial alternatives} \label{al:afs:greedy-wrapper:line:init}
	$iters \leftarrow 1$ \tcp*{Number of iterations = solver calls}
	\lIf(\tcp*[f]{No valid alternatives exist}){$S = \emptyset$}{\Return{$\emptyset$}}
	$j_1 \leftarrow 1$ \tcp*{Indices of features to be swapped}
	$j_2 \leftarrow j_1 + 1$\;
	\While{$iters < max\_iters$ \textbf{and} $j_1 < n$}{ \label{al:afs:greedy-wrapper:line:stop}
		$S' \leftarrow \text{Solve}(Cons \cup \{\neg s^{(i)}_{j_1}, \neg s^{(i)}_{j_2} \mid i \in \{0, \dots, a\}\})$ \tcp*{Try swap} \label{al:afs:greedy-wrapper:line:swap}
		$iters \leftarrow iters + 1$\;
		\If(\tcp*[f]{Swap if improved}){$S' \neq \emptyset$ \textbf{and} $Q(S',X,y) > Q(S,X,y)$}{ \label{al:afs:greedy-wrapper:line:improved-condition}
			$S \leftarrow S'$\; \label{al:afs:greedy-wrapper:line:improved-start}
			$j_1 \leftarrow 1$ \tcp*{Reset swap-feature indices}
			$j_2 \leftarrow j_1 + 1$\; \label{al:afs:greedy-wrapper:line:improved-end}
		}
		\ElseIf(\tcp*[f]{Try next swap; advance one index}){$j_2 < n$}{ \label{al:afs:greedy-wrapper:line:next-start}
			$j_2 \leftarrow j_2 + 1$\;
		}
		\Else(\tcp*[f]{Try next swap; advance both indices}){
			$j_1 \leftarrow j_1 + 1$\;
			$j_2 \leftarrow j_1 + 1$\; \label{al:afs:greedy-wrapper:line:next-end}
		}
	}
	\Return{$S$}
	\caption{\emph{Greedy Wrapper} for alternative feature selection.}
	\label{al:afs:greedy-wrapper}
\end{algorithm}

\paragraph{Greedy Wrapper}

For wrapper feature selection in our experiments, we use a method that falls into the category \emph{adapting search}.
In particular, we propose a novel greedy hill-climbing procedure, displayed in Algorithm~\ref{al:afs:greedy-wrapper}.
Unlike standard hill climbing for feature selection~\cite{kohavi1997wrappers}, our procedure observes constraints.
First, the algorithm uses a solver to find one solution that is alternative enough, given the current constraints (Line~\ref{al:afs:greedy-wrapper:line:init}).
Thus, it has a valid starting point and can always return a solution unless there are no valid solutions at all.
Next, it tries `swapping' two features, i.e., selecting the features if they were deselected or deselecting them if they were selected (Line~\ref{al:afs:greedy-wrapper:line:swap}).
For simultaneous search, we swap the affected two features in each alternative feature set.
This swap might violate cardinality constraints as well as constraints for alternatives.
Thus, the algorithm calls the solver again to find one solution~$S'$ containing this swap and satisfying the other constraints.
If such a solution~$S'$ exists and its quality~$Q(S',X,y)$ is higher than the one of the current solution, the algorithm proceeds with the new solution, attempting again to swap the first and second features (Lines~\ref{al:afs:greedy-wrapper:line:improved-start}--\ref{al:afs:greedy-wrapper:line:improved-end}).
Otherwise, it tries to swap the next pair of features (Lines~\ref{al:afs:greedy-wrapper:line:next-start}--\ref{al:afs:greedy-wrapper:line:next-end}).
Specifically, we assess only one solution per swap before proceeding instead of exhaustively enumerating and evaluating all valid solutions involving the swap.

The algorithm terminates if no swap leads to an improvement or a fixed number of iterations~$max\_iters$ is reached (Line~\ref{al:afs:greedy-wrapper:line:stop}).
Due to its heuristic nature, the algorithm might get stuck in local optima rather than yielding the global optimum.
In particular, $max\_iters$ only is an upper bound on the iteration count since the algorithm can stop earlier.
We define the iteration count as the number of invocations of the solver, i.e., attempts to generate valid alternatives.
This number also bounds the number of prediction models trained.
However, we only train a model for valid solutions (Line~\ref{al:afs:greedy-wrapper:line:improved-condition}), and not all solver invocations may yield one.

\subsubsection{Embedding Alternatives}
\label{sec:afs:approach:objectives:embedding}

If feature selection is embedded into a prediction model, there is no general approach for finding alternative feature sets.
Instead, one would need to embed the search for alternatives into model training as well.
Thus, we leave the formulation of specific approaches open for future work.
E.g., one could adapt the training of decision trees to not split on a feature if the resulting feature set of the tree was too similar to a given feature set.
As another example, there are various formal encodings of prediction models, e.g., as \textsc{SAT} formulas~\cite{narodytska2018learning, schidler2021sat, yu2021learning}, where `training' already uses a solver.
In such representations, one may directly add constraints for alternatives.

\subsection{Computational Complexity}
\label{sec:afs:approach:complexity}

In this section, we analyze the time complexity of alternative feature selection.
In particular, we study the scalability regarding the number of features~$n \in \mathbb{N}$, also considering the feature-set size~$k \in \mathbb{N}$ and the number of alternatives~$a \in \mathbb{N}_0$.
Section~\ref{sec:afs:approach:complexity:exhaustive} discusses exhaustive search, which works for arbitrary feature-selection methods, while Section~\ref{sec:afs:approach:complexity:univariate} examines the optimization problem with univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}).
Section~\ref{sec:afs:approach:complexity:summary} summarizes key results.

\subsubsection{Exhaustive Search for Arbitrary Feature-Selection Methods}
\label{sec:afs:approach:complexity:exhaustive}

An exhaustive search over the entire search space is the arguably simplest though inefficient approach to finding alternative feature sets.
This approach provides an upper bound for the time complexity of a runtime-optimal search algorithm.
In this section, we assume unit costs for elementary arithmetic operations like addition, multiplication, and comparison of two numbers.

\paragraph{Conventional feature selection}

In general, the search space of feature selection grows exponentially with~$n$, even without alternatives.
In particular, there are $2^n - 1$ possibilities to form a single non-empty feature set of arbitrary size.
For a fixed feature-set size~$k$, there are $\binom{n}{k} = \frac{n!}{k! \cdot (n-k)!} \leq n^k$ solution candidates.
In an exhaustive search, we iterate over these feature sets:
%
\begin{proposition}[Complexity of exhaustive conventional feature selection]
	Exhaustive search for one feature set of size~$k$ from $n$~features has a time complexity of~$O(n^k)$ without the cost of evaluating the objective function.
	\label{prop:afs:complexity-exhaustive-conventional}
\end{proposition}
%
Evaluating the objective means computing the quality of each solution candidate so that we can determine the best feature set in the end.
The cost of this step depends on the feature-selection method but should usually be polynomial in~$n$.
Even better, since feature-set quality typically only depends on selected features rather than unselected ones, this cost may be polynomial in~$k \ll n$.

If we assume $k \ll n,~k \in O(1)$, i.e., $k$ being a small constant, independent from~$n$, then the complexity in Proposition~\ref{prop:afs:complexity-exhaustive-conventional} is polynomial rather than exponential in~$n$.
This assumption makes sense for feature selection, where one typically wants to obtain a small feature set from a high-dimensional dataset.
However, the exponent~$k$ may still render an exhaustive search practically infeasible.
In terms of parameterized complexity, the problem resides in class~$\mathcal{XP}$ since the complexity term has the form $O(f(k) \cdot n^{g(k)})$~\cite{downey1997parameterized}, here with parameter~$k$ and functions $f(k) = 1$, $g(k) = k$.

\paragraph{Sequential search}

Like conventional feature selection, sequential search for alternatives (cf.~Definition~\ref{def:afs:sequential-alternative}) optimizes feature sets one at a time.
However, not all size-$k$ feature sets are valid anymore.
In particular, the constraints for alternatives put an extra cost on each solution candidate.
Constraint checking involves iterating over all existing feature sets and features to compute the dissimilarity between sets (cf.~Equation~\ref{eq:afs:afs-sequential-complete}).
This procedure entails a cost of~$O(a \cdot n)$ for each new alternative and~$O(a^2 \cdot n)$ for the whole sequential search with $a$~alternatives.
Combining this cost with Proposition~\ref{prop:afs:complexity-exhaustive-conventional}, we obtain the following proposition:
%
\begin{proposition}[Complexity of exhaustive sequential search]
	Exhaustive sequential search (cf.~Equation~\ref{eq:afs:afs-sequential}) for $a \in \mathbb{N}$~alternative feature sets of size~$k$ from $n$~features has a time complexity of~$O(a^2 \cdot n^{k+1})$ without the cost of evaluating the objective function.
	\label{prop:afs:complexity-exhaustive-sequential}
\end{proposition}
%
Thus, the runtime resides in the parameterized complexity class~$\mathcal{XP}$ with the parameter~$k$ and remains polynomial if $k \in O(1)$ and $a \in O(n^c),~c \in O(1)$, i.e., $k$ is a small constant and $a$ is at most polynomial in~$n$.

\paragraph{Simultaneous search}

The simultaneous-search problem (cf.~Definition~\ref{def:afs:simultaneous-alternative}) enlarges the search space since it optimizes $a+1$ feature sets at once.
Thus, an exhaustive search over size-$k$ feature sets iterates over~$O((n^k)^{a+1}) = O(n^{k \cdot (a+1)})$ solution candidates.
Including the cost of constraint checking, we arrive at the following proposition:
%
\begin{proposition}[Complexity of exhaustive simultaneous search]
	Exhaustive simultaneous search (cf.~Equation~\ref{eq:afs:afs-simultaneous}) for $a \in \mathbb{N}$~alternative feature sets of size~$k$ from $n$~features has a time complexity of~$O(a^2 \cdot n^{k \cdot (a+1) + 1})$ without the cost of evaluating the objective function.
	\label{prop:afs:complexity-exhaustive-simultaneuos}
\end{proposition}
%
The scalability with~$n$ is worse than for exhaustive sequential search since the number of alternatives appears in the exponent now, except for a special case discussed in Appendix~\ref{sec:afs:appendix:complexity:exhaustive-simultaneous-special-case}.
Further, Proposition~\ref{prop:afs:complexity-exhaustive-simultaneuos} assumes that the constraints do not use linearization variables (cf.~Equations~\ref{eq:afs:product-linear} and~\ref{eq:afs:afs-simultaneous-complete}), which would enlarge the search space even further.
Finally, the complexity remains polynomial in~$n$ if $a$ and~$k$ are small and independent from~$n$, i.e., $a \cdot k \in O(1)$:
%
\begin{proposition}[Parameterized complexity of simultaneous-search problem]
	The simultaneous-search problem (cf.~Equation~\ref{eq:afs:afs-simultaneous}) for $a \in \mathbb{N}$~alternative feature sets of size~$k$ from $n$~features resides in the parameterized complexity class $\mathcal{XP}$ for the parameter~$a \cdot k$.
	\label{prop:afs:complexity-simultaneuos-xp}
\end{proposition}

\subsubsection{Univariate Feature Qualities}
\label{sec:afs:approach:complexity:univariate}

\paragraph{Motivation}

While the assumption $a \cdot k \in O(1)$ ensures polynomial runtime regarding~$n$ for arbitrary feature-selection methods, the optimization problem can still be hard without this assumption.
In the following, we derive complexity results for \emph{univariate feature qualities} (cf.~Equation~\ref{eq:afs:univariate-filter} and Appendix~\ref{sec:afs:appendix:univariate-complete-optimization-problem}).
This feature-selection method arguably has the simplest objective function, where the quality of a feature set is equal to the sum of the individual qualities of its constituent features.
This simplicity eases the transformation from and to well-known $\mathcal{NP}$-hard problems.
Appendix~\ref{sec:afs:appendix:complexity:related-work} discusses related work on these problems in detail.

In the following complexity analyses, we assume that the feature qualities~$q(X_{\cdot{}j},y)$ are given.
In particular, one can pre-compute these qualities before searching alternatives and treat them as constants in the optimization problem.
The complexity of this computation depends on the particular feature-quality measure and the number of data objects~$m$.
However, the number of features~$n$ should only affect the complexity linearly due to the univariate setting.

\paragraph{Min-aggregation with complete partitioning}

We start with three assumptions, which we will drop later:
First, we use a dissimilarity threshold of~$\tau = 1$, i.e., zero overlap of feature sets.
Second, all features must be part of one set.
Third, we analyze the simultaneous-search problem with min-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-min-objective}).
We call the combination of the first two assumptions, which implies $n = (a+1) \cdot k$, a \emph{complete partitioning}.
This scenario differs from $a \cdot k \in O(1)$, for which we made polynomial-runtime claims in Section~\ref{sec:afs:approach:complexity:exhaustive}.

A key factor for the hardness of partitioning is the number of solutions:
There are $\stirling{n}{a}$~ways to partition a set of $n$~elements into $a$~non-empty subsets, a Stirling number of the second kind~\cite{graham1994concrete}, which roughly scale like $a^n / a!$~\cite{moser1958stirling}, i.e., exponential in~$n$ for a fixed~$a$.
Even if the subset sizes are fixed, the scalability regarding~$n$ remains bad since it bases on a multinomial coefficient.

Our complete-partitioning scenario is a variant of the \textsc{Multi-Way Number Partitioning} problem:
Partition a multiset of $n$~integers into a fixed number of $a$~subsets such that the sums of all subsets are as equal as possible~\cite{korf2010objective}.
One problem formulation, called \textsc{Multiprocessor Scheduling} in~\cite{garey2003computers}, minimizes the maximum subset sum:
The goal is to assign tasks with different lengths to a fixed number of processors such that the maximum processor runtime is minimal.
Multiplying task lengths with~$-1$, one can turn the minimax problem of \textsc{Multiprocessor Scheduling} into the maximin formulation of the simultaneous-search problem with min-aggregation:
The tasks become features, the negative task lengths become univariate feature qualities, and the processors become feature sets. 
Since \textsc{Multiprocessor Scheduling} is $\mathcal{NP}$-complete, even for just two partitions~\cite{garey2003computers}, our problem is $\mathcal{NP}$-complete as well:
%
\begin{proposition}[Complexity of simultaneous-search problem with min-aggregation, complete partitioning, and unconstrained feature-set size]
	Assuming univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}), a dissimilarity threshold~$\tau = 1$, unconstrained feature-set sizes, and all $n$~features have to be selected, the simultaneous-search problem (cf.~Equation~\ref{eq:afs:afs-simultaneous}) for alternative feature sets with min-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-min-objective}) is $\mathcal{NP}$-complete.
	\label{prop:afs:complexity-partitioning-min-unconstrained-k}
\end{proposition}
%
Since the assumptions in Proposition~\ref{prop:afs:complexity-partitioning-min-unconstrained-k} denote a special case of alternative feature selection, we directly obtain the following, more general proposition:
%
\begin{proposition}[Complexity of simultaneous-search problem with min-aggregation]
	The simultaneous-search problem (cf.~Equation~\ref{eq:afs:afs-simultaneous}) for alternative feature sets with min-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-min-objective}) is $\mathcal{NP}$-hard.
	\label{prop:afs:complexity-simultaneous-np}
\end{proposition}
%
While Proposition~\ref{prop:afs:complexity-partitioning-min-unconstrained-k} allowed arbitrary sets sizes, there are also existing partitioning problems for constrained~$k$, e.g., called \textsc{Balanced Number Partitioning} or \textsc{K-Partitioning}.
\textsc{K-Partitioning} with a minimax objective is $\mathcal{NP}$-hard~\cite{babel1998thek} and
can be transformed into our maximin objective as above:
%
\begin{proposition}[Complexity of simultaneous-search problem with min-aggregation, complete partitioning, and constrained feature-set size]
	Assuming univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}), a dissimilarity threshold~$\tau = 1$, desired feature-set size~$k$, and all $n$~features have to be selected, the simultaneous-search problem (cf.~Equation~\ref{eq:afs:afs-simultaneous}) for alternative feature sets with min-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-min-objective}) is $\mathcal{NP}$-complete.
	\label{prop:afs:complexity-partitioning-min-constrained-k}
\end{proposition}

\paragraph{Min-aggregation with incomplete partitioning}

We now allow that some features may not be part of any feature set while we keep the assumption of zero feature-set overlap.
The problem of finding such an \emph{incomplete partitioning} still is $\mathcal{NP}$-complete in general (cf.~Appendix~\ref{sec:afs:appendix:complexity:proofs} for the proof):
%
\begin{proposition}[Complexity of simultaneous-search problem with min-aggregation, incomplete partitioning, and constrained feature-set size]
	Assuming univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}), a dissimilarity threshold~$\tau = 1$, desired feature-set size~$k$, and \emph{not} all $n$~features have to be selected, the simultaneous-search problem (cf.~Equation~\ref{eq:afs:afs-simultaneous}) for alternative feature sets with min-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-min-objective}) is $\mathcal{NP}$-complete.
	\label{prop:afs:complexity-incomplete-partitioning-min-constrained-k}
\end{proposition}

\paragraph{Min-aggregation with overlapping feature sets}

The problem with $\tau < 1$, i.e., set overlap, also is $\mathcal{NP}$-hard in general (cf.~Appendix~\ref{sec:afs:appendix:complexity:proofs} for the proof):
%
\begin{proposition}[Complexity of simultaneous-search problem with min-aggregation, $\tau < 1$, and constrained feature-set size]
	Assuming univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}), a dissimilarity threshold~$\tau < 1$, and desired feature-set size~$k$, the simultaneous-search problem (cf.~Equation~\ref{eq:afs:afs-simultaneous}) for alternative feature sets with min-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-min-objective}) is $\mathcal{NP}$-hard.
	\label{prop:afs:complexity-no-partitioning-min-constrained-k}
\end{proposition}

\paragraph{Sum-aggregation}

In contrast to the previous $\mathcal{NP}$-hardness results for min-aggregation, sum-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-sum-objective}) with $\tau=1$ admits polynomial-time algorithms (cf.~Appendix~\ref{sec:afs:appendix:complexity:proofs} for the proof):
%
\begin{proposition}[Complexity of search problems with sum-aggregation and $\tau=1$]
	Assuming univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}) and a dissimilarity threshold~$\tau = 1$, the search problem for alternative feature sets with sum-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-sum-objective}) has a time complexity of $O(n)$ for a complete partitioning of $n$~features and $O(n \cdot \log n)$ for an incomplete partitioning.
	\label{prop:afs:complexity-partitioning-sum}
\end{proposition}
%
This feasibility result applies to sequential and simultaneous search, an arbitrary number of alternatives~$a$, and arbitrary feature-set sizes.
The key reason for polynomial runtime is that sum-aggregation does not require balancing the feature sets' qualities.
Thus, $\tau=1$ allows many solutions with the same objective value.
While at least one of these solutions also optimizes the objective with min-aggregation, most do not.
Hence, it is not a contradiction that optimizing with min-aggregation is considerably harder.

\subsubsection{Summary}
\label{sec:afs:approach:complexity:summary}

We showed that the simultaneous-search problem for alternative feature sets is $\mathcal{NP}$-hard in general (cf.~Proposition~\ref{prop:afs:complexity-simultaneous-np}).
We also placed it in the parameterized complexity class $\mathcal{XP}$ (cf.~Proposition~\ref{prop:afs:complexity-simultaneuos-xp}), having~$a$ and~$k$ as the parameters that drive the hardness of the problem.
For univariate feature qualities and min-aggregation, we obtained more specific $\mathcal{NP}$-hardness results for (1) complete partitioning, i.e., $\tau = 1$ and $(a+1) \cdot k = n$ (cf.~Proposition~\ref{prop:afs:complexity-partitioning-min-constrained-k}), (2) incomplete partitioning, i.e., $(a+1) \cdot k < n$ (cf.~Proposition~\ref{prop:afs:complexity-incomplete-partitioning-min-constrained-k}) and (3) feature set overlap, i.e., $\tau < 1$ (cf.~Proposition~\ref{prop:afs:complexity-no-partitioning-min-constrained-k}).
In contrast, we also inferred polynomial runtime for univariate feature qualities, sum-aggregation, and $\tau = 1$ (cf.~Proposition~\ref{prop:afs:complexity-partitioning-sum}).

\subsection{Heuristic Search for Univariate Feature Qualities}
\label{sec:afs:approach:univariate-heuristics}

In this section, we propose heuristic search methods for univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter} and Section~\ref{sec:afs:appendix:univariate-complete-optimization-problem}), complementing the solver-based search that we discussed in Section~\ref{sec:afs:approach:objectives:white-box}.
The proposed heuristics may be faster than exact optimization at the expense of lower feature-set quality.
In particular, we describe \emph{Greedy Replacement Search} (cf.~Section~\ref{sec:afs:approach:univariate-heuristics:greedy-replacement}), which is a sequential search method, and \emph{Greedy Balancing Search} (cf.~Section~\ref{sec:afs:approach:univariate-heuristics:greedy-balancing}), which is a simultaneous search method.
Additionally, Appendix~\ref{sec:afs:appendix:greedy-depth} introduces \emph{Greedy Depth Search}.
All three heuristics leverage that the univariate objective sums up the individual qualities~$q_j$ of selected features and does not consider interactions between features.

\subsubsection{Greedy Replacement Search}
\label{sec:afs:approach:univariate-heuristics:greedy-replacement}

\emph{Greedy Replacement Search} is our first heuristic for alternative feature selection with univariate feature qualities.
This heuristic conducts a sequential search.

\paragraph{Algorithm}

\begin{algorithm}[t]
	\DontPrintSemicolon
	\KwIn{Univariate feature qualities~$q_j$ with $j \in \{1, \dots, n\}$, \newline
		Feature-set size~$k$, \newline
		Number of alternatives~$a$, \newline
		Dissimilarity threshold~$\tau$}
	\KwOut{List of feature-selection decision vectors~$s^{(\cdot)}$}
	\BlankLine
	$indices \leftarrow$ sort\_indices($q$, order=descending) \tcp*{Order by qualities} \label{al:afs:greedy-replacement:line:sorting}
	$s \leftarrow \{0\}^n$ \tcp*{Initial selection for all alternatives} \label{al:afs:greedy-replacement:line:common-features-start}
	$feature\_position \leftarrow 1$ \tcp*{Index of index of current feature}
	\While{$feature\_position \leq \lfloor (1 - \tau) \cdot k \rfloor$}{
		$j \leftarrow indices[feature\_position]$ \tcp*{Index feature by quality}
		$s_j \leftarrow 1$ \;
		$feature\_position \leftarrow feature\_position + 1$\;
	} \label{al:afs:greedy-replacement:line:common-features-end}
	$i \leftarrow 0$\ \tcp*{Number of current alternative} \label{al:afs:greedy-replacement:line:disjoint-features-start}
	\While{$i \leq a$ \textbf{and} $i \leq \frac{n - k}{\lceil \tau \cdot k \rceil}$}{ \label{al:afs:greedy-replacement:line:stop}
		$s^{(i)} \leftarrow s$ \tcp*{Select top $\lfloor (1 - \tau) \cdot k \rfloor$ features} \label{al:afs:greedy-replacement:line:copy-common-selection}
		\For(\tcp*[f]{Select remaining $\lceil \tau \cdot k \rceil$ features}){$\_ \leftarrow 1$ \KwTo $\lceil \tau \cdot k \rceil$}{ \label{al:afs:greedy-replacement:line:one-disjoint-feature-start}
			$j \leftarrow indices[feature\_position]$\;
			$s^{(i)}_j \leftarrow 1$\;
			$feature\_position \leftarrow feature\_position + 1$\; \label{al:afs:greedy-replacement:line:one-disjoint-feature-end}
		}
		$i \leftarrow i + 1$\;
	} \label{al:afs:greedy-replacement:line:disjoint-features-end}
	\Return{$s^{(0)}, \dots, s^{(i)}$}
	\caption{\emph{Greedy Replacement Search} for alternative feature sets.}
	\label{al:afs:greedy-replacement}
\end{algorithm}

Algorithm~\ref{al:afs:greedy-replacement} outlines \emph{Greedy Replacement Search}.
We start by sorting the features decreasingly based on their qualities~$q_j$ (Line~\ref{al:afs:greedy-replacement:line:sorting}).
For a fixed feature-set size~$k$, a dissimilarity threshold~$\tau$, and using the Dice dissimilarity (cf.~Equation~\ref{eq:afs:dice}), one subset with $\lfloor (1 - \tau) \cdot k \rfloor$~features can be contained in all alternatives without violating the dissimilarity threshold (cf.~Equation~\ref{eq:afs:dice-rearranged-equal-size}).
Thus, our algorithms indeed selects the $\lfloor (1 - \tau) \cdot k \rfloor$~features with highest quality in each alternative~$s^{(\cdot)}$ (Lines~\ref{al:afs:greedy-replacement:line:common-features-start}--\ref{al:afs:greedy-replacement:line:common-features-end}).
We fill the remaining spots in the sets by iterating over the alternatives and remaining features (Lines~\ref{al:afs:greedy-replacement:line:disjoint-features-start}--\ref{al:afs:greedy-replacement:line:disjoint-features-end}).
For each alternative, we select the $\lceil \tau \cdot k \rceil$~highest-quality features not used in any prior alternative, thereby satisfying the dissimilarity threshold.
We continue this procedure until we reach the desired number of alternatives~$a$ or until there are not enough unused features to form further alternatives (Line~\ref{al:afs:greedy-replacement:line:stop}).
%
\begin{example}[Algorithm of \emph{Greedy Replacement Search}]
	With $n=10$ features, feature-set size~$k=5$, and $\tau=0.4$, each feature set must differ by $\lceil \tau \cdot k \rceil = 2$ features from the other feature sets.
	The original feature set~$s^{(0)}$ consists of the top $k=5$ features regarding quality~$q_j$.
	The first alternative~$s^{(1)}$ consists of the top $\lfloor (1 - \tau) \cdot k \rfloor = 3$ features plus the sixth- and seventh-best feature.
	The second alternative~$s^{(2)}$ consists of the top three features plus the eighth- and ninth-best one.
	The algorithm has to stop at $i=2$ since there are not enough unused features to form further alternatives in the same manner.
	\label{ex:afs:greedy-replacement:algorithm}
\end{example}
%
In general, $i$-th alternative consists of the top $\lfloor (1 - \tau) \cdot k \rfloor$ features plus the features $k + (i-1) \cdot \lceil \tau \cdot k \rceil + 1$ to $k + i \cdot \lceil \tau \cdot k \rceil$ in descending quality order.

\paragraph{Complexity}

Sorting the qualities of $n$~features (Line~\ref{al:afs:greedy-replacement:line:sorting}) has a complexity of $O(n \cdot \log n)$.
Next, the algorithm iterates over the features and processes each feature at most once.
In particular, after selecting a feature in an alternative, $feature\_position$ increases by~1.
The maximum value of this variable depends on~$a$ and~$k$ (Line~\ref{al:afs:greedy-replacement:line:stop}) but cannot exceed the total number of features~$n$.
For each $feature\_position$, the algorithm accesses the arrays $indices$ and $s^{(i)}$ (Lines~\ref{al:afs:greedy-replacement:line:one-disjoint-feature-start}--\ref{al:afs:greedy-replacement:line:one-disjoint-feature-end}).
Further, each alternative $s^{(i)}$ gets initialized as the selection~$s$ of the top $\lfloor (1 - \tau) \cdot k \rfloor$ features (Line~\ref{al:afs:greedy-replacement:line:copy-common-selection}), which the algorithm only needs to determine once before the main loop (Lines~\ref{al:afs:greedy-replacement:line:common-features-start}--\ref{al:afs:greedy-replacement:line:common-features-end}).
Each of these array operations runs in $O(n)$ or faster.
Combining the cost per $feature\_position$ with the number of $feature\_position$s, the overall time complexity is~$O(n^2)$, i.e., polynomial in~$n$.

\paragraph{Quality}

While not optimizing exactly, \emph{Greedy Replacement Search} still offers an approximation guarantee relative to exact search methods:
%
\begin{proposition}[Approximation quality of \emph{Greedy Replacement Search}]
	Assume non-negative univariate feature qualities of $n$~features (cf.~Equation~\ref{eq:afs:univariate-filter}), $a \in \mathbb{N}_0$~alternatives, a dissimilarity threshold~$\tau$, desired feature-set size~$k$, and $k + a \cdot \lceil \tau \cdot k \rceil \leq n$.
	Under these conditions, \emph{Greedy Replacement Search} reaches at least a fraction of $\frac{\lfloor (1 - \tau) \cdot k \rfloor}{k}$ of the optimal objective values of the optimization problems for (1) sequential search (cf.~Equation~\ref{eq:afs:afs-sequential}), (2) simultaneous search with sum-aggregation (cf.~Equations~\ref{eq:afs:afs-simultaneous} and~\ref{eq:afs:afs-simultaneous-sum-objective}), and (3) simultaneous search with min-aggregation (cf.~Equations~\ref{eq:afs:afs-simultaneous} and~\ref{eq:afs:afs-simultaneous-min-objective}).
	\label{prop:afs:approximation-greedy-replacement}
\end{proposition}

%
\begin{proof}
	In the univariate objective, the quality of a feature set is the sum of the qualities of the contained features.
	\emph{Greedy Replacement Search} includes the $\lfloor (1 - \tau) \cdot k \rfloor$ highest-quality features in each alternative of size~$k$, while the remaining $\lceil \tau \cdot k \rceil$ features may have an arbitrary quality.
	In comparison, the single, i.e., unconstrained, optimal feature set of size~$k$ contains the top $k$ features, which are the union of the top $\lfloor (1 - \tau) \cdot k \rfloor$ features and the next-best $\lceil \tau \cdot k \rceil$ features.
	Due to quality sorting, each of the next-best $\lceil \tau \cdot k \rceil$ features has at most the quality of each of the top $\lfloor (1 - \tau) \cdot k \rfloor$ features.
	Hence, assuming non-negative qualities, each alternative yielded by \emph{Greedy Replacement Search} has at least a quality of $\lfloor (1 - \tau) \cdot k \rfloor / k$ relative to the single optimal feature set of size~$k$.
	Next, the single optimal feature set of size~$k$ upper-bounds the quality of any individual feature set of size~$k$ found by any search method.
	Thus, the bound also applies to the minimum and sum of qualities over feature sets.
\end{proof}
%
In particular, \emph{Greedy Replacement Search} yields a constant-factor approximation for the three optimization problems (cf.~Equation~\ref{eq:afs:afs-sequential} and~\ref{eq:afs:afs-simultaneous}) mentioned in Proposition~\ref{prop:afs:approximation-greedy-replacement}.
The condition $k + a \cdot \lceil \tau \cdot k \rceil \leq n$ describes scenarios where \emph{Greedy Replacement Search} can yield all desired alternatives, i.e., does not run out of unused features.
As the heuristic has polynomial runtime, alternative feature selection lies in the complexity class $\mathcal{APX}$~\cite{khanna1998syntactic} under the specified conditions:
%
\begin{proposition}[Approximation complexity of alternative feature selection]
	Assume non-negative univariate feature qualities of $n$~features (cf.~Equation~\ref{eq:afs:univariate-filter}), $a \in \mathbb{N}_0$~alternatives, a dissimilarity threshold~$\tau$, desired feature-set size~$k$, and $k + a \cdot \lceil \tau \cdot k \rceil \leq n$.
	Under these conditions, the optimization problems of alternative feature selection with (1) sequential search (cf.~Equation~\ref{eq:afs:afs-sequential}), (2) simultaneous search with sum-aggregation (cf.~Equations~\ref{eq:afs:afs-simultaneous} and~\ref{eq:afs:afs-simultaneous-sum-objective}), and (3) simultaneous search with min-aggregation (cf.~Equations~\ref{eq:afs:afs-simultaneous} and~\ref{eq:afs:afs-simultaneous-min-objective}) reside in the complexity class~$\mathcal{APX}$.
	\label{prop:afs:approximation-apx}
\end{proposition}
%
For~$\tau = 1$, \emph{Greedy Replacement Search} even yields the same objective values as exact sequential search and exact simultaneous search with sum-aggregation since it becomes identical to a procedure we outlined in our complexity analysis earlier (cf.~Proposition~\ref{prop:afs:complexity-partitioning-sum}).
In contrast, the following example shows that the heuristic can be worse than exact sequential search for as few as $a=2$ alternatives:
%
\begin{example}[Quality of \emph{Greedy Replacement Search} vs. exact search]
	Consider $n=6$~features with univariate feature qualities $q = (9,8,7,3,2,1)$, feature-set size~$k=2$, number of alternatives~$a=2$, and dissimilarity threshold~$\tau = 0.5$, which permits an overlap of one feature between sets here.
	Exact sequential search and exact simultaneous search, for min- and sum-aggregation, yield the selection $s^{(0)} = (1,1,0,0,0,0)$, $s^{(1)} = (1,0,1,0,0,0)$, and $s^{(2)} = (0,1,1,0,0,0)$, with a summed quality of $17+16+15=48$.
	\emph{Greedy Replacement Search} yields the selection $s^{(0)} = (1,1,0,0,0,0)$, $s^{(1)} = (1,0,1,0,0,0)$, and $s^{(2)} = (1,0,0,1,0,0)$, with a summed quality of $17+16+12=45$.
	\label{ex:afs:greedy-replacement:worse-than-exact}
\end{example}
%
While the first two feature sets are identical between exact and heuristic search, the quality of $s^{(2)}$ is lower for the heuristic (12 vs. 15).
In particular, by always selecting the top $\lfloor (1 - \tau) \cdot k \rfloor$~features, the heuristic misses out on feature sets only involving the next-best features.

For min-aggregation in the objective, $a=1$ alternative already suffices such that the heuristic may be worse than exact search:
%
\begin{example}[Quality of \emph{Greedy Replacement Search} vs. min-aggregation]
	Consider $n=6$~features with univariate feature qualities $q = (9,8,7,3,2,1)$, feature-set size~$k=3$, number of alternatives~$a=1$, and dissimilarity threshold~$\tau = 0.5$, which permits an overlap of one feature between sets here.
	Exact simultaneous search with min-aggregation yields the selection $s^{(0)} = (1,1,0,0,1,0)$ and $s^{(1)} = (1,0,1,1,0,0)$, with a quality of $\min \{19,19\} = 19$.
	\emph{Greedy Replacement Search} and exact sequential search yield the selection $s^{(0)} = (1,1,1,0,0,0)$ and $s^{(1)} = (1,0,0,1,1,0)$, with a quality of $\min \{24,14\} = 14$.
	Exact simultaneous search with sum-aggregation may yield either of these two solutions or the selection $s^{(0)} = (1,1,0,1,0,0)$ and $s^{(1)} = (1,0,1,0,1,0)$ with the same summed quality.
	\label{ex:afs:greedy-replacement:worse-than-min-agg}
\end{example}
%
In particular, \emph{Greedy Replacement Search} does not balance feature-set qualities since it is a sequential search method.
We alleviate this issue with the heuristic~\emph{Greedy Balancing Search} (cf.~Section~\ref{sec:afs:approach:univariate-heuristics:greedy-balancing}).

\paragraph{Limitations}

Proposition~\ref{prop:afs:approximation-greedy-replacement} and Examples~\ref{ex:afs:greedy-replacement:worse-than-exact},~\ref{ex:afs:greedy-replacement:worse-than-min-agg} already showed the potential quality loss of the heuristic compared to an exact search for alternatives.
Further, \emph{Greedy Replacement Search} only works as long as some features have not been part of any feature set yet, i.e., $k + a \cdot \lceil \tau \cdot k \rceil \leq n$.
Once the heuristic runs out of unused features, one would need to switch the search method.
Thus, to obtain a high number of alternatives~$a$, the following conditions are beneficial for the heuristic:
The number of features~$n$ should be high, the feature-set size~$k$ show be low, and the dissimilarity threshold~$\tau$ should be low.
These conditions align well with typical feature-selection scenarios where~$k \ll n$.

Another drawback is that \emph{Greedy Replacement Search} assumes a very simple structure of the optimization problem.
If the objective function becomes more complex than a sum of univariate qualities, quality-based feature ordering may be impossible or suboptimal.
Further, \emph{Greedy Replacement Search} cannot accommodate additional constraints on feature sets, e.g., based on domain knowledge.
Finally, the heuristic assumes the same size~$k$ for all feature sets.

\subsubsection{Greedy Balancing Search}
\label{sec:afs:approach:univariate-heuristics:greedy-balancing}

\emph{Greed Balancing Search} modifies \emph{Greedy Replacement Search} to obtain more balanced feature-set qualities with a simultaneous search method.

\begin{algorithm}[tp]
	\DontPrintSemicolon
	\KwIn{Univariate feature qualities~$q_j$ with $j \in \{1, \dots, n\}$, \newline
		Feature-set size~$k$, \newline
		Number of alternatives~$a$, \newline
		Dissimilarity threshold~$\tau$}
	\KwOut{List of feature-selection decision vectors~$s^{(0)}, \dots, s^{(a)}$}
	\BlankLine
	\If{$\lceil \tau \cdot k \rceil \cdot a + k > n$}{ \label{al:afs:greedy-balancing:line:stop-early}
		\Return{$\emptyset$}
	}
	$indices \leftarrow$ sort\_indices($q$, order=descending) \tcp*{Order by qualities} \label{al:afs:greedy-balancing:line:sorting} \label{al:afs:greedy-balancing:line:common-features-start}
	\For(\tcp*[f]{Initial selection for all alternatives}){$i \leftarrow 0$ \KwTo $a$}{
		$s^{(i)} \leftarrow \{0\}^n$ \;
	}
	$feature\_position \leftarrow 1$ \tcp*{Index of index of current feature}
	\While(\tcp*[f]{Select top features}){$feature\_position \leq \lfloor (1 - \tau) \cdot k \rfloor$}{
		$j \leftarrow indices[feature\_position]$ \tcp*{Index feature by quality}
		\For(\tcp*[f]{Same features in all alternatives}){$i \leftarrow 0$ \KwTo $a$}{
			$s^{(i)}_j \leftarrow 1$ \;
		}
		$feature\_position \leftarrow feature\_position + 1$\;
	} \label{al:afs:greedy-balancing:line:common-features-end}
	\For{$i \leftarrow 0$ \KwTo $a$}{ \label{al:afs:greedy-balancing:line:disjoint-features-start}
		$Q^{(i)} \leftarrow 0$\ \tcp*{Relative quality of each alternative}
	}
	\While(\tcp*[f]{Fill all positions}){$feature\_position \leq \lceil \tau \cdot k \rceil \cdot a + k$}{ \label{al:afs:greedy-balancing:line:stop}
		$Q_\text{min} \leftarrow \infty$ \tcp*{Find alternative with lowest quality}
		$i_\text{min} \leftarrow -1$ \;
		\For{$i \leftarrow 0$ \KwTo $a$}{
			\If(\tcp*[f]{Check cardinality}){$Q^{(i)} < Q_\text{min}$ \textbf{and} $\sum_{j=1}^{n} s^{(i)}_j < k$}{
				$Q_\text{min} \leftarrow Q^{(i)}$ \;
				$i_\text{min} \leftarrow i$ \;
			}
		}
		$j \leftarrow indices[feature\_position]$ \tcp*{Index feature by quality}
		$s^{(i_\text{min})}_j \leftarrow 1$ \tcp*{Add to lowest-quality, non-full alternative}
		$Q^{(i_\text{min})} \leftarrow Q^{(i_\text{min})} + q_j$ \tcp*{Update quality of that alternative}
		$feature\_position \leftarrow feature\_position + 1$\;
	} \label{al:afs:greedy-balancing:line:disjoint-features-end}
	\Return{$s^{(0)}, \dots, s^{(a)}$}
	\caption{\emph{Greedy Balancing Search} for alternative feature sets.}
	\label{al:afs:greedy-balancing}
\end{algorithm}

\paragraph{Algorithm}

Algorithm~\ref{al:afs:greedy-balancing} outlines \emph{Greedy Balancing Search}.
First, we check whether the algorithm should terminate early, i.e., whether the number of features~$n$ is not high enough to satisfy the desired user parameters~$k$, $a$, and~$\tau$ (Line~\ref{al:afs:greedy-balancing:line:stop-early}).
Next, we select the first $\lfloor (1 - \tau) \cdot k \rfloor$ features in each alternative like in \emph{Greedy Replacement Search} (cf.~Algorithm~\ref{al:afs:greedy-replacement}), i.e., we pick the features with the highest quality~$q_j$ (Lines~\ref{al:afs:greedy-balancing:line:common-features-start}--\ref{al:afs:greedy-balancing:line:common-features-end}).

For the remaining spots in the alternatives, we use a Longest Processing Time (LPT) heuristic (Lines~\ref{al:afs:greedy-balancing:line:disjoint-features-start}--\ref{al:afs:greedy-balancing:line:disjoint-features-end}).
Such heuristics are common for \textsc{Multiprocessor Scheduling} and \textsc{Balanced Number Partitioning} problems~\cite{babel1998thek, chen20023partitioning, lawrinenko2018reduction} (cf.~Section~\ref{sec:afs:appendix:complexity:related-work}).
In particular, we continue iterating over features by decreasing quality.
We assign each feature to the alternative that currently has the lowest summed quality~$Q^{(i)}$ and whose size~$k$ has not been reached yet.
We continue this procedure until all alternatives have reached size~$k$ (Line~\ref{al:afs:greedy-balancing:line:stop}).
%
\begin{example}[Algorithm of \emph{Greedy Balancing Search}]
	Consider $n=6$~features with univariate feature qualities $q = (9,8,7,3,2,1)$, feature-set size~$k=4$, number of alternatives~$a=1$, and dissimilarity threshold~$\tau = 0.5$, which permits an overlap of two features between sets here.
	The features with qualities~$9$ and $8$ become part of both feature sets, $s^{(0)}$ and $s^{(1)}$ (Lines~\ref{al:afs:greedy-balancing:line:common-features-start}--\ref{al:afs:greedy-balancing:line:common-features-end}).
	At this point, both alternatives have the same relative quality $Q^{(0)} = Q^{(1)} = 0$, i.e., $Q^{(i)}$ in the algorithm ignores the quality of the shared features.
	Now the LPT heuristic becomes active (Lines~\ref{al:afs:greedy-balancing:line:disjoint-features-start}--\ref{al:afs:greedy-balancing:line:disjoint-features-end}).
	The feature with quality~$7$ is added to $s^{(0)}$, which causes $Q^{(0)} > Q^{(1)}$ (i.e., $7 > 0$).
	Thus, the feature with quality~3 is added to $s^{(1)}$.
	As $Q^{(0)} > Q^{(1)}$ (i.e., $7 > 3$) still holds, the feature with quality~2 becomes part of $s^{(1)}$ as well.
	Because $s^{(1)}$ has reached size~$k = 4$, the feature with quality~1 is added to $s^{(0)}$, even if the latter still has a higher quality (i.e., $7 > 5$).
	Now both alternatives have reached their desired size and $n = 6 = \lceil 0.5 \cdot 4 \rceil \cdot 1 + 4 = \lceil \tau \cdot k \rceil \cdot a + k$ (Line~\ref{al:afs:greedy-balancing:line:stop}).
	Thus, the algorithm terminates.
	The solution consists of $s^{(0)} = (1,1,1,0,0,1)$ and $s^{(1)} = (1,1,0,1,1,0)$.
	\label{ex:afs:greedy-balancing:algorithm}
\end{example}

\paragraph{Complexity}

Like \emph{Greedy Replacement Search}, \emph{Greedy Balancing Search} has an upfront cost of $O(n \cdot \log n)$ for sorting feature qualities (Line~\ref{al:afs:greedy-balancing:line:sorting}) and then iterates over $O(n)$ $feature\_position$s.
For each $feature\_position$, the algorithm iterates over $a$~alternatives and conducts a fixed number of array operations in $O(n)$.
Thus, the overall complexity of \emph{Greedy Balancing Search} is $O(a \cdot n^2)$.

\paragraph{Quality}

\emph{Greed Balancing Search} selects the same features as \emph{Greedy Replacement Search} and only changes their assignment to the feature sets.
Thus, the summed feature-set quality remains the same, while the minimum feature-set quality may be higher due to balancing.
Hence, the quality guarantee of \emph{Greedy Replacement Search} (cf.~Proposition~\ref{prop:afs:approximation-greedy-replacement}) holds here as well:
%
\begin{proposition}[Approximation quality of \emph{Greedy Balancing Search}]
	Assume non-negative univariate feature qualities of $n$~features (cf.~Equation~\ref{eq:afs:univariate-filter}), $a \in \mathbb{N}_0$~alternatives, a dissimilarity threshold~$\tau$, desired feature-set size~$k$, and $k + a \cdot \lceil \tau \cdot k \rceil \leq n$.
	Under these conditions, \emph{Greedy Balancing Search} reaches at least a fraction of $\frac{\lfloor (1 - \tau) \cdot k \rfloor}{k}$ of the optimal objective values of the optimization problems for (1) sequential search (cf.~Equation~\ref{eq:afs:afs-sequential}), (2) simultaneous search with sum-aggregation (cf.~Equations~\ref{eq:afs:afs-simultaneous} and~\ref{eq:afs:afs-simultaneous-sum-objective}), and (3) simultaneous search with min-aggregation (cf.~Equations~\ref{eq:afs:afs-simultaneous} and~\ref{eq:afs:afs-simultaneous-min-objective}).
	\label{prop:afs:approximation-greedy-balancing}
\end{proposition}
%
For min-aggregation in the objective, \emph{Greedy Balancing Search} can even be better than exact sequential search, as Example~\ref{ex:afs:greedy-replacement:worse-than-min-agg} shows, where the heuristic yields the same solution as exact simultaneous search with min-aggregation.
However, the heuristic can also be worse than exact sequential search and exact simultaneous search, as Example~\ref{ex:afs:greedy-replacement:worse-than-exact} shows, where \emph{Greedy Balancing Search} would yield the same solution as \emph{Greedy Replacement Search}.

\paragraph{Limitations}

\emph{Greedy Balancing Search} shares several limitations with \emph{Greedy Replacement Search}, e.g., it may be worse than exact search, assumes univariate feature qualities, and does not work if the number of features~$n$ is too low relative to~$k$, $a$, and $\tau$.
In the latter case, \emph{Greedy Balancing Search} yields no solution due to its simultaneous nature, while \emph{Greedy Replacement Search} yields at least some alternatives.
However, if running out of features is not an issue, \emph{Greedy Balancing Search} has the advantage of more balanced feature-set qualities.

\section{Related Work}
\label{sec:afs:related-work}

In this section, we review related work from the fields of feature selection (cf.~Section~\ref{sec:afs:related-work:feature-selection}), subgroup discovery (cf.~Section~\ref{sec:afs:related-work:subgroup-discovery}), clustering (cf.~Section~\ref{sec:afs:related-work:clustering}), subspace clustering and subspace search (cf.~Section~\ref{sec:afs:related-work:subspace}), explainable artificial intelligence (cf.~Section~\ref{sec:afs:related-work:xai}),
and Rashomon sets (cf.~Section~\ref{sec:afs-related-work:rashomon-sets}).
To the best of our knowledge, searching for optimal alternative feature sets in the sense of this paper is novel.
However, there is literature on optimal alternatives outside the field of feature selection.
Also, there are works on finding multiple, diverse feature sets.

\subsection{Feature Selection}
\label{sec:afs:related-work:feature-selection}

\paragraph{Conventional feature selection}

Most feature-selection methods only yield one solution~\cite{borboudakis2021extending}, though some exceptions exist.
Nevertheless, none of the following approaches searches for optimal alternatives in our sense.

\cite{siddiqi2020genetic}~proposes a genetic algorithm that iteratively updates a population of multiple feature sets.
To foster diversity, the algorithm's fitness criterion does not only consider feature-set quality but also a penalty on feature-set overlap in the population.
However, users cannot control the admissible overlap, i.e., there is no parameter comparable to~$\tau$.
In contrast, the genetic algorithm's parameter for the population size corresponds to the number of alternatives.

\cite{emmanouilidis1999selecting}~employs multi-objective genetic algorithms to obtain prediction models with different complexity and diverse feature sets.
However, the two objectives are prediction performance and feature-set size, while diversity only influences the genetic selection step under particular circumstances.

\cite{mueller2021feature}~clusters features and forms alternatives by picking one feature from each cluster.
However, they do this to reduce the number of features for subsequent model selection and model evaluation, not as a guided search for alternatives.

\paragraph{Ensemble feature selection}

Ensemble feature selection~\cite{saeys2008robust, seijo2017ensemble} combines feature-selection results, e.g., obtained by different feature-selection methods or on different samples of the data.
Fostering diverse feature sets might be a sub-goal to improve prediction performance, but it is usually only an intermediate step.
This focus differs from our goal of finding optimal alternatives.

\cite{woznica2012model}~obtains feature sets or rankings on bootstrap samples of the data.
Next, an aggregation strategy creates one or multiple diverse feature sets.
The authors propose using k-medoid clustering and frequent itemset mining for the latter.
While these approaches allow to control the number of feature sets, there is no parameter for their dissimilarity.
Also, aggregation builds on bootstrap sampling instead of being allowed to form arbitrary alternatives.

\cite{liu2019subspace}~builds an ensemble prediction model from classifiers trained on different feature sets.
To this end, a genetic algorithm iteratively evolves a population of feature sets.
Diversity is one of multiple fitness criteria, with the Hamming distances quantifying the dissimilarity of feature sets.
However, since feature diversity is only one of several objectives, users cannot control it directly.

\cite{guru2018alternative}~computes feature relevance separately for each class and then combines the top features.
This procedure can yield alternatives but does not enforce dissimilarity.
Also, the number of alternatives is fixed to the number of classes.

\paragraph{Statistically equivalent feature sets}

Approaches for statistically equivalent feature sets~\cite{borboudakis2021extending, lagani2017feature} use statistical tests to determine features or feature sets that are equivalent for predictions.
E.g., a feature may be independent of the target given another feature.
A search algorithm conducts multiple such tests and outputs equivalent feature sets or a corresponding feature grouping.

Our notion of alternatives differs from equivalent feature sets in several aspects.
In particular, building optimal alternatives from equivalent feature sets is not straightforward.
Depending on how the statistical tests are configured, there can be an arbitrary number of equivalent feature sets without explicit quality-based ordering.
Instead, we always provide a fixed number of alternatives.
Also, our alternatives need not have equivalent quality but should be optimal under constraints.
Further, our dissimilarity threshold allows controlling overlap between feature sets instead of eliminating all redundancies.

\paragraph{Constrained feature selection}

We define alternatives via constraints on feature sets.
There already is work on other kinds of constraints in feature selection, e.g., for feature cost~\cite{paclik2002feature}, feature groups~\cite{yuan2006model}, or domain knowledge~\cite{bach2022empirical, groves2015toward}.
These approaches are orthogonal to our work, as such constraints do not explicitly foster optimal alternatives.
At most, they might implicitly lead to alternative solutions~\cite{bach2022empirical}.
Further, most of the approaches are tied to particular constraint types, while our integer-programming formulation also supports such constraints besides the ones for alternatives.
\cite{bach2022empirical} is an exception in that regard since it models feature selection as a Satisfiability Modulo Theories (\textsc{SMT}) optimization problem, which admits our constraints for alternatives as well.

\subsection{Subgroup Discovery}
\label{sec:afs:related-work:subgroup-discovery}

\cite{leeuwen2012diverse}~presents six strategies to foster diversity in subgroup set discovery, which searches for interesting regions in the data space, i.e., combinations of conditions on feature values, rather than only selecting features.
Three strategies yield a fixed number of alternatives, and the other three a variable number.
The strategies become part of beam search, i.e., a heuristic search procedure, while we mainly consider exact optimization.
Also, the criteria for alternatives differ from ours.
The strategy \emph{fixed-size description-based selection} prunes subgroups with the same quality as previously found ones if they differ by at most one feature-value condition.
In contrast, we require dissimilarity independent from the quality, have a flexible dissimilarity threshold, and support simultaneous besides sequential search for alternatives.
Another strategy, \emph{variable-size description-based selection}, limits the total number of subgroups a feature may occur in but does not constrain subgroup overlap per se.
The four remaining strategies in~\cite{leeuwen2012diverse} have no obvious counterpart in our feature-selection scenario.

\subsection{Clustering}
\label{sec:afs:related-work:clustering}

Finding alternative solutions has been addressed extensively in the field of clustering.
\cite{bailey2014alternative} gives a taxonomy and describes algorithms for alternative clustering.
Our problem definition in Sections~\ref{sec:afs:approach:problem} and~\ref{sec:afs:approach:constraints} is, on a high level, inspired by the one in~\cite{bailey2014alternative}:
Find multiple solutions that maximize quality while minimizing similarity.
\cite{bailey2014alternative} also distinguishes between singular/multiple alternatives and sequential/simultaneous search.
They mention constraint-based search for alternatives as one of several solution paradigms.
Further, feature selection can help to find alternative clusterings~\cite{tao2012novel}.
Nevertheless, the problem definition for alternatives in clustering and feature selection is fundamentally different.
First, the notion of dissimilarity differs, as we want to find differently composed feature sets while alternative clustering targets at different assignments of data objects to clusters.
Second, our objective function, i.e., feature-set quality, relates to a supervised prediction scenario while clustering is unsupervised.

Two exemplary approaches for alternative clustering are \emph{COALA}~\cite{bae2006coala} and \emph{MAXIMUS}~\cite{bae2010clustering}.
COALA~\cite{bae2006coala} imposes \emph{cannot-link constraints} on pairs of data objects rather than constraining features:
Data objects from the same cluster in the original clustering should be assigned to different clusters in the alternative clustering.
In each step of its iterative clustering procedure, COALA compares the quality of an action observing the constraints to another one violating them.
Based on a threshold on the quality ratio, either action is taken.
MAXIMUS~\cite{bae2010clustering} employs an integer program to formulate dissimilarity between clusterings.
In particular, it wants to maximize the dissimilarity of the feature-value distributions in clusters between the clusterings.
The output of the integer program leads to constraints for a subsequent clustering procedure.

\subsection{Subspace Clustering and Subspace Search}
\label{sec:afs:related-work:subspace}

Finding multiple useful feature sets plays a role in subspace clustering~\cite{hu2018subspace, mueller2009relevant} and subspace search~\cite{fouche2021efficient, nguyen20134s, trittenbach2019dimension}.
These approaches strive to improve the results of data-mining algorithms by using subspaces, i.e., feature sets, rather than the full space, i.e., all features.
While some subspace approaches only consider individual subspaces, others explicitly try to remove redundancy between subspaces~\cite{mueller2009relevant, nguyen20134s} or foster subspace diversity~\cite{fouche2021efficient, trittenbach2019dimension}.
In particular, \cite{hu2018subspace} surveys subspace-clustering approaches yielding multiple results and discusses the redundancy aspect.
However, subspace clustering and -search approaches differ from alternative feature selection in at least one of the following aspects:

First, the objective differs, i.e., definitions of subspace quality deviate from feature-set quality in our scenario.
Second, definitions of subspace redundancy may consider dissimilarity between projections of the entire data, i.e., data objects with feature values, into subspaces, while our notion of dissimilarity purely bases on binary feature-selection decisions.
Third, controlling dissimilarity in subspace approaches is often less user-friendly than with our parameter~$\tau$.
E.g., dissimilarity might be a regularization term in the objective rather than a hard constraint, or there might not be an explicit control parameter at all.

\subsection{Explainable Artificial Intelligence (XAI)}
\label{sec:afs:related-work:xai}

In the field of XAI, alternative explanations might provide additional insights into predictions, enable users to develop and test different hypotheses, appeal to different kinds of users, and foster trust in the predictions~\cite{kim2021multi, wang2019designing}.
In contrast, obtaining significantly different explanations for the same prediction might raise doubts about how meaningful the explanations are~\cite{jain2019attention}.
Finding diverse explanations had been studied for various explainers, e.g., for counterfactuals~\cite{dandl2020multi, karimi2020model, mohammadi2021scaling, mothilal2020explaining, russell2019efficient, wachter2017counterfactual}, criticisms~\cite{kim2016examples}, and semifactual explanations~\cite{artelt2022even}.
There are several approaches to foster diversity, e.g., ensembling different kinds of explanations~\cite{silva2019produce}, considering multiple local minima~\cite{wachter2017counterfactual}, using a search algorithm that maintains diversity~\cite{dandl2020multi}, extending the optimization objective~\cite{artelt2022even, kim2016examples, mothilal2020explaining}, or introducing constraints~\cite{karimi2020model, mohammadi2021scaling, russell2019efficient}.
The last option is similar to the way we enforce alternatives.
Of the various mentioned approaches, only~\cite{artelt2022even, mohammadi2021scaling, mothilal2020explaining} introduce a parameter to control the diversity of solutions.
Of these three works, only~\cite{mohammadi2021scaling} offers a user-friendly dissimilarity threshold in~$[0,1]$, while the other two approaches employ a regularization parameter in the objective.

Despite similarities, all the previously mentioned XAI techniques tackle different problems than alternative feature selection.
In particular, they provide local explanations, i.e., target at prediction outcomes for individual data objects and build on feature values.
In contrast, we are interested in the global prediction quality of feature sets.
For example, counterfactual explanations~\cite{guidotti2022counterfactual, stepin2021survey, verma2020counterfactual} alter feature \emph{values} \emph{as little as possible} to produce an alternative prediction \emph{outcome}.
In contrast, alternative feature sets might alter the feature \emph{selection} \emph{significantly} while trying to maintain the original prediction \emph{quality}.

\subsection{Rashomon Sets}
\label{sec:afs-related-work:rashomon-sets}

A Rashomon set is a set of prediction models that reach a certain, e.g., close-to-optimal, prediction performance~\cite{fisher2019all}.
Despite similar performance, these models may still assign different feature importance scores, leading to different explanations~\cite{laberge2023partial}.
Thus, Rashomon sets may yield partial information about alternative feature sets.
However, approaches for Rashomon sets do not explicitly search for alternative feature sets as a whole, i.e., feature sets satisfying a dissimilarity threshold relative to other sets.
Instead, these approaches focus on the range of each feature's importance over prediction models.
Further, our notion of alternatives is not bound to model-based feature importance but encompasses a broader range of feature-selection methods.
Finally, we use importance scores from one instead of multiple models to find importance-based alternatives.

\section{Experimental Design}
\label{sec:afs:experimental-design}

In this section, we describe our experimental design.
We give a brief overview of its goal and components (cf.~Section~\ref{sec:afs:experimental-design:overview}) before elaborating on the components in detail.
In particular, we describe evaluation metrics (cf.~Section~\ref{sec:afs:experimental-design:evaluation}), methods (cf.~Section~\ref{sec:afs:experimental-design:approaches}), datasets (cf.~Section~\ref{sec:afs:experimental-design:datasets}), and implementation (cf.~Section~\ref{sec:afs:experimental-design:implementation}).

\subsection{Overview}
\label{sec:afs:experimental-design:overview}

We conduct experiments with 30 binary-classification datasets.
As evaluation metrics, we consider feature-set quality and runtime.
We compare five feature-selection methods, representing different notions of feature-set quality.
Also, we train prediction models with the resulting feature sets and analyze prediction performance.
To find alternatives, we consider simultaneous as well as sequential search, both with solver-based and heuristic search methods.
We systematically vary the number of alternatives~$a$ and the dissimilarity threshold~$\tau$.

\subsection{Evaluation Metrics}
\label{sec:afs:experimental-design:evaluation}

\paragraph{Feature-set quality}

We evaluate feature-set quality with two metrics.
First, we report the \emph{objective value}~$Q(s,X,y)$ of the feature-selection methods, which guided the search for alternatives.
Second, we train prediction models with the found feature sets.
We report \emph{prediction performance} in terms of the Matthews correlation coefficient (MCC)~\cite{matthews1975comparison}.
This coefficient is insensitive to class imbalance, reaches its maximum of~1 for perfect predictions, and is~0 for random guessing as well as constant predictions.

To analyze how well feature selection and prediction models generalize, we conduct a stratified five-fold cross-validation.
The search for alternatives and model training only have access to the training data.
However, we also use the test data to evaluate the quality of each feature set found with the training data.
For the test-set objective value, we initialize the objective function with feature qualities computed on the test set but insert the feature selection from the training set.
For the test-set prediction performance, we predict on the test set but use a prediction model trained with these features on the training set.

\paragraph{Runtime}

We consider two metrics related to runtime.

First, we analyze the \emph{optimization time}.
For white-box feature-selection methods, we measure the total runtime of solver calls.
We exclude the time for computing feature qualities and feature dependencies for the objective since one can compute these values once per dataset and then re-use them in each solver call.
For \emph{Greedy Wrapper}, we measure the runtime of the entire black-box optimization procedure involving multiple solver calls and model trainings.

Second, we examine the \emph{optimization status}, which can take four values.
If the solver finished before reaching a timeout, it either found an \emph{optimal} solution or proved the problem \emph{infeasible}, i.e., no solution exists.
If the solver reached its timeout, it either found a \emph{feasible} solution whose optimality it could not prove or found no valid solution though one might exist, so the problem is \emph{not solved}.

\subsection{Methods}
\label{sec:afs:experimental-design:approaches}

We compare several approaches for making predictions (cf.~Section~\ref{sec:afs:experimental-design:approaches:prediction}), feature selection (cf.~Section~\ref{sec:afs:experimental-design:approaches:feature-selection}), and searching alternatives (cf.~Section~\ref{sec:afs:experimental-design:approaches:alternatives}).

\subsubsection{Prediction}
\label{sec:afs:experimental-design:approaches:prediction}

As prediction models, we use decision trees~\cite{breiman1984classification} and random forests with 100 trees \cite{breiman2001random}.
Both these models admit learning complex, non-linear dependencies from the data.
We leave the hyperparameters of the models at their defaults, except for using information gain instead of Gini impurity as the split criterion, to be consistent with our parametrization of filter feature-selection methods.
Preliminary experiments with a k-nearest neighbors classifier yielded similar insights paired with a lower average prediction performance.

Note that tree models also carry out feature selection themselves, i.e., they are embedded approaches.
Thus, they might not use all features from the alternative feature sets.
However, this is not a problem for our study.
We are interested in which performance the models achieve if they are limited to certain feature sets, not if and how they use each feature from these sets.

\subsubsection{Feature Selection (Objective Functions)}
\label{sec:afs:experimental-design:approaches:feature-selection}

We search for alternatives under different notions of feature-set quality as the objective function.
We choose five well-known feature-selection methods that are easy to parameterize and cover the different categories from Section~\ref{sec:afs:fundamentals:quality} except \emph{embedded}, as explained in Section~\ref{sec:afs:approach:objectives:embedding}.
However, we use feature importance from an embedded method, i.e., decision trees, as post-hoc importance scores.

One method (\emph{Greedy Wrapper}) requires black-box optimization, while the other four are white-box.
With each feature-selection method, we select $k \in \{5,10\}$ features, thereby obtaining small feature sets.
We enforce the desired $k$ with a simple constraint in optimization, using the feature-set-size expression from Equation~\ref{eq:afs:feature-set-size}.

\paragraph{Filter feature selection}

We evaluate three filter methods, all using mutual information~\cite{kraskov2004estimating} as the dependency measure~$q(\cdot)$.
This measure allows to capture arbitrary dependencies rather than, e.g., just linear correlations.
\emph{MI} denotes a univariate filter (cf.~Equation~\ref{eq:afs:univariate-filter}), while \emph{FCBF} (cf.~Equation~\ref{eq:afs:fcbf}) and \emph{mRMR} (cf.~Equation~\ref{eq:afs:mrmr-linear}) are multivariate.
Since mutual information has no fixed upper bound, we normalize its value per dataset and cross-validation fold to improve the comparability of feature-set quality.
For \emph{FCBF} and \emph{MI}, we normalize the individual features' qualities such that selecting all features yields a quality of~1 and selecting no feature yields a quality of~0.
For \emph{mRMR}, we min-max-normalize all mutual-information values to $[0,1]$, so the overall objective is in $[-1,1]$.

\paragraph{Wrapper feature selection}

As a wrapper method, we employ our hill-climbing procedure \emph{Greedy Wrapper} (cf.~Algorithm~\ref{al:afs:greedy-wrapper}).
We set $max\_iters$ to 1000.
To evaluate feature-set quality within the wrapper, we apply a stratified 80:20 holdout split and train decision trees.
$Q(s,X,y)$ corresponds to the prediction performance in terms of MCC on the 20\% validation part.

\paragraph{Post-hoc feature importance}

As a post-hoc importance measure called \emph{Model Gain}, we use importance scores from \emph{scikit-learn's} decision trees.
There, importance expresses a feature's contribution towards optimizing the split criterion of the tree, for which we choose information gain.
These importances are normalized to sum up to~1 by default.
We plug the importances into Equation~\ref{eq:afs:univariate-filter}, i.e., treat them like univariate filter scores.
The interpretation is different, though, since the scores originate from trees trained with all features rather than assessing features in isolation.

\subsubsection{Alternatives (Constraints)}
\label{sec:afs:experimental-design:approaches:alternatives}

\paragraph{Overview}

In our evaluation, we categorize search methods for alternatives in two dimensions that are orthogonal to each other:
Solver-based vs. heuristic and sequential vs. simultaneous.
Also, we analyze the impact of the user parameters~$a$ and~$\tau$ on the search for alternatives.

\paragraph{Solver-based search methods}

In Sections~\ref{sec:afs:evaluation:solver-search-methods}, \ref{sec:afs:evaluation:num-alternatives}, and~\ref{sec:afs:evaluation:tau}, we use solver-based search methods to find alternative feature sets.
For the four white-box feature-selection methods, we use the solver to exactly solve the underlying optimization problems.
Thus, given sufficient solving time, these alternatives are globally optimal.
For \emph{Greedy Wrapper} as the feature-selection method, the search procedure (Algorithm~\ref{al:afs:greedy-wrapper}) is heuristic (though still solver-based) and might not cover the entire search space.
There, the solver only assists in finding valid solutions but does not optimize.

For each feature selection method, we analyze \emph{sequential} (cf.~Equation~\ref{eq:afs:afs-sequential}) and \emph{simultaneous} (cf.~Equation~\ref{eq:afs:afs-simultaneous}) solver-base search for alternatives.
For the latter, we employ sum-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-sum-objective}) and min-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-min-objective}) in the objective.
In figures and tables, we use the abbreviations \emph{seq.}, \emph{sim. (sum)}, and \emph{sim. (min)} to denote these solver-based search methods.

\paragraph{Heuristic search methods}

In Section~\ref{sec:afs:evaluation:heuristic-search-methods}, we additionally compare solver-free heuristic search methods (cf.~Section~\ref{sec:afs:approach:univariate-heuristics}).
In particular, we employ \emph{Greedy Replacement} (cf.~Algorithm~\ref{al:afs:greedy-replacement}), which is a sequential search method, and \emph{Greedy Balancing} (cf.~Algorithm~\ref{al:afs:greedy-balancing}), which is a simultaneous search method.
In figures and tables, we use the abbreviations \emph{rep.} and \emph{bal.} to denote these heuristic search methods.
Since these heuristics assume univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}), we only combine them with the univariate feature-selection methods \emph{MI} and \emph{Model Gain}.

\paragraph{Search parametrization}

We vary the parameters of the search systematically:
We evaluate $a \in \{1, \dots, 10\}$~alternatives for sequential search methods and $a \in \{1, \dots, 5\}$ for simultaneous search methods due to the potentially higher runtime of the latter.
For the dissimilarity threshold~$\tau$, we analyze all possible sizes of the feature-set overlap in the Dice dissimilarity (cf.~Equations~\ref{eq:afs:dice} and~\ref{eq:afs:dice-rearranged-equal-size}).
Thus, for $k=5$, we consider $\tau \in \{0.2, 0.4, 0.6, 0.8, 1.0\}$, corresponding to an overlap of four to zero features.
For $k=10$, we consider $\tau \in \{0.1, 0.2, \dots, 1.0\}$.
We exclude $\tau = 0$, which would allow returning duplicate feature sets.

\paragraph{Timeout}

In solver-based search, we employ a timeout to make a large-scale evaluation feasible and to account for the high variance of solver runtime, which even occurs for optimization problems of the same size.
In particular, we grant each solver call 60~s multiplied by the number of feature sets sought.
Thus, solver-based sequential search conducts multiple solver calls with 60~s timeout each, while solver-based simultaneous search conducts one solver call with proportionally more time, e.g., 300~s for five feature sets (i.e., four alternatives).
The summed timeout for a fixed number of alternatives is the same for both solver-based search methods.
For 84\% of the feature sets in our evaluation, the solver finished before the timeout.

\paragraph{Competitors for search methods}

As discussed in Section~\ref{sec:afs:related-work}, approaches from related work pursue different objective functions, operate with different notions of alternatives, and may only work for particular feature-selection methods.
All these points prevent a meaningful comparison of these approaches to ours.
E.g., a feature set considered alternative in related work might violate our constraints for alternatives.
Further, within our search methods, we can still put the feature-set quality into perspective by comparing alternatives to each other.
In particular, the quality of the `original' feature set, i.e., obtained by running the feature-selection methods without constraints for alternatives, serves as a natural reference point.

\begin{table}[p]
	\centering
	\begin{tabular}{lrrr}
		\toprule
		Dataset & $m$ & $n$ & Mean corr. \\
		\midrule
		backache & 180 & 32 & 0.10 \\
		chess & 3196 & 36 & 0.08 \\
		churn & 5000 & 20 & 0.04 \\
		clean1 & 476 & 168 & 0.25 \\
		clean2 & 6598 & 168 & 0.25 \\
		coil2000 & 9822 & 85 & 0.07 \\
		credit\_a & 690 & 15 & 0.12 \\
		credit\_g & 1000 & 20 & 0.07 \\
		dis & 3772 & 29 & 0.08 \\
		GE\_2\_Way\_20atts\_0.1H\_EDM\_1\_1 & 1600 & 20 & 0.02 \\
		GE\_2\_Way\_20atts\_0.4H\_EDM\_1\_1 & 1600 & 20 & 0.02 \\
		GE\_3\_Way\_20atts\_0.2H\_EDM\_1\_1 & 1600 & 20 & 0.02 \\
		GH\_20atts\_1600\_Het\_0.4\_0.2\_50\_EDM\_2\_001 & 1600 & 20 & 0.02 \\
		GH\_20atts\_1600\_Het\_0.4\_0.2\_75\_EDM\_2\_001 & 1600 & 20 & 0.02 \\
		hepatitis & 155 & 19 & 0.15 \\
		Hill\_Valley\_with\_noise & 1212 & 100 & 1.00 \\
		horse\_colic & 368 & 22 & 0.16 \\
		house\_votes\_84 & 435 & 16 & 0.30 \\
		hypothyroid & 3163 & 25 & 0.15 \\
		ionosphere & 351 & 34 & 0.25 \\
		molecular\_biology\_promoters & 106 & 57 & 0.08 \\
		mushroom & 8124 & 22 & 0.18 \\
		ring & 7400 & 20 & 0.02 \\
		sonar & 208 & 60 & 0.21 \\
		spambase & 4601 & 57 & 0.14 \\
		spect & 267 & 22 & 0.20 \\
		spectf & 349 & 44 & 0.19 \\
		tokyo1 & 959 & 44 & 0.44 \\
		twonorm & 7400 & 20 & 0.17 \\
		wdbc & 569 & 30 & 0.42 \\
		\bottomrule
	\end{tabular}
	\caption{
		Datasets from PMLB used in our experiments.
		$m$~denotes the number of data objects and $n$~the number of features.
		\emph{Mean corr.} is the average of absolute values of all pairwise Spearman's rank correlations between features.
		In dataset names, we replaced \emph{GAMETES\_Epistasis} with  \emph{GE\_} and \emph{GAMETES\_Heterogeneity} with \emph{GH\_} to reduce the table's width.
	}
	\label{tab:afs:datasets}
\end{table}

\subsection{Datasets}
\label{sec:afs:experimental-design:datasets}

\paragraph{Selection criteria}

We use a variety of datasets from the Penn Machine Learning Benchmarks (PMLB)~\cite{olson2017pmlb,romano2021pmlb}.
To harmonize evaluation, we only consider binary-classification datasets, though alternative feature selection also works for regression and multi-class problems.
We exclude datasets with less than 100 data objects since they might entail a high uncertainty when assessing feature-set quality.
Otherwise, the number of data objects should not systematically impact the feature-set quality and is unimportant for our evaluation.
Also, we exclude datasets with less than 15 features to leave some room for alternatives.
Next, we exclude one dataset with 1000 features, which would dominate the overall runtime of the experiments.
Finally, we manually exclude datasets that seem duplicated or modified versions of other datasets from the benchmark.

Consequently, we obtain 30 datasets with 106 to 9822 data objects and 15 to 168 features.
The datasets do not contain any missing values.
Categorical features have an ordinal encoding by default.
Table~\ref{tab:afs:datasets} lists these datasets.

\paragraph{Feature correlation}

Table~\ref{tab:afs:datasets} also displays the datasets' average feature correlation.
In particular, we compute Spearman's rank correlation between each pair of features and take the absolute values to evaluate the strength of dependencies rather than their sign.
For the datasets in our study, the average feature correlations are often weak, mostly below 0.3.
Generally, correlated features indicate that alternative feature sets may exist.
However, there are two caveats.
First, rank correlation only captures certain types of dependencies, while our feature-selection criteria and prediction models are more general.
Second, for optimal alternatives, the dependency between highly predictive features is crucial, while the dependency between unimportant features matters less.
However, the table only shows the mean over all feature pairs.

\subsection{Implementation and Execution}
\label{sec:afs:experimental-design:implementation}

We implemented our experimental pipeline in Python~3.8, using \emph{scikit-learn}~\cite{pedregosa2011scikit-learn} for machine learning and the solver \emph{SCIP}~\cite{bestuzheva2021scip} via the package \emph{OR-Tools}~\cite{perron2022or-tools} for optimization.
A requirements file in our code specifies the versions of all packages.
The experimental pipeline parallelizes over datasets, cross-validation folds, and feature-selection methods, while solver calls and model training are single-threaded.
We ran the pipeline on a server with 128~GB RAM and an \emph{AMD EPYC 7551} CPU, having 32~physical cores and a base clock of 2.0~GHz.
The parallelized pipeline run took 255~hours, i.e., about 10.6~days.

\section{Evaluation}
\label{sec:afs:evaluation}

In this section, we evaluate our experiments.
In particular, we discuss the parametrization for searching alternatives with solver-based search methods: the search method (cf.~Section~\ref{sec:afs:evaluation:solver-search-methods}), number of alternatives~$a$ (cf.~Section~\ref{sec:afs:evaluation:num-alternatives}), and dissimilarity threshold~$\tau$ (cf.~Section~\ref{sec:afs:evaluation:tau}).
Further, we compare heuristic search methods for univariate feature qualities to solver-based optimization (cf.~Section~\ref{sec:afs:evaluation:heuristic-search-methods}).
Section~\ref{sec:afs:evaluation:summary} summarizes key findings.
Additionally, Appendix~\ref{sec:afs:appendix:evaluation} contains results for further dimensions of our experimental design.

\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 25 15 10, clip]{plots/afs-impact-search-stddev-train-objective.pdf}
		\caption{Standard deviation of training-set objective value within search runs.}
		\label{fig:afs:impact-search-stddev-train-objective}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 25 15 10, clip]{plots/afs-impact-search-mean-train-objective.pdf}
		\caption{Mean of training-set objective value within search runs.}
		\label{fig:afs:impact-search-mean-train-objective}
	\end{subfigure}
	\\ \vspace{\baselineskip}
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 25 15 15, clip]{plots/afs-impact-search-stddev-test-objective.pdf}
		\caption{Standard deviation of test-set objective value within search runs.}
		\label{fig:afs:impact-search-stddev-test-objective}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 25 15 15, clip]{plots/afs-impact-search-mean-test-objective.pdf}
		\caption{Mean of test-set objective value with\-in search runs.}
		\label{fig:afs:impact-search-mean-test-objective}
	\end{subfigure}
	\\ \vspace{\baselineskip}
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 25 15 5, clip]{plots/afs-impact-search-stddev-decision-tree-test-mcc.pdf}
		\caption{Standard deviation of test-set prediction performance within search runs.}
		\label{fig:afs:impact-search-stddev-decision-tree-test-mcc}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 25 15 5, clip]{plots/afs-impact-search-mean-decision-tree-test-mcc.pdf}
		\caption{Mean of test-set prediction performance within search runs.}
		\label{fig:afs:impact-search-mean-decision-tree-test-mcc}
	\end{subfigure}
	\caption{
		Feature-set quality over the number of alternatives~$a$, by solver-based search method for alternatives and evaluation metric.
		Results with \emph{MI} as the feature-selection method and $k=5$.
		Y-axes are truncated to improve readability.
	}
	\label{fig:afs:impact-search-quality}
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 35 5 10, clip]{plots/afs-impact-search-fs-method-decision-tree-test-mcc.pdf}
		\caption{Test-set prediction performance.}
		\label{fig:afs:impact-search-fs-method-decision-tree-test-mcc}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 35 5 10, clip]{plots/afs-impact-search-fs-method-metric-diff.pdf}
		\caption{
			Difference in quality between simultaneous (sum-aggregation) and sequential search by evaluation metric.
			Y-axis is truncated to improve readability.
		}
		\label{fig:afs:impact-search-fs-method-metric-diff}
	\end{subfigure}
	\caption{
		Feature-set quality by feature-selection method and solver-based search method for alternatives.
		Results with $k=5$ and $a \in \{1,2,3,4,5\}$.
	}
	\label{fig:afs:impact-search-fs-method-quality}
\end{figure}

\subsection{Solver-Based Search Methods for Alternatives}
\label{sec:afs:evaluation:solver-search-methods}

\paragraph{Variance in feature-set quality}

As expected, the search method influences how much the training-set objective value~$Q$ varies between alternatives found within each search run.
Figure~\ref{fig:afs:impact-search-stddev-train-objective} visualizes this result for \emph{MI} as the feature-selection method and $k=5$.
Each box in the figure shows how the variance within individual search runs for alternatives is distributed over other experimental settings, e.g., datasets and cross-validation folds.
In particular, the quality of multiple alternatives found by solver-based sequential search usually varies more than for solver-based simultaneous search.
For solver-based simultaneous search, min-aggregation yields considerably more homogeneous feature-set quality than sum-aggregation.
These findings apply to all white-box feature-selection methods but not the heuristic \emph{Greedy Wrapper}.

As Figures~\ref{fig:afs:impact-search-stddev-test-objective} and~\ref{fig:afs:impact-search-stddev-decision-tree-test-mcc} show, the variance of feature-set quality differs considerably less between the solver-based search methods on the test set, for the objective value as well as prediction performance.
In particular, alternatives found by solver-based simultaneous search do not have considerably more homogeneous test feature-set quality than for solver-based sequential search.
This effect might result from overfitting:
Even if training feature-set quality is similar, some alternatives might generalize better, i.e., lose less quality on the test set than others.
Thus, the variance in test feature-set quality caused by overfitting could alleviate the effect on variance caused by the search method.

\paragraph{Average value of feature-set quality}

While obtaining alternatives of homogeneous quality can be one goal of simultaneous search, the main selling point compared to sequential search would be alternatives of higher average quality.
However, we found that solver-based simultaneous search is not clearly better than solver-based sequential search in that regard.
In particular, Figure~\ref{fig:afs:impact-search-mean-train-objective} compares the distribution of the mean training-set objective in search runs with \emph{MI} as the feature-selection method and $k=5$.
We observe that all solver-based search methods yield very similar distributions of feature-set quality.
The other four feature-selection methods besides \emph{MI} also do not show a general quality advantage of solver-based simultaneous search.
At most, solver-based simultaneous search tends to develop a slight advantage with a growing number of alternatives for \emph{MI}, as visible in Figure~\ref{fig:afs:impact-search-mean-train-objective}, and \emph{Model Gain}.

The test-set objective value in Figure~\ref{fig:afs:impact-search-mean-test-objective} and the test-set prediction performance in Figure~\ref{fig:afs:impact-search-mean-decision-tree-test-mcc} also exhibit the negligible quality difference between the solver-based search methods.
As Figure~\ref{fig:afs:impact-search-fs-method-decision-tree-test-mcc} displays, the variation in prediction performance caused by other dimensions of the experimental design, e.g., dataset, dissimilarity threshold~$\tau$, etc., exceeds the variation due to the choice of the solver-based search method.

Finally, Figure~\ref{fig:afs:impact-search-fs-method-metric-diff} displays the difference in feature-set quality between solver-based sequential and simultaneous search compared on each search setting separately, i.e., each combination of dataset, dissimilarity threshold~$\tau$, etc.
Positive values in this figure denote a higher quality of simultaneous-search solutions, while negative values denote a higher quality of sequential-search solutions.
The figure matches previous results by showing little variation in quality between the solver-based search methods except for \emph{Greedy Wrapper} feature selection.
In particular, the quality difference is usually close to zero, apart from a few outliers.
Additionally, the figure highlights that outliers can occur in both directions:
While solver-based simultaneous search can yield better feature sets in some scenarios, solver-based sequential search can be better in others.

\begin{table}[t]
	\centering
	\begin{tabular}{llrrr}
		\toprule
		\multirow{2}{*}{Feature selection} & \multirow{2}{*}{Search} & \multicolumn{3}{c}{Optimization status} \\
		\cmidrule(r){3-5}
		& & Infeasible & Feasible & Optimal \\
		\midrule
		FCBF & seq. & 74.51\% & 0.00\% & 25.49\% \\
		FCBF & sim. (min) & 73.07\% & 1.73\% & 25.20\% \\
		FCBF & sim. (sum) & 73.07\% & 2.19\% & 24.75\% \\
		MI & seq. & 4.93\% & 0.00\% & 95.07\% \\
		MI & sim. (min) & 4.67\% & 9.15\% & 86.19\% \\
		MI & sim. (sum) & 4.67\% & 2.88\% & 92.45\% \\
		Model Gain & seq. & 4.93\% & 0.00\% & 95.07\% \\
		Model Gain & sim. (min) & 4.67\% & 5.17\% & 90.16\% \\
		Model Gain & sim. (sum) & 4.67\% & 1.84\% & 93.49\% \\
		mRMR & seq. & 4.88\% & 9.63\% & 85.49\% \\
		mRMR & sim. (min) & 4.67\% & 49.04\% & 46.29\% \\
		mRMR & sim. (sum) & 4.67\% & 67.39\% & 27.95\% \\
		\bottomrule
	\end{tabular}
	\caption{
		Frequency of optimization statuses (cf.~Section~\ref{sec:afs:experimental-design:evaluation}) by feature-selection method and solver-based search method for alternatives.
		Results with $k=5$, $a \in \{1,2,3,4,5\}$, and excluding \emph{Greedy Wrapper}, which uses the solver for satisfiability checking rather than optimizing.
		Each row adds up to 100\%.
	}
	\label{tab:afs:impact-search-fs-method-optimization-status}
\end{table}

\begin{table}[t]
	\centering
	\begin{tabular}{rrrr}
		\toprule
		\multirow{2}{*}{$a$} & \multicolumn{3}{c}{Optimization status} \\
		\cmidrule(r){2-4}
		& Infeasible & Feasible & Optimal \\
		\midrule
		1 & 16.10\% & 7.57\% & 76.33\% \\
		2 & 17.50\% & 13.43\% & 69.07\% \\
		3 & 20.00\% & 20.40\% & 59.60\% \\
		4 & 27.00\% & 21.47\% & 51.53\% \\
		5 & 28.23\% & 30.00\% & 41.77\% \\
		\bottomrule
	\end{tabular}
	\caption{
		Frequency of optimization statuses (cf.~Section~\ref{sec:afs:experimental-design:evaluation}) by number of alternatives~$a$.
		Results from solver-based simultaneous search with sum-aggregation, $k=5$, and excluding \emph{Greedy Wrapper}.
		Each row adds up to 100\%.
	}
	\label{tab:afs:impact-num-alternatives-optimization-status}
\end{table}

\paragraph{Optimization status}

One reason why solver-based simultaneous search fails to consistently beat solver-based sequential search quality-wise is that search results can be suboptimal.
For \emph{Greedy Wrapper}, the search is heuristic per se and does not cover the entire search space.
For all feature-selection methods, the solver can time out.
Table~\ref{tab:afs:impact-search-fs-method-optimization-status} shows that solver-based simultaneous search has a higher likelihood of timeouts than solver-based sequential search, likely due to the larger size of the optimization problem (cf.~Table~\ref{tab:afs:seq-sim-comparison}).
In particular, for up to five alternatives and $k=5$, all solver-based sequential searches for \emph{FCBF}, \emph{MI}, and \emph{Model Gain} finished within the timeout, i.e., yielded the optimal feature set or ascertained infeasibility, while \emph{mRMR} had about 10\% timeouts.
In contrast, for solver-based simultaneous search with sum-aggregation, all feature-selection methods experience timeouts:
1-3\% of the searches for \emph{FCBF}, \emph{MI}, and \emph{Model Gain}, and 67\% of the searches for \emph{mRMR} found a feasible solution but could not prove optimality.
Such timeout-affected simultaneous solutions can be worse than optimal sequential solutions.
The optimization status \emph{not solved}, i.e., not finding a feasible solution without proving infeasibility, did not occur in the displayed results.
The feature-selection method \emph{mRMR} is especially prone to suboptimal solutions, likely because it has a more complex objective than~\emph{MI} and \emph{Model Gain}.
In contrast, \emph{FCBF} often results in infeasible optimization problems since its constraints, preventing the selection of redundant features (cf.~Equation~\ref{eq:afs:fcbf}), might prevent finding any valid feature set of size~$k$.
Min-aggregation instead of sum-aggregation in solver-based simultaneous search exhibits more timeouts for \emph{MI} and \emph{Model Gain} but less for \emph{FCBF} and \emph{mRMR}.
Still, solver-based sequential search incurs fewer timeouts for all these four feature-selection methods.

Finally, note that the fraction of timeouts strongly depends on the number of alternatives~$a$, as Table~\ref{tab:afs:impact-num-alternatives-optimization-status} displays:
For solver-based simultaneous search with $k=5$ and sum-aggregation, roughly 8\% of the white-box searches timed out for~$a=1$ but 20\% for~$a=3$ and 30\% for~$a=5$.
While we grant solver-based simultaneous searches proportionally more time for multiple alternatives, the observed increase in timeouts suggests that runtime clearly increases super-proportionally, as we analyze next.

\begin{table}[t]
	\centering
	\begin{tabular}{lrrr}
		\toprule
		\multirow{2}{*}{Feature selection} & \multicolumn{3}{c}{Optimization time} \\
		\cmidrule(r){2-4}
		& Seq. & Sim. (min) & Sim. (sum) \\
		\midrule
		FCBF & 0.22~s & 11.91~s & 13.09~s \\
		Greedy Wrapper & 54.23~s & 61.10~s & 63.45~s \\
		MI & 0.02~s & 46.36~s & 24.19~s \\
		Model Gain & 0.02~s & 29.46~s & 18.79~s \\
		mRMR & 34.12~s & 157.87~s & 189.76~s \\
		\bottomrule
	\end{tabular}
	\caption{
		Mean optimization time by feature-selection method and solver-based search method for alternatives.
		Results with $k=5$ and $a \in \{1,2,3,4,5\}$.
	}
	\label{tab:afs:impact-search-fs-method-optimization-time}
\end{table}

\begin{table}[t]
	\centering
	\begin{tabular}{lrrrrr}
		\toprule
		\multirow{2}{*}{$a$} & \multicolumn{5}{c}{Optimization time} \\
		\cmidrule(r){2-6}
		& FCBF & Wrapper & MI & Model Gain & mRMR \\
		\midrule
		1 & 0.52~s & 25.94~s & 0.02~s & 0.02~s & 44.99~s \\
		2 & 0.95~s & 39.44~s & 0.07~s & 0.06~s & 118.80~s \\
		3 & 3.26~s & 56.52~s & 0.25~s & 0.22~s & 208.90~s \\
		4 & 14.02~s & 86.13~s & 3.39~s & 3.19~s & 258.40~s \\
		5 & 46.71~s & 109.20~s & 117.21~s & 90.49~s & 317.69~s \\
		\bottomrule
	\end{tabular}
	\caption{
		Mean optimization time by number of alternatives and feature-selection method.
		Results from solver-based simultaneous search with sum-aggregation and $k=5$.
	}
	\label{tab:afs:impact-num-alternatives-fs-method-optimization-time}
\end{table}

\paragraph{Optimization time}

The optimization times also speak in favor of sequential search.
As Table~\ref{tab:afs:impact-search-fs-method-optimization-time} shows, the mean optimization time of solver-based sequential search is lower for all five feature-selection methods.
In particular, the difference between solver-based sequential and simultaneous search is up to three orders of magnitude for the four white-box feature-selection methods.
Further, \emph{FCBF}, \emph{MI}, and \emph{Model Gain} experience a dramatic increase in optimization time with the number of alternatives~$a$ in solver-based simultaneous search, as Table~\ref{tab:afs:impact-num-alternatives-fs-method-optimization-time} displays.
In contrast, the runtime increase is considerably less for solver-based sequential search, which shows an approximately linear trend with the number of alternatives.

Based on all results described in this section, we focus on solver-based sequential search in the subsequent Sections~\ref{sec:afs:evaluation:num-alternatives} and~\ref{sec:afs:evaluation:tau}.
In particular, it was significantly faster than solver-based simultaneous search while yielding similar feature-set quality.

Another interesting question for practitioners is how the runtime relates to~$n$, the number of features in the dataset.
One would expect a positive correlation since the optimization problem's instance size increases with~$n$.
Roughly speaking, this trend appears in our experimental data indeed.
However, the observed trend is rather noisy, particularly for solver-based simultaneous search, and some higher-dimensional datasets even show lower average runtimes than lower-dimensional ones.
This result indicates that other factors than~$n$ influence runtime.
Besides factors related to the datasets and experimental design, search heuristics used by the solver may also cause the runtime to fluctuate considerably.

\subsection{Number of Alternatives \texorpdfstring{$a$}{}}
\label{sec:afs:evaluation:num-alternatives}

\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=14 33 14 16, clip]{plots/afs-impact-num-alternatives-quality-max.pdf}
		\caption{Max-normalized, infeasible feature sets excluded.}
		\label{fig:afs:impact-num-alternatives-quality-max}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=14 33 14 16, clip]{plots/afs-impact-num-alternatives-quality-max-fillna.pdf}
		\caption{Max-normalized, infeasible feature sets assigned a quality of~0.}
		\label{fig:afs:impact-num-alternatives-quality-max-fillna}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=14 33 14 16, clip]{plots/afs-impact-num-alternatives-quality-min-max.pdf}
		\caption{Min-max-normalized, infeasible feature sets excluded.}
		\label{fig:afs:impact-num-alternatives-quality-min-max}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=14 33 14 16, clip]{plots/afs-impact-num-alternatives-quality-min-max-fillna.pdf}
		\caption{Min-max-normalized, infeasible feature sets assigned a quality of~0.}
		\label{fig:afs:impact-num-alternatives-quality-min-max-fillna}
	\end{subfigure}
	\caption{
		Feature-set quality, normalized per search run for alternatives, over the number of alternatives, by evaluation metric and normalization method.
		Results from solver-based sequential search with \emph{MI} as the feature-selection method and $k=5$.
	}
	\label{fig:afs:impact-num-alternatives-quality}
\end{figure}

\paragraph{Feature-set quality}

For sequential search, the training-set objective value has to decrease with the number of alternatives, at least for exact optimization.
In particular, each found feature set constrains the optimization problem further.
Figures~\ref{fig:afs:impact-num-alternatives-quality-max} and~\ref{fig:afs:impact-num-alternatives-quality-min-max} illustrate this trend for \emph{MI}-based feature selection.
Since feature-set quality varies between datasets (cf.~Appendix~\ref{sec:afs:appendix:evaluation:datasets}), we additionally normalize feature-set quality here.
In particular, we analyze the relative development of feature-set quality within each search run for alternatives.
First, we shift the range of all evaluation metrics to~$[0,1]$ since prediction performance and the objectives of \emph{Greedy Wrapper} and \emph{mRMR} have the range~$[-1,1]$ without this shift.
Second, we max-normalize feature-set quality for each search of alternatives, i.e., the highest feature-set quality in the search run is set to~1, and the other qualities are scaled accordingly.
Figure~\ref{fig:afs:impact-num-alternatives-quality-max} shows that multiple alternatives may have a similar quality, as the median training-set objective value remains relatively stable over the alternatives and is above~0.8 even for the tenth alternative.
For comparison, Figure~\ref{fig:afs:impact-num-alternatives-quality-min-max} uses min-max normalization, i.e., the worst of the alternatives gets~0 as objective.
This figure makes the decrease in quality over the alternatives more visible.
In particular, this figure highlights that the training-set objective value decreases most from the original feature set, i.e., the zeroth alternative in the figures, to the first alternative, but the decrease is less beyond.

Additionally, Figures~\ref{fig:afs:impact-num-alternatives-quality-max} and~\ref{fig:afs:impact-num-alternatives-quality-min-max} show that the test-set objective value also drops most to the first alternative.
However, this decrease is less prominent than on the training set, and there is no clear trend beyond the first few alternatives.
In particular, alternatives can even have a higher test-set objective value than the original feature set due to overfitting.
Similar findings hold for test-set prediction performance.
Overall, these results indicate that alternative feature sets fulfill their purpose of being different solutions with similar quality.

\paragraph{Optimization status}

The prior observations refer to the quality of the found feature sets.
However, the more alternatives are desired, the likelier an infeasible optimization problem is (cf.~Table~\ref{tab:afs:impact-num-alternatives-optimization-status}).
For example, \emph{MI}-based feature selection in solver-based sequential search always finds an original feature set.
However, with $k=5$, the problem is infeasible in 2\% of the cases for the third alternative, 12\% for the fifth, and 17\% for the tenth.
Increasing the feature-set size~$k$ or having lower dataset dimensionality~$n$ naturally causes more infeasible solutions, as fewer features become available for alternatives.
Thus, even if the quality of found feature sets remains relatively stable for more alternatives, valid alternatives may simply not exist.
Figures~\ref{fig:afs:impact-num-alternatives-quality-max-fillna} and~\ref{fig:afs:impact-num-alternatives-quality-min-max-fillna} show the same data as Figures~\ref{fig:afs:impact-num-alternatives-quality-max} and~\ref{fig:afs:impact-num-alternatives-quality-min-max} but with the quality of infeasible feature sets set to zero, i.e., the theoretical minimum after we shifted the value ranges of evaluation metrics.
In these figures, the downward trend of feature-set quality over the alternatives becomes slightly more prominent, particularly for many alternatives.
This trend also depends on the dissimilarity threshold~$\tau$, which we analyze in the next section.

\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=20 35 15 15, clip]{plots/afs-impact-num-alternatives-fs-method-train-objective-max.pdf}
		\caption{
			Training-set objective value.
			Infeasible feature sets excluded.
		}
		\label{fig:afs:impact-num-alternatives-fs-method-train-objective-max}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=20 35 15 15, clip]{plots/afs-impact-num-alternatives-fs-method-train-objective-max-fillna.pdf}
		\caption{
			Training-set objective value.
			Infeasible feature sets assigned a quality of~0.
		}
		\label{fig:afs:impact-num-alternatives-fs-method-train-objective-max-fillna}
	\end{subfigure}
	\\ \vspace{\baselineskip}
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=20 35 15 15, clip]{plots/afs-impact-num-alternatives-fs-method-decision-tree-test-mcc-max.pdf}
		\caption{
			Test-set prediction performance.
			Infeasible feature sets excluded.
		}
		\label{fig:afs:impact-num-alternatives-fs-method-decision-tree-test-mcc-max}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=20 35 15 15, clip]{plots/afs-impact-num-alternatives-fs-method-decision-tree-test-mcc-max-fillna.pdf}
		\caption{
			Test-set prediction performance.
			Infeasible feature sets assigned a quality of~0.
		}
		\label{fig:afs:impact-num-alternatives-fs-method-decision-tree-test-mcc-max-fillna}
	\end{subfigure}
	\caption{
		Mean of feature-set quality, max-normalized per search run for alternatives, over the number of alternatives, by feature-selection method and evaluation metric.
		Results from solver-based sequential search with $k=5$.
	}
	\label{fig:afs:impact-num-alternatives-fs-method-quality}
\end{figure}

\paragraph{Influence of feature-selection method}

While we discussed \emph{MI} before, the decrease in objective value over the number of alternatives occurs for all feature-selection methods in our experiments, as Figure~\ref{fig:afs:impact-num-alternatives-fs-method-train-objective-max} displays.
The strength of the decrease varies between the feature selection methods.
For example, \emph{Greedy Wrapper} and \emph{mRMR} show little effect of increasing the number of alternatives, while \emph{MI} and \emph{Model Gain} exhibit the strongest effect.
As Figure~\ref{fig:afs:impact-num-alternatives-fs-method-train-objective-max-fillna} displays, the quality decrease becomes more prominent if one sets the quality of infeasible feature sets to zero.
Further, for the test-set prediction performance shown in Figure~\ref{fig:afs:impact-num-alternatives-fs-method-decision-tree-test-mcc-max}, no feature-selection method exhibits a strong decrease over the number of alternatives, unless we account for infeasible feature sets (cf.~Figure~\ref{fig:afs:impact-num-alternatives-fs-method-decision-tree-test-mcc-max-fillna}).

\subsection{Dissimilarity Threshold \texorpdfstring{$\tau$}{}}
\label{sec:afs:evaluation:tau}

\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 17 10 15, clip]{plots/afs-impact-num-alternatives-tau-train-objective-max.pdf}
		\caption{
			Training-set objective value.
			Infeasible feature sets excluded.
		}
		\label{fig:afs:impact-num-alternatives-tau-train-objective-max}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 17 10 15, clip]{plots/afs-impact-num-alternatives-tau-train-objective-max-fillna.pdf}
		\caption{
			Training-set objective value.
			Infeasible feature sets assigned a quality of~0.
		}
		\label{fig:afs:impact-num-alternatives-tau-train-objective-max-fillna}
	\end{subfigure}
	\\ \vspace{\baselineskip}
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 17 10 15, clip]{plots/afs-impact-num-alternatives-tau-test-objective-max.pdf}
		\caption{
			Test-set objective value.
			Infeasible feature sets excluded.
		}
		\label{fig:afs:impact-num-alternatives-tau-test-objective-max}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 17 10 15, clip]{plots/afs-impact-num-alternatives-tau-test-objective-max-fillna.pdf}
		\caption{
			Test-set objective value.
			Infeasible feature sets assigned a quality of~0.
		}
		\label{fig:afs:impact-num-alternatives-tau-test-objective-max-fillna}
	\end{subfigure}
	\\ \vspace{\baselineskip}
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 17 10 15, clip]{plots/afs-impact-num-alternatives-tau-decision-tree-test-mcc-max.pdf}
		\caption{
			Test-set prediction performance.
			Infeasible feature sets excluded.
		}
		\label{fig:afs:impact-num-alternatives-tau-decision-tree-test-mcc-max}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 17 10 15, clip]{plots/afs-impact-num-alternatives-tau-decision-tree-test-mcc-max-fillna.pdf}
		\caption{
			Test-set prediction performance.
			Infeasible feature sets assigned a quality of~0.
		}
		\label{fig:afs:impact-num-alternatives-tau-decision-tree-test-mcc-max-fillna}
	\end{subfigure}
	\caption{
		Mean of feature-set quality, max-normalized per search run for alternatives, over the number of alternatives and dissimilarity threshold~$\tau$, by evaluation metric.
		Results from solver-based sequential search with \emph{MI} as the feature-selection method and $k=10$.
	}
	\label{fig:afs:impact-num-alternatives-tau-quality}
\end{figure}

\paragraph{Feature-set quality}

As Figure~\ref{fig:afs:impact-num-alternatives-tau-train-objective-max} shows for \emph{MI} as the feature-selection method, the decrease in the objective value~$Q$ over the number of alternatives strongly depends on the dissimilarity threshold~$\tau$.
We use results with $k=10$ instead of $k=5$ here to show more distinct values of $\tau$.
For a low dissimilarity threshold, e.g., $\tau=0.1$, the objective value barely drops over the number of alternatives.
In contrast, the objective value decreases significantly for a high dissimilarity threshold, e.g., $\tau=1$.
This trend is expected since a higher~$\tau$ constrains the feature selection more.
Further, datasets with more features~$n$ tend to experience a smaller decrease in quality over~$a$ and~$\tau$.
As higher-dimensional datasets offer more options for alternatives, this observation makes sense.
However, this effect is not guaranteed since datasets with many features could also contain many useless features instead of interesting alternatives.

As Figure~\ref{fig:afs:impact-num-alternatives-tau-test-objective-max} displays, the decreasing quality over $\tau$ also shows for the test-set objective value, though the trend is weaker there.
The effect of~$\tau$ on prediction performance exhibits an even less clear trend, as visualized in Figure~\ref{fig:afs:impact-num-alternatives-tau-decision-tree-test-mcc-max}.
This result underlines our previous observations that the objective value is only partially indicative of prediction performance.

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 15 10 15, clip]{plots/afs-impact-num-alternatives-tau-optimization-status-k-5.pdf}
		\caption{Feature-set size~$k=5$.}
		\label{fig:afs:impact-num-alternatives-tau-optimization-status-k-5}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 15 10 15, clip]{plots/afs-impact-num-alternatives-tau-optimization-status-k-10.pdf}
		\caption{Feature-set size~$k=10$.}
		\label{fig:afs:impact-num-alternatives-tau-optimization-status-k-10}
	\end{subfigure}
	\caption{
		Fraction of optimization runs yielding a valid feature set over the number of alternatives and dissimilarity threshold~$\tau$, by feature-set size~$k$.
		Results from solver-based sequential search with \emph{MI} as the feature-selection method.
	}
	\label{fig:afs:impact-num-alternatives-tau-optimization-status}
\end{figure}

\paragraph{Optimization status}

Similar to our analysis for the number of alternatives (cf.~Section~\ref{sec:afs:evaluation:num-alternatives}), one needs to consider that setting~$\tau$ too high can make the optimization problem infeasible.
In particular, a higher dissimilarity threshold increases the likelihood that no feature set is alternative enough.
Figure~\ref{fig:afs:impact-num-alternatives-tau-optimization-status} visualizes the fraction of valid feature sets over the number of alternatives and dissimilarity threshold~$\tau$.
Figures~\ref{fig:afs:impact-num-alternatives-tau-train-objective-max-fillna},~\ref{fig:afs:impact-num-alternatives-tau-test-objective-max-fillna}, and~\ref{fig:afs:impact-num-alternatives-tau-decision-tree-test-mcc-max-fillna} account for infeasible feature sets by setting their feature-set quality to zero.
Compared to Figures~\ref{fig:afs:impact-num-alternatives-tau-train-objective-max},~\ref{fig:afs:impact-num-alternatives-tau-test-objective-max}, and~\ref{fig:afs:impact-num-alternatives-tau-decision-tree-test-mcc-max}, the decrease in feature-set quality is noticeably stronger.
In contrast, if only considering valid feature sets, the mean quality can increase over the number of alternatives, as visible in Figure~\ref{fig:afs:impact-num-alternatives-tau-train-objective-max} for $\tau=1.0$ or in Figure~\ref{fig:afs:impact-num-alternatives-fs-method-train-objective-max} for \emph{MI} and \emph{Model Gain}.
This counterintuitive phenomenon can occur because some datasets run out of valid feature sets sooner than others, so the average quality may be determined for different sets of datasets at each number of alternatives.

\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=20 35 15 15, clip]{plots/afs-impact-tau-fs-method-train-objective-max.pdf}
		\caption{
			Training-set objective value.
			Infeasible feature sets excluded.
		}
		\label{fig:afs:impact-tau-fs-method-train-objective-max}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=20 35 15 15, clip]{plots/afs-impact-tau-fs-method-train-objective-max-fillna.pdf}
		\caption{
			Training-set objective value.
			Infeasible feature sets assigned a quality of~0.
		}
		\label{fig:afs:impact-tau-fs-method-train-objective-max-fillna}
	\end{subfigure}
	\\ \vspace{\baselineskip}
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=20 35 15 15, clip]{plots/afs-impact-tau-fs-method-decision-tree-test-mcc-max.pdf}
		\caption{
			Test-set prediction performance.
			Infeasible feature sets excluded.
		}
		\label{fig:afs:impact-tau-fs-method-decision-tree-test-mcc-max}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=20 35 15 15, clip]{plots/afs-impact-tau-fs-method-decision-tree-test-mcc-max-fillna.pdf}
		\caption{
			Test-set prediction performance.
			Infeasible feature sets assigned a quality of~0.
		}
		\label{fig:afs:impact-tau-fs-method-decision-tree-test-mcc-max-fillna}
	\end{subfigure}
	\caption{
		Mean of feature-set quality, max-normalized per search run for alternatives, over the dissimilarity threshold~$\tau$, by feature-selection method and evaluation metric.
		Results from solver-based sequential search with $k=10$.
	}
	\label{fig:afs:impact-tau-fs-method-quality}
\end{figure}

\paragraph{Influence of feature-selection method}

The impact of~$\tau$ on feature-set quality varies between feature-selection methods, as Figure~\ref{fig:afs:impact-tau-fs-method-train-objective-max} shows.
Besides \emph{MI}, the objective value of \emph{Model Gain} strongly depends on~$\tau$ as well.
In contrast, the remaining three feature-selection methods exhibit little influence of~$\tau$ on feature-set quality unless one also accounts for infeasible feature sets (cf.~Figure~\ref{fig:afs:impact-tau-fs-method-train-objective-max-fillna}).
For \emph{Greedy Wrapper}, this outcome may be explained by the heuristic, inexact search procedure.
For \emph{FCBF}, the additional constraints on feature-feature correlation (cf.~Equation~\ref{eq:afs:fcbf}) may alleviate the effect of~$\tau$.
For \emph{mRMR}, the low influence of~$\tau$ matches the low influence of the number of alternatives.
For this feature-selection method, alternatives tend to vary little in their objective value.
Finally, the test-set prediction performance does not vary considerably over $\tau$ for any feature-selection method, as Figure~\ref{fig:afs:impact-tau-fs-method-decision-tree-test-mcc-max} displays.
Only considering infeasible feature sets results in decreased prediction performance (cf.~Figure~\ref{fig:afs:impact-tau-fs-method-decision-tree-test-mcc-max-fillna}).

\subsection{Heuristic Search Methods for Alternatives}
\label{sec:afs:evaluation:heuristic-search-methods}

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 25 15 15, clip]{plots/afs-impact-search-heuristics-stddev-train-objective.pdf}
		\caption{Standard deviation of training-set objective value within search runs.}
		\label{fig:afs:impact-search-heuristics-stddev-train-objective}
	\end{subfigure}
	\begin{subfigure}[t]{\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 25 15 15, clip]{plots/afs-impact-search-heuristics-stddev-test-objective.pdf}
		\caption{Standard deviation of test-set objective value within search runs.}
		\label{fig:afs:impact-search-heuristics-stddev-test-objective}
	\end{subfigure}
	\caption{
		Feature-set quality over the number of alternatives~$a$, by search method for alternatives (including heuristics) and evaluation metric.
		Results with \emph{MI} as the feature-selection method and $k=5$.
		Y-axes are truncated to improve readability.
	}
	\label{fig:afs:impact-search-heuristics-stddev}
\end{figure}

\paragraph{Variance in feature-set quality}

Figure~\ref{fig:afs:impact-search-heuristics-stddev} shows how feature-set qualities varies over alternatives within individual search runs for alternatives.
The figure uses \emph{MI} as the feature-selection method but results for \emph{Model Gain} are similar.
Also, this figure is similar to Figure~\ref{fig:afs:impact-search-stddev-train-objective} from our analysis of solver-based search methods but includes heuristic search methods.

For the training-set objective value (cf.~Figure~\ref{fig:afs:impact-search-heuristics-stddev-train-objective}), we observe that the heuristic \emph{Greedy Balancing} yields a small variance of quality within search runs, very similar to the solver-based simultaneous search with min-aggregation.
While the variance within search runs produced by the heuristic is slightly higher than for the solver-based search, this difference is barely visible in the figure.
In contrast, the heuristic \emph{Greedy Replacement} rather mimics solver-based sequential search, having a substantial variance over alternatives within search runs.
Also, the variance of \emph{Greedy Replacement} noticeably grows with the number of alternatives~$a$ and the dissimilarity threshold~$\tau$.
For \emph{Greedy Balancing}, the increase in variance over~$a$ also exists but is considerably smaller.

For the test-set objective value (cf.~Figure~\ref{fig:afs:impact-search-heuristics-stddev-train-objective}) and the test-set prediction performance, the variance within search runs differs less between the different search methods for alternatives.
As explained for solver-based search methods in Section~\ref{sec:afs:evaluation:solver-search-methods}, this phenomenon may be caused by overfitting, i.e., the training-set quality does not perfectly correspond to the test-set quality, thereby introducing additional variance.

\begin{figure}[p]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 30 15 15, clip]{plots/afs-impact-search-heuristics-metric-diff-sim-num-alternatives.pdf}
		\caption{Between solver-based simultaneous (min-aggregation) search and \emph{Greedy Balancing}, over the number of alternatives~$a$.}
		\label{fig:afs:impact-search-heuristics-metric-diff-sim-num-alternatives}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 30 15 15, clip]{plots/afs-impact-search-heuristics-metric-diff-seq-num-alternatives.pdf}
		\caption{
			Between solver-based sequential search and \emph{Greedy Replacement}, over the number of alternatives~$a$.
		}
		\label{fig:afs:impact-search-heuristics-metric-diff-seq-num-alternatives}
	\end{subfigure}
	\\ \vspace{\baselineskip}
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 30 15 15, clip]{plots/afs-impact-search-heuristics-metric-diff-sim-tau.pdf}
		\caption{Between solver-based simultaneous (min-aggregation) search and \emph{Greedy Balancing}, over dissimilarity threshold~$\tau$.}
		\label{fig:afs:impact-search-heuristics-metric-diff-sim-tau}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 30 15 15, clip]{plots/afs-impact-search-heuristics-metric-diff-seq-tau.pdf}
		\caption{
			Between solver-based sequential search and \emph{Greedy Replacement}, over dissimilarity threshold~$\tau$.
		}
		\label{fig:afs:impact-search-heuristics-metric-diff-seq-tau}
	\end{subfigure}
	\caption{
		Difference in training-set objective value between solver-based and heuristic search methods for alternatives, over user parameters~$a$ and~$\tau$, by feature-selection method and search methods.
		Results with $k=5$.
	}
	\label{fig:afs:impact-search-heuristics-metric-diff}
\end{figure}

\paragraph{Average value of feature-set quality}

Figure~\ref{fig:afs:impact-search-heuristics-metric-diff} compares the average training-set objective value of alternatives within search runs for each search setting separately, like Figure~\ref{fig:afs:impact-search-fs-method-metric-diff} does.
Positive values in Figure~\ref{fig:afs:impact-search-heuristics-metric-diff} mean that solver-based search is better, negative values favor heuristic search.
The latter can only happen if solver-based search runs into timeouts and therefore may yield suboptimal solutions, which is the case for solver-based simultaneous search (cf.~Table~\ref{tab:afs:impact-search-fs-method-optimization-status}).

Figures~\ref{fig:afs:impact-search-heuristics-metric-diff-sim-num-alternatives} and~\ref{fig:afs:impact-search-heuristics-metric-diff-seq-num-alternatives} compare solver-based and heuristic search over the number of alternatives~$a$.
For $a=1$, solver-based and heuristic search yield the same training-set objective value except when timeouts occur.
The more alternatives are desired, the more advantageous a solver-based search is in terms of training-set objective value.
As in prior analyses, the picture is less clear on the test set.
In particular, the quality advantage of solver-based search is smaller for the test-objective value than for the training-set objective value and vanishes for the test-set prediction performance.

Further, the difference in training-set objective value between \emph{Greedy Balancing} and solver-based simultaneous search (cf.~Figure~\ref{fig:afs:impact-search-heuristics-metric-diff-sim-num-alternatives}) grows faster with~$a$ than between \emph{Greedy Replacement} and solver-based sequential search (cf.~Figure~\ref{fig:afs:impact-search-heuristics-metric-diff-seq-num-alternatives}).
This phenomenon may be explained by the fact simultaneous search generally can develop an advantage over sequential search with a higher number of alternatives.
While \emph{Greedy Balancing} is a simultaneous search heuristic in the sense that it balances the quality of the alternatives, it selects the same feature as \emph{Greedy Replacement}, i.e., a sequential search heuristic, and only distributes them differently into feature sets.

Figures~\ref{fig:afs:impact-search-heuristics-metric-diff-sim-tau} and~\ref{fig:afs:impact-search-heuristics-metric-diff-seq-tau} compare solver-based and heuristic search over the dissimilarity threshold~$\tau$.
Unlike for~$a$, the difference in training-set objective value between solver-based and heuristic search does not increase over the whole range of~$\tau$ but shows an increase followed by a decrease.
In particular, $\tau=1$ allows the two heuristic search methods to reach the same training-set objective value as the solver-based methods.
This observation corresponds to our theoretical result that optimizing the summed quality of alternatives with~$\tau=1$ admits polynomial-time algorithms (cf.~Proposition~\ref{prop:afs:complexity-partitioning-sum}).

Regarding the feature-selection methods, \emph{MI} exhibits a larger difference in training-set objective value between solver-based and heuristic search than \emph{Model Gain}, which is also visible in Figure~\ref{fig:afs:impact-search-heuristics-metric-diff}.
In particular, the univariate feature qualities computed by \emph{Model Gain} in our experiments may have a distribution where heuristic search is closer to the optimum than for \emph{MI}-based feature qualities.

\begin{table}[t]
	\centering
	\begin{tabular}{llrrrr}
		\toprule
		\multirow{2}{*}{Feature selection} & \multirow{2}{*}{Search} & \multicolumn{4}{c}{Optimization status} \\
		\cmidrule(r){3-6}
		& & Infeasible & Not solved & Feasible & Optimal \\
		\midrule
		MI & bal. & 0.00\% & 9.20\% & 90.80\% & 0.00\% \\
		MI & rep. & 0.00\% & 9.20\% & 90.80\% & 0.00\% \\
		MI & seq. & 4.93\% & 0.00\% & 0.00\% & 95.07\% \\
		MI & sim. (min) & 4.67\% & 0.00\% & 9.15\% & 86.19\% \\
		MI & sim. (sum) & 4.67\% & 0.00\% & 2.88\% & 92.45\% \\
		Model Gain & bal. & 0.00\% & 9.20\% & 90.80\% & 0.00\% \\
		Model Gain & rep. & 0.00\% & 9.20\% & 90.80\% & 0.00\% \\
		Model Gain & seq. & 4.93\% & 0.00\% & 0.00\% & 95.07\% \\
		Model Gain & sim. (min) & 4.67\% & 0.00\% & 5.17\% & 90.16\% \\
		Model Gain & sim. (sum) & 4.67\% & 0.00\% & 1.84\% & 93.49\% \\
		\bottomrule
	\end{tabular}
	\caption{
		Frequency of optimization statuses (cf.~Section~\ref{sec:afs:experimental-design:evaluation}) by feature-selection method and search method for alternatives.
		Results with $k=5$ and $a \in \{1,2,3,4,5\}$.
		Each row adds up to 100\%.
	}
	\label{tab:afs:impact-search-heuristics-fs-method-optimization-status}
\end{table}

\paragraph{Optimization status}

Table~\ref{tab:afs:impact-search-heuristics-fs-method-optimization-status} displays the frequency of optimization statuses for the different search methods.
We only use \emph{not solved} and \emph{infeasible} as statuses for the two heuristic search methods, as they are neither guaranteed to find the optimum nor to prove infeasibility if they terminate early.
The status \emph{feasible} for solver-based simultaneous search indicates timeouts, which explain why \emph{Greedy Balancing} may be better in some cases (cf.~Figure~\ref{fig:afs:impact-search-heuristics-metric-diff}).

The table shows that \emph{Greedy Replacement} more often did not find a valid alternative (9.2\%) than solver-based sequential search (4.93\%).
A similar phenomenon occurred for \emph{Greedy Replacement} (9.2\%) compared to solver-based simultaneous search (4.67\%).
One can expect such a result, since the termination criterion of both heuristic search methods causes them to terminate early as soon as each feature is part of at least one alternative.
The latter criterion does not prevent further solutions to exist, but one would need a new strategy to enumerate further valid alternatives.

\paragraph{Optimization time}

The optimization time of the heuristic search methods is negligible.
In particular, \emph{Greedy Replacement} and \emph{Greedy Balancing} never took longer than 1~ms per search run for alternatives.
As discussed in Section~\ref{sec:afs:evaluation:solver-search-methods}, solver-based sequential search required less than 1~s on average for \emph{MI} and \emph{Model Gain} as feature-selection methods (cf.~Table~\ref{tab:afs:impact-search-fs-method-optimization-time}), while solver-based simultaneous search sometimes did not even finish within the 1-min timeout per alternative (cf.~Table~\ref{tab:afs:impact-search-fs-method-optimization-status}).
These results highlight the runtime advantage of the heuristics, particularly of \emph{Greedy Balancing} to search alternatives simultaneously.

\subsection{Summary}
\label{sec:afs:evaluation:summary}

\paragraph{Datasets (cf.~Appendix~\ref{sec:afs:appendix:evaluation:datasets})}

Generally, feature-set quality strongly depended on the dataset.
Thus, an analysis of alternative feature sets should be dataset-specific or appropriately normalize quality, as we did.

\paragraph{Feature-set quality metrics (cf.~Appendix~\ref{sec:afs:appendix:evaluation:metrics})}

Different notions of feature-set quality exhibited different trends in our experiments, so one should choose a notion of feature-set quality carefully.
In particular, the objective function of feature-selection methods might disagree with the prediction performance of the corresponding feature sets.
Further, we observed overfitting, i.e., a gap between training-set quality and test-set quality, also for simple objective functions, though to a lesser extent than for prediction performance.

\paragraph{Feature-selection methods (cf.~Appendix~\ref{sec:afs:appendix:evaluation:feature-selection})}

Among the feature-selection methods, \emph{Model Gain} resulted in the best prediction performance on average, though the simple univariate \emph{MI} also turned out competitive.
\emph{Greedy Wrapper} and \emph{mRMR} required high optimization times, while our constraint-based version of \emph{FCBF} yielded many infeasible solutions.
Finally, selecting $k=10$ instead of $k=5$ features yielded only a slight improvement in prediction performance for all feature-selection methods, so one might stick to smaller feature-set sizes if such a setting benefits interpretability for users.

\paragraph{Solver-based search methods for alternatives (cf.~Section~\ref{sec:afs:evaluation:solver-search-methods})}

Solver-based simultaneous search, particularly with min-aggregation, considerably reduced the variance of the training-set objective value over alternatives compared to solver-based sequential search, as we desired.
However, results were less clear on the test set and when considering prediction performance to measure feature-set quality.
Further, the average quality of alternatives was similar to solver-based sequential search.
In addition, the latter was considerably faster and led to less solver timeouts, particularly when increasing the number of alternatives.
Also, sequential search allows users to stop searching after each alternative instead of requiring the number of alternatives to be specified beforehand.
Thus, we recommend using sequential search.

\paragraph{Number of alternatives $a$ (cf.~Section~\ref{sec:afs:evaluation:num-alternatives})}

Feature-set quality decreased most from the original feature set to the first alternative but less beyond.
The strength of this decrease depended on the feature-selection method.
There usually were several alternatives of similar quality, if such valid alternatives existed at all.
In particular, the frequency of infeasible solutions increased with~$a$ due to more constraints.
Finally, the quality decrease was more prominent on the training than on the test set.

\paragraph{Dissimilarity threshold $\tau$ (cf.~Section~\ref{sec:afs:evaluation:tau})}

A higher dissimilarity threshold caused a stronger decrease in feature-set quality in terms of objective value for the feature-selection methods \emph{MI} and \emph{Model Gain}.
This result shows that users can control a trade-off between quality and dissimilarity.
However, results regarding prediction performance and for the other three feature-selection methods were less clear.
In any case, a higher~$\tau$ naturally caused more infeasible solutions, which users should be aware of.

\paragraph{Heuristic search methods for alternatives (cf.~Section~\ref{sec:afs:evaluation:heuristic-search-methods})}

The heuristic search methods \emph{Greedy Replacement} and \emph{Greedy Balancing} for univariate feature qualities achieved a good feature-set quality relative to solver-based search, particularly for a low number of alternatives and on the test set.
Apart from noticeable differences in training-set objective value for a high number of alternatives, users should be aware that the heuristics may stop in situations where solver-based search can still find further alternatives.
As a positive point, both the heuristics' runtime was negligible.
Also, \emph{Greedy Balancing} achieved a low variance of training-set objective value between alternatives found simultaneously, similar to solver-based simultaneous search with min-aggregation.

\section{Conclusions and Future Work}
\label{sec:afs:conclusion}

In this section, we summarize our work (cf.~Section~\ref{sec:afs:conclusion:conclusion}) and give an outlook on potential future work (cf.~Section~\ref{sec:afs:conclusion:future-work}).

\subsection{Conclusions}
\label{sec:afs:conclusion:conclusion}

Feature-selection methods are a valuable tool to foster interpretable predictions.
Conventional feature-selection methods typically yield only one feature set.
However, users may be interested in obtaining multiple, sufficiently diverse feature sets of high quality.
Such alternative feature sets may provide alternative explanations for predictions from the data.

In this article, we defined alternative feature selection as an optimization problem.
We formalized alternatives via constraints that are independent of the feature-selection method, can be combined with other constraints on feature sets, and allow users to control diversity according to their needs with two parameters, i.e., the number of alternatives~$a$ and a dissimilarity threshold~$\tau$.
Further, we discussed how to integrate different categories of conventional feature-selection methods as objectives.
We also analyzed the complexity of this optimization problem and proved $\mathcal{NP}$-hardness, even for simple notions of feature-set quality.
Additionally, we showed that the problem gives way to a constant-factor approximation under certain conditions, and we proposed corresponding heuristic search methods.
Finally, we evaluated alternative feature selection with 30 classification datasets and five feature-selection methods.
We compared sequential and simultaneous search for alternatives, both with solver-based and heuristic search methods, and varied the number of alternatives as well as the threshold for alternatives.

\subsection{Future Work}
\label{sec:afs:conclusion:future-work}

\paragraph{Feature selection (objective function)}

One could search for alternatives with other feature-selection methods than the five we analyzed.
In particular, we implemented only one procedure to find alternatives for wrapper feature selection (cf.~Section~\ref{sec:afs:approach:objectives:black-box}).
Embedded feature selection, which we did not evaluate, would also need adapted search methods for alternatives (cf.~Section~\ref{sec:afs:approach:objectives:embedding}).

\paragraph{Alternatives (constraints)}

One could vary the definition of alternatives, e.g., the set-dissimilarity measure (cf.~Section~\ref{sec:afs:approach:constraints:single}), the quality aggregation for simultaneous alternatives (cf.~Appendix~\ref{sec:afs:appendix:simultaneous-objective-aggregation}), or the overall optimization problem (cf.~Section~\ref{sec:afs:approach:problem}).
While we made general and straightforward decisions for each of these points, particular applications might demand other formalizations of alternatives.
E.g., one could use soft instead of hard constraints.

\paragraph{Computational complexity} Appendix~\ref{sec:afs:appendix:complexity:future-work} discusses how one could extend our complexity analysis of alternative feature selection (cf.~Section~\ref{sec:afs:approach:complexity}).

\paragraph{Runtime}

Our experiments (cf.~Section~\ref{sec:afs:evaluation:solver-search-methods}) and theoretical analyses (cf.~Section~\ref{sec:afs:approach:constraints:multiple}) revealed that solver-based simultaneous search scales poorly with the number of alternatives.
One could conceive a more efficient problem formulation.
Further, one could limit the solver runtime and take the intermediate results once the timeout is reached.
We already used a fixed timeout in our experiments, but studying the exact influence of timeouts on feature-set quality is an open topic.
Next, one could use a different solver, e.g., one for non-linear optimization, so the auxiliary variables from Equation~\ref{eq:afs:product-linear} become superfluous.
Finally, one could develop further simultaneous heuristics (cf.~Section~\ref{sec:afs:approach:univariate-heuristics:greedy-balancing}).

\paragraph{Datasets}

In the current article, we conducted a broad quantitative evaluation of alternative feature selection on datasets from various domains. (cf.~Section~\ref{sec:afs:experimental-design:datasets}).
While we could uncover several general trends, the existence and quality of alternatives naturally depend on the dataset.
Thus, practitioners may employ alternative feature selection in domain-specific case studies and evaluate the alternative feature sets qualitatively, thereby assessing their usefulness for interpretability.

~\\
\noindent \textsc{Acknowledgments}\quad
This work was supported by the Ministry of Science, Research and the Arts Baden-Wrttemberg, project \emph{Algorithm Engineering for the Scalability Challenge (AESC)}.

\appendix

\section{Appendix}
\label{sec:afs:appendix}

In this section, we provide supplementary materials.
Section~\ref{sec:afs:appendix:simultaneous-objective-aggregation} discusses possible aggregation operators for the objective of the simultaneous-search problem (cf.~Equation~\ref{eq:afs:afs-simultaneous}).
Section~\ref{sec:afs:appendix:multivariate-filter-objectives} discusses additional objective functions for multivariate filter feature selection (cf.~Section~\ref{sec:afs:approach:objectives:white-box}).
Section~\ref{sec:afs:appendix:univariate-complete-optimization-problem} provides complete definitions of the alternative-feature-selection problem (cf.~Section~\ref{sec:afs:approach:constraints}) for the univariate objective (cf.~Equation~\ref{eq:afs:univariate-filter}).
Section~\ref{sec:afs:appendix:univariate-pre-selection} proposes how to speed up optimization for the univariate objective (cf.~Equation~\ref{eq:afs:univariate-filter}).
Section~\ref{sec:afs:appendix:complexity} complements the complexity analysis (cf.~Section~\ref{sec:afs:approach:complexity}).
Section~\ref{sec:afs:appendix:greedy-depth} proposes another heuristic search method for the univariate objective (cf.~Equation~\ref{eq:afs:univariate-filter}), complementing Section~\ref{sec:afs:approach:univariate-heuristics}.
Section~\ref{sec:afs:appendix:evaluation} contains additional evaluation results (cf.~Section~\ref{sec:afs:evaluation}).

\subsection{Aggregation Operators for the Simultaneous-Search Problem}
\label{sec:afs:appendix:simultaneous-objective-aggregation}

In this section, we discuss operators to aggregate the feature-set quality of multiple alternatives in the objective of the simultaneous-search problem (cf.~Equation~\ref{eq:afs:afs-simultaneous}).

\paragraph{Sum-aggregation}

The arguably simplest way to aggregate the qualities of multiple feature sets is to sum them up, which we call \emph{sum-aggregation}:
%
\begin{equation}
	\max_{s^{(0)}, \dots, s^{(a)}} \sum_{i=0}^a Q(s^{(i)},X,y)
	\label{eq:afs:afs-simultaneous-sum-objective}
\end{equation}
%
While this objective fosters a high average quality of feature sets, it does not guarantee that the alternatives have similar quality:
%
\begin{example}[Sum-aggregation]
Consider $n=6$~features with univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}) $q = (9,8,7,3,2,1)$, feature-set size~$k=3$, number of alternatives~$a=2$, and dissimilarity threshold~$\tau = 0.5$, which permits an overlap of one feature between sets here.
Exact sequential search yields the selection $s^{(0)} = (1,1,1,0,0,0)$, $s^{(1)} = (1,0,0,1,1,0)$, and $s^{(2)} = (0,1,0,1,0,1)$, with a summed quality of $24+14+12=50$.
One possible exact simultaneous-search solution consists of the feature sets $s^{(0)} = (1,1,0,1,0,0)$, $s^{(1)} = (1,0,1,0,1,0)$, and $s^{(2)} = (0,1,1,0,0,1)$, with a summed quality of $20+18+16=54$.
Another possible exact simultaneous-search solution is $s^{(0)} = (1,1,0,0,0,1)$, $s^{(1)} = (1,0,1,0,1,0)$, and $s^{(2)} = (0,1,1,1,0,0)$, with a summed quality of $18+18+18=54$.
\label{ex:afs:sum-aggregation}
\end{example}
%
This example allows several insights.
First, exact sequential search yields worse quality than exact simultaneous search here, i.e., 50 vs.~54.
Second, the feature-set qualities of the sequential solution, i.e., 24, 14, and~12, differ significantly.
Third, exact simultaneous search can yield multiple solutions whose feature-set quality is differently balanced.
Here, the feature-set qualities in the second simultaneous-search solution, i.e., 18, 18, and~18, are more balanced than in the first, i.e., 20, 18, and~16.
However, both solutions are equally optimal for sum-aggregation.

\paragraph{Min-aggregation}

To actively foster balanced feature-set qualities in simultaneous search, we propose \emph{min-aggregation} in the objective:
%
\begin{equation}
	\max_{s^{(0)}, \dots, s^{(a)}} \min_{i \in \{0, \dots, a\}} Q(s^{(i)},X,y) \\
	\label{eq:afs:afs-simultaneous-min-objective}
\end{equation}
%
In the terminology of social choice theory, this objective uses an egalitarian rule instead of a utilitarian one~\cite{myerson1981utilitarianism}.
In particular, \emph{min-aggregation} maximizes the quality of the worst selected alternative.
Thereby, it incentivizes all alternatives to have high quality and implicitly balances their quality.

Note that optimizing the objective with either sum-aggregation or min-aggregation does not necessarily optimize the other.
We already showed a solution optimizing sum-aggregation but not min-aggregation (cf.~Example~\ref{ex:afs:sum-aggregation}).
In the following, we demonstrate the other direction:
%
\begin{example}[Min-aggregation]
Consider $n=6$~features with univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}) $q = (11,10,6,5,4,1)$, feature-set size~$k=3$, number of alternatives~$a=1$, and dissimilarity threshold~$\tau = 0.5$, which permits an overlap of one feature between sets here.
One solution optimizing the objective with min-aggregation is $s^{(0)} = (1,1,0,0,1,0)$ and $s^{(1)} = (1,0,1,1,0,0)$, with a summed quality of $25+22=47$.
Another solution is $s^{(0)} = (1,1,0,0,0,1)$ and $s^{(1)} = (1,0,1,1,0,0)$, with a summed quality of $22+22=44$.
\label{ex:afs:min-aggregation}
\end{example}
%
While both solutions have the same minimum feature-set quality, only the first solution optimizes the objective with sum-aggregation.
In particular, min-aggregation permits reducing the quality of feature sets as long as the latter remains above the minimum quality of all sets.

From the technical perspective, Equation~\ref{eq:afs:afs-simultaneous-min-objective} has the disadvantage of being non-linear regarding the decision variables $s^{(0)}, \dots, s^{(a)}$.
However, we can linearize it with one constraint per feature set and an auxiliary variable~$Q_{\text{min}}$:
%
\begin{equation}
	\begin{aligned}
		\max_{s^{(0)}, \dots, s^{(a)}} &\quad &Q_{\text{min}} & \\
		\text{subject to:} &\quad \forall i \in \{0, \dots, a\}: &Q_{\text{min}} &\leq Q(s^{(i)},X,y) \\
		&\quad & Q_{\text{min}} &\in \mathbb{R}
	\end{aligned}
	\label{eq:afs:afs-simultaneous-min-objective-linear}
\end{equation}
%
As we maximize~$Q_{\text{min}}$, this variable will implicitly assume the actual minimum value of~$Q(s^{(i)},X,y)$ with equality since the solution would not be optimal otherwise.
This situation relieves us from introducing further auxiliary variables that are usually necessary when linearizing maximum or minimum expressions~\cite{mosek2022modeling}.

\paragraph{Further approaches for balancing quality}

Min-aggregation provides no control or guarantee of how much the feature-set qualities will actually differ between alternatives since it only incentivizes high quality for all sets.
One can alleviate this issue by adapting the objective or constraints.
First, related work on \textsc{Multi-Way Number Partitioning} also uses other objectives for balancing~\cite{korf2010objective, lawrinenko2017identical} (cf.~Section~\ref{sec:afs:appendix:complexity:related-work}).
E.g., one could minimize the difference between maximum and minimum feature-set quality.
Second, one could use sum-aggregation but constrain the minimum or maximum quality of sets, or the difference between the qualities.
However, such constraint-based approaches introduce one or several parameters bounding feature-set quality, which are difficult to determine a priori.
Third, one could treat balancing qualities as another objective besides maximizing the summed quality.
One can then optimize two objectives simultaneously, filtering results for Pareto-optimal solutions or optimizing a weighted combination of the two objectives.
In both cases, users may need to define an acceptable trade-off between the objectives.
It is an open question if a solution always exists that jointly optimizes min- and sum-aggregation.
If yes, then optimizing a weighted combination of the two objectives would also optimize each of them on its own, assuming positive weights.

\subsection{Further Objectives for Multivariate Filter Methods}
\label{sec:afs:appendix:multivariate-filter-objectives}

While Section~\ref{sec:afs:approach:objectives:white-box} already addressed FCBF and mRMR as multivariate filter feature-selection methods, we discuss the objectives of CFS and Relief here.

\paragraph{CFS}

Correlation-based Feature Selection (CFS)~\cite{hall1999correlation, hall2000correlation} follows a similar principle as mRMR but uses the ratio instead of the difference between a relevance term and a redundancy term for feature-set quality.
Using a bivariate dependency measure $q(\cdot)$ to quantify correlation, the objective is as follows:
%
\begin{equation}
	Q_{\text{CFS}}(s,X,y) = \frac{\sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j}{\sqrt{\sum_{j=1}^{n} s_j + \sum_{j_1=1}^{n} \sum_{\substack{j_2=1 \\ j_2 \neq j_1}}^{n} q(X_{\cdot{}j_1}, X_{\cdot{}j_2}) \cdot s_{j_1} \cdot s_{j_2}}}
	\label{eq:afs:cfs}
\end{equation}
%
One can square this objective to remove the square root in the denominator~\cite{nguyen2010towards}.
Nevertheless, the objective remains non-linear in the decision variables~$s$ since it involves a fraction and multiplications between variables.
However, one can linearize the objective with additional variables and constraints~\cite{nguyen2010improving, nguyen2010towards}, allowing to formulate alternative feature selection for CFS as a linear problem.

\paragraph{Relief}

Relief~\cite{kira1992feature, robnik1997adaptation} builds on the idea that data objects with a similar value of the prediction target should have similar feature values, but data objects that differ in their target should differ in their feature values.
Relief assigns a score to each feature by sampling data objects and quantifying the difference in feature values and target values compared to their nearest neighbors.
We deem Relief to be multivariate since the nearest-neighbor computations involve all features instead of considering them independently.
However, the resulting feature scores can directly be put into the univariate objective (cf.~Equation~\ref{eq:afs:univariate-filter}) to obtain a linear problem.
One can also use Relief scores in CFS to consider feature redundancy~\cite{hall1999correlation, hall2000correlation}, which the default Relief does not.

\subsection{Complete Specifications of the Optimization Problem for the Univariate Objective}
\label{sec:afs:appendix:univariate-complete-optimization-problem}

In this section, we provide complete specifications of the alternative-feature-selection problem for sequential and simultaneous search as integer-linear problem.
In particular, we combine all relevant definitions and equations from Section~\ref{sec:afs:approach}.
We use the objective of univariate filter feature selection (cf.~Equation~\ref{eq:afs:univariate-filter}).
The corresponding feature qualities $q(\cdot)$ are constants in the optimization problem.
Further, we use the Dice dissimilarity (cf.~Equation~\ref{eq:afs:dice-rearranged-equal-size}) to measure feature-set dissimilarity for alternatives.
The dissimilarity threshold~$\tau \in [0,1]$ is a user-defined constant.
Finally, we assume fixed, user-defined feature-set sizes~$k \in \mathbb{N}$.

\paragraph{Sequential-search problem}

In the sequential case, only one feature set~$F_s$ is variable in the optimization problem, while the existing feature sets $F_{\bar{s}} \in \mathbb{F}$ with their selection vectors $\bar{s}$ are constants.
%
\begin{equation}
	\begin{aligned}
		\max_s &\quad & Q_{\text{uni}}(s,X,y) &= \sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j \\
		\text{subject to:} &\quad \forall F_{\bar{s}} \in \mathbb{F}: & \sum_{j=1}^n s_j \cdot \bar{s}_j &\leq (1 - \tau) \cdot k \\
		&\quad & \sum_{j=1}^n s_j &= k \\
		&\quad & s &\in \{0,1\}^n
	\end{aligned}
	\label{eq:afs:afs-sequential-complete}
\end{equation}
%
\paragraph{Simultaneous-search problem}

In the simultaneous case, all feature sets are variable.
$a \in \mathbb{N}_0$ denotes the number of alternatives, which corresponds to the number of feature sets minus one.
Next, we introduce auxiliary variables to linearize products between variables (cf.~Equation~\ref{eq:afs:product-linear}).
Finally, we use sum-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-sum-objective}) over alternatives in the objective here.
%
\begin{equation}
	\begin{aligned}
		\max_{s^{(0)}, \dots, s^{(a)}} &\quad & \sum_i Q_{\text{uni}}(s^{(i)},X,y) &= \sum_i \sum_j q(X_{\cdot{}j},y) \cdot s^{(i)}_j\\
		\text{subject to:} &\quad \forall i_1~\forall i_2: & \sum_j t^{(i_1,i_2)}_j &\leq (1 - \tau) \cdot k \\
		&\quad \forall i_1~\forall i_2~\forall j: & t^{(i_1,i_2)}_j &\leq s^{(i_1)}_j \\
		&\quad \forall i_1~\forall i_2~\forall j: & t^{(i_1,i_2)}_j &\leq s^{(i_2)}_j \\
		&\quad \forall i_1~\forall i_2~\forall j: & 1 + t^{(i_1,i_2)}_j &\geq s^{(i_1)}_j + s^{(i_2)}_j \\
		&\quad \forall i: & \sum_j s^{(i)}_j &= k \\
		&\quad \forall i: & s^{(i)} &\in \{0,1\}^n \\
		&\quad \forall i_1~\forall i_2: & t^{(i_1,i_2)} &\in \{0,1\}^n \\
		\text{with indices:} &\quad & i &\in \{0, \dots, a\} \\
		&\quad & i_1 &\in \{1, \dots, a\} \\
		&\quad & i_2 &\in \{0, \dots, i_1-1\} \\
		&\quad & j &\in \{1, \dots, n\}
	\end{aligned}
	\label{eq:afs:afs-simultaneous-complete}
\end{equation}

\subsection{Pre-Selection for the Univariate Objective}
\label{sec:afs:appendix:univariate-pre-selection}

In this section, we describe how to potentially speed up the optimization of the univariate objective (cf.~Equation~\ref{eq:afs:univariate-filter}) by \emph{pre-selection} if the user-defined feature-set sizes~$k$ and the number of alternatives~$a$ are small.

The univariate objective is monotonic in the features' qualities~$q(X_{\cdot{}j},y)$ and the selection decisions~$s_j$.
In particular, the objective is non-decreasing when replacing a feature with another one of higher quality.
Further, unless some feature qualities are negative, selecting more features does not decrease the objective.
Sum-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-sum-objective}) and min-aggregation (cf.~Equation~\ref{eq:afs:afs-simultaneous-min-objective}) for the simultaneous-search problem are monotonic as well.

Thus, assuming $(a + 1) \cdot k < n$, it suffices to use the $(a + 1) \cdot k$ highest feature qualities when searching for an optimal solution out of $a + 1$ feature sets.
Due to monotonicity, the remaining feature qualities cannot improve the objective, so one can drop them before optimization.
We call this step \emph{pre-selection}.
While there might also be optimal solutions using the dropped features, their objective value cannot be higher than with pre-selection.
For example, such solutions can arise in case of multiple identical qualities or for min-aggregation in the objective (cf.~Example~\ref{ex:afs:min-aggregation}).
Also, the optimal solution might not contain all pre-selected features, i.e., pre-selection over-approximates the set of selected features.

One can conduct pre-selection before using a solver or any other search mechanism, e.g., exhaustive search.
The latter generally has polynomial runtime regarding~$n$ assuming small, constant $a$ and $k$, i.e., $a \cdot k \in O(1)$ (cf.~Section~\ref{sec:afs:approach:complexity:exhaustive}).
With pre-selection, the pure search cost would even become independent from~$n$, i.e., $O(1)$ under that assumption.
However, one would need to determine the highest feature qualities first, e.g., by sorting all qualities in~$O(n \cdot \log n)$ or iteratively determining the maximum quality in~$O((a+1) \cdot k \cdot n)$.

\subsection{Computational Complexity}
\label{sec:afs:appendix:complexity}

In this section, we provide details for our analysis of computational complexity (cf.~Section~\ref{sec:afs:approach:complexity}).
In particular, we discuss a special case of exhaustive simultaneous search (cf.~Section~\ref{sec:afs:appendix:complexity:exhaustive-simultaneous-special-case}), outline related work (cf.~Section~\ref{sec:afs:appendix:complexity:related-work}), provide proofs (cf.~Section~\ref{sec:afs:appendix:complexity:proofs}), and describe future work (cf.~Section~\ref{sec:afs:appendix:complexity:future-work}).

\subsubsection{A Special Case of Exhaustive Simultaneous Search}
\label{sec:afs:appendix:complexity:exhaustive-simultaneous-special-case}

The complexity of exhaustive simultaneous search is lower than in Proposition~\ref{prop:afs:complexity-exhaustive-simultaneuos} for the special case~$0 < \tau \cdot k \leq 1$, i.e., if feature sets need to differ in only one feature.
There, each feature set is an alternative to each other unless both sets are identical.
Thus, each set of $a + 1$ distinct feature sets constitutes a valid solution, and further constraint checking is unnecessary.
Hence, instead of iterating over sets of feature sets, one can iterate over individual feature sets and maintain a buffer containing the $a + 1$ feature sets with the highest quality.
For each feature set iterated over, one needs to determine if its quality is higher than the lowest feature-set quality in the buffer and replace it if yes.
This procedure has a runtime of $O((a + 1) \cdot n^k)$ without the cost of evaluating the objective.
I.e., unlike in Proposition~\ref{prop:afs:complexity-exhaustive-simultaneuos}, the number of alternatives~$a$ is not part of the exponent anymore, and the cost corresponds to the search for one feature set times the cost of updating the buffer.
For large $a$, one can implement the buffer as a heap, thereby reducing the linear factor regarding~$a$ to a logarithmic one.

\subsubsection{Related Work}
\label{sec:afs:appendix:complexity:related-work}

In this section, we discuss related work on $\mathcal{NP}$-hard problems that resemble alternative feature-selection with univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}), providing background for Section~\ref{sec:afs:approach:complexity:univariate}.

\paragraph{Integer programming}

The univariate objective and several other feature-selection methods allow us to phrase alternative feature selection as a 0-1 integer linear program (cf.~Section~\ref{sec:afs:approach:objectives:white-box}).
\textsc{Integer Programming} is $\mathcal{NP}$-complete in general, even for binary decision variables~\cite{garey2003computers, karp1972reducibility}.
Thus, alternative feature selection with a white-box objective suitable for \textsc{Integer Programming} resides in $\mathcal{NP}$.
However, it could still be easier since alternative feature selection only uses particular constraint types instead of expressing arbitrary integer linear problems.
Vice versa, the membership in $\mathcal{NP}$ based on \textsc{Integer Programming} assumes a particular encoding of alternative feature selection, i.e., each constraint is stored separately and counts towards the problem's input size.
If we instead define the input size only as the number of features~$n$ or the total encoding length of the objective function plus parameters~$a$, $k$, and $\tau$, the problem could be harder than $\mathcal{NP}$, e.g., for a high number of alternatives.
In particular, increasing the number of alternatives would increase the encoding length logarithmically but the cost of constraint checking quadratically.

\paragraph{Multi-way number partitioning / multiprocessor scheduling}

The literature provides different formulations of \textsc{Multi-Way Number Partitioning} and \textsc{Multiprocessor Scheduling}.
In particular, different objectives formalize the notion of balanced subset sums and can lead to different optimal solutions~\cite{korf2010objective, lawrinenko2017identical}.
The maximin formulation we use for min-aggregation in the simultaneous-search problem (cf.~Equations~\ref{eq:afs:afs-simultaneous} and~\ref{eq:afs:afs-simultaneous-min-objective}) is one such notion.

There are several exact algorithms to solve \textsc{Multi-Way Number Partitioning}, e.g., using branch-and-bound approaches that might have exponential runtime~\cite{haouari2008maximizing, schreiber2018optimal, walter2017improved}.
For a fixed number of partitions, the problem is weakly $\mathcal{NP}$-complete since it admits pseudo-polynomial algorithms~\cite{garey2003computers, korf2009multi}.
Such algorithms run in polynomial time if the input numbers are bounded to a particular size known in advance.
Since our feature qualities typically are real numbers, one would need to scale and discretize them to apply such an algorithm.
Also, for an arbitrary number of partitions, the problem is strongly $\mathcal{NP}$-complete, so no pseudo-polynomial algorithm can exist unless $\mathcal{P}=\mathcal{NP}$~\cite{garey2003computers}.

However, $\mathcal{NP}$-completeness does not exclude the existence of approximation routines that run in polynomial time and have a guaranteed quality relative to the optimal solution.
For example, \cite{alon1998approximation, deuermeyer1982scheduling, woeginger1997polynomial}~present such algorithms for the maximin formulation of \textsc{Multi-Way Number Partitioning}, which corresponds to our objective with min-aggregation.
In particular, \cite{alon1998approximation, woeginger1997polynomial} describe polynomial-time approximation schemes (PTAS), which can provide a solution arbitrarily close to the optimum.
However, the runtime depends on the desired approximation ratio and can grow exponentially the more precision is desired.
Unless $\mathcal{P}=\mathcal{NP}$, the strong $\mathcal{NP}$-completeness of the problem prevents the existence of a fully polynomial-time approximation scheme (FPTAS), which would only polynomially depend on the precision of approximation~\cite{alon1998approximation, woeginger1997polynomial}.
However, an FPTAS does exist for each fixed number of partitions~\cite{sahni1976algorithms}.
Further, besides approximations, the problem also has polynomial-time exact algorithms if certain parameters of the problem are fixed, e.g., the number of unique numbers to be partitioned or the largest number~\cite{mnich2018parameterized}.
Thus, the problem is fixed-parameter tractable ($\mathcal{FPT}$) for an appropriate definition of `parameter'.

\paragraph{Balanced number partitioning / k-partitioning}

While the previous approaches considered sets of arbitrary sizes, there are number-partitioning problems with constrained~$k$ as well, e.g., called \textsc{Balanced Number Partitioning} or \textsc{K-Partitioning}.
The problem formulations differ in their objective and cardinality constraints, e.g., if equalities or inequalities are used.

For the minimax objective, \cite{babel1998thek, michiels2012computer, zhang2011heuristic} propose heuristic algorithms, some with approximation guarantees.
\cite{babel1998thek} also provides a bound of the objective value relative to the unconstrained case.
Further, there is a PTAS for each fixed set size~$k$~\cite{michiels2012computer}.
Finally, the problem exhibits a polynomial-time exact algorithm for $k=2$~\cite{dellamico2004heuristic, dellamico2001bounds} and an FPTAS for $k=n/2$~\cite{woeginger2005comment}.

One can also loosen the cardinality constraints by requiring $\leq k$ instead of $= k$.
Further, the cardinality~$k$ might vary between partitions.
This generalized problem is strongly $\mathcal{NP}$-hard but has heuristics running in polynomial time~\cite{kellerer2011a32approximation}.
In particular, \cite{chen2016efficient} provides an efficient PTAS (EPTAS).

As another problem formulation, \cite{chen20023partitioning, he2003kappa, lawrinenko2018reduction} use a maximin objective as we do.
This objective was rarely addressed in combination with cardinality constraints in the literature~\cite{lawrinenko2018reduction}.
Also, all these three references use $\leq k$ constraints instead of $= k$.
Again, this problem is strongly $\mathcal{NP}$-hard~\cite{he2003kappa}, but \cite{chen20023partitioning, he2003kappa, lawrinenko2018reduction} propose approximation algorithms, partly with quality guarantees.

\paragraph{Other partitioning problems}

There are other $\mathcal{NP}$-complete problems that partition elements into non-overlapping subsets~\cite{garey2003computers}.
E.g., \textsc{Partition}~\cite{karp1972reducibility} asks if one can partition a set of elements with positive integer weights into two subsets with the same subset sum.
\textsc{3-Partition}~\cite{garey2003computers} demands a partitioning into three-element subsets with an identical, predefined subset sum of the elements' positive integer weights.
In contrast to these two problems, we do not require alternative feature sets to have the same quality.

\paragraph{Bin covering}

\textsc{Bin Covering}~\cite{assmann1984dual} distributes elements with individual weights into bins such that the number of bins is maximal and the summed weights in each bin surpass a predefined limit.
\cite{lawrinenko2017identical} noted a relationship between \textsc{Multi-Way Number Partitioning} and \textsc{Bin Covering}, which may improve solution approaches for either problem~\cite{walter2017lower, walter2017improved}.
In our case, we could maximize the number of alternatives such that each feature set's quality exceeds a threshold.

\paragraph{Multiple knapsack}

The simultaneous-search problem with sum-aggregation, $\tau=1$, and univariate feature qualities is a special case of the \textsc{Multiple Knapsack} problem~\cite{chekuri2005polynomial}.
The latter involves knapsacks, i.e., sets with individual capacities, and elements with individual weights and profits.
The goal is to assign elements to knapsacks such that the summed profit of selected elements is maximal.
Each element can be assigned to at most one knapsack, and the weights of all elements in the knapsack must not violate its capacity.
This problem is strongly $\mathcal{NP}$-complete in general, though it exhibits a PTAS~\cite{chekuri2005polynomial}.
However, our problem is a special case where the feature qualities act as profits, the feature-set sizes are capacities, and each feature has a weight of~1.
These uniform weights enable the polynomial-runtime result stated in Proposition~\ref{prop:afs:complexity-partitioning-sum}.

\subsubsection{Proofs}
\label{sec:afs:appendix:complexity:proofs}

In this Section, we provide proofs for propositions from Section~\ref{sec:afs:approach:complexity:univariate}.

\paragraph{Proof of Proposition~\ref{prop:afs:complexity-incomplete-partitioning-min-constrained-k}}
%
\begin{proof}
Let an arbitrary problem instance~$I$ of the complete-partitioning problem be given and the feature-set size~$k$ be fixed.
We add one feature~$f'$ to~$I$ and keep $a$, $k$, and $\tau$ as before, obtaining an instance~$I'$ of the incomplete-partitioning problem since one feature will not be selected.
We choose the quality~$q'$ of~$f'$ to be lower than the quality of all other features in~$I$.
Since the univariate objective with min-aggregation is monotonically increasing in the selected feature qualities, selecting feature~$f'$ in a solution of~$I'$ does not have any benefit since~$f'$ would replace a feature with higher quality.
If~$f'$ is not selected, then this solution of~$I'$ also solves~$I$.
However, if the qualities of the resulting alternatives are not equal, $f'$ might be chosen in a set that does not have the minimum quality of all sets since only the latter determines the overall objective value (cf.~Example~\ref{ex:afs:min-aggregation}).
In that case, we replace $f'$ with the remaining feature that was not selected instead; the objective value remains the same, and the solution becomes valid for~$I$.
Thus, in any case, we can easily transform a solution for~$I'$ to a solution for~$I$.
	
This argument shows that an algorithm for incomplete partitioning can solve arbitrary complete-partitioning problem instances with negligible computational overhead.
Thus, a polynomial-time algorithm for incomplete partitioning could also solve complete partitioning polynomially.
However, the latter problem type is $\mathcal{NP}$-complete (cf.~Proposition~\ref{prop:afs:complexity-partitioning-min-constrained-k}), so incomplete partitioning has to be $\mathcal{NP}$-hard.
Since checking a solution for incomplete partitioning needs only polynomial time, we obtain membership in $\mathcal{NP}$ and thereby $\mathcal{NP}$-completeness.
\end{proof}

\paragraph{Proof of Proposition~\ref{prop:afs:complexity-no-partitioning-min-constrained-k}}
%
\begin{proof}
Let an arbitrary problem instance~$I$ of the complete-partitioning problem be given and the feature-set size~$k$ be fixed.
We create a new problem instance~$I'$ by adding a new feature~$f'$ and increasing the feature-set size to $k' = k + 1$.
Further, we set $\tau' = (k' - 1) / k'$, thereby allowing an overlap of at most one feature between feature sets.
Also, we choose~$f'$ to have a considerably higher quality~$q'$ than all other features.
The goal is to force the selection of~$f'$ in all feature sets such that any other solution would be worse, no matter which other features are selected.
One possible choice is $q' = \sum_{j=1}^n q_j + \varepsilon$, with $\varepsilon \in \mathbb{R}_{> 0}$ being a small positive number, or, if the qualities are integers, $\varepsilon = 1$.
This quality~$q'$ of~$f'$ is higher than of any feature set not containing it.
Thus, a solution for~$I'$ contains~$f'$ in each feature set while the remaining features are part of exactly one feature set.
Hence, we remove~$f'$ to get feature sets of size~$k = k' - 1$ that constitute an optimal solution for the original problem instance~$I$.
	
This transformation shows how an algorithm for problem instances with $\tau < 1$ can help solve arbitrary problem instances with $\tau = 1$.
Given the $\mathcal{NP}$-completeness of the latter problem, we obtain $\mathcal{NP}$-hardness of the former.
\end{proof}
%
Adding the proposed~$f'$ with a high quality~$q'$ enlarges the size of the problem instance.
However, the transformation from $I$ to $I'$ still runs in polynomial time and increases the input size by at most a fixed factor.
In particular, encoding a problem instance involves $n$~feature qualities and the values of $a$, $k$, and $\tau$.
Assuming the feature qualities in $I$ have an average encoding size of $c \in \mathbb{R}$, the overall quality encoding has the size $c \cdot n$.
As~$q'$ roughly equals the sum of all feature qualities, its encoding size is upper-bounded by $c \cdot n$ if we disregard~$\epsilon$.
The change of $k$ and $\tau$ is negligible for the encoding size of the problem instance overall.
In consequence, the input size of~$I'$ is at most roughly double the size of~$I$.
If we explicitly stored all the constraints instead of only the relevant parameters, we would obtain a similar result:
Besides adding~$q'$ to the objective, all constraints would accommodate one new feature, independent of its quality, increasing their encoding size from $O(n)$ to $O(n+1)$, i.e., less than double.

One can extend the reduction above from $\tau' = (k' - 1) / k'$ to all other $\tau > 0$.
In particular, for a fixed feature set-size~$k$, there is only a finite number of $\tau$ values leading to different set overlaps, i.e., $\tau = \{0, 1/k, \dots, (k - 1) / k, 1\}$.
The highest overlap except~$\tau=0$ requires creating an instance $I'$ with $\tau'= 1/k$ from an instance with $\tau = 1$.
For this purpose, $k^2 - k$ features need to be added since $\tau' = k / k' = k / (k + k^2 -k) = 1/k$.
I.e., $k$ out of $k' = k^2$ features need to form a complete partitioning, while the remaining $k^2 - k$~features occur in each feature set and will be removed after solving~$I'$.
The maximum number of features to be added is polynomial in~$k$ and thereby also polynomial in~$n$.

\paragraph{Proof of Proposition~\ref{prop:afs:complexity-partitioning-sum}}
%
\begin{proof}
For a complete partitioning, we must use each of the $n$~features exactly once.
How we distribute the features among sets does not change the objective value, which is the sum of all $n$~qualities in any case.
We only need to ensure that each feature set satisfies cardinality constraints if the latter exist.
Thus, `searching' for alternatives amounts to iterating over the features once to assign them to the feature sets.
Hence, the time complexity is $O(n)$.

For an incomplete partitioning, we use the monotonicity of the univariate objective with sum-aggregation (cf.~Section~\ref{sec:afs:appendix:univariate-pre-selection}):
This objective cannot decrease when selecting features of higher quality.
Thus, we order the features decreasingly by their individual quality.
Next, we pick features without replacement until we have the desired number of alternatives with the desired feature-set sizes.
Again, assigning features to sets does not matter for the objective value.
Due to the quality-based sorting, the time complexity is~$O(n \cdot \log n)$.
If only a small fraction of features is used, one might slightly improve complexity by iteratively picking the maximum instead of sorting all qualities.
\end{proof}

\subsubsection{Future Work}
\label{sec:afs:appendix:complexity:future-work}

In this section, we outline future work on alternative feature selection from the complexity-theory perspective, supplementing the Sections~\ref{sec:afs:approach:complexity} and~\ref{sec:afs:conclusion:future-work}.

\paragraph{Scenarios of alternative feature selection}

Our prior complexity analyses focused on special cases of alternative feature selection.
E.g., while we obtained $\mathcal{NP}$-hardness for min-aggregation with feature-set overlap (cf.~Proposition~\ref{prop:afs:complexity-no-partitioning-min-constrained-k}), an analysis of sum-aggregation with overlap is open, even for the sequential-search problem.
Sum-aggregation admits polynomial runtime for $\tau=1$ (cf.~Proposition~\ref{prop:afs:complexity-partitioning-sum}), but this result might not extend to~$\tau < 1$.
In particular, $\tau < 1$ increases the number of solution candidates, which could negatively affect the runtime.

Further, our complexity analyses mostly assumed univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter}).
Other feature-selection methods can reside in different complexity classes.

\paragraph{Complexity classes}

For analyzing other scenarios of alternative feature selection, several questions spring to mind.
First, one could establish a complexity result like $\mathcal{NP}$-hardness or membership in~$\mathcal{P}$.
In the former case, there might be pseudo-polynomial approaches or (F)PTAS.
As a first step in that direction, we showed membership in complexity class~$\mathcal{APX}$ under certain conditions (cf.~Proposition~\ref{prop:afs:approximation-apx}), i.e., there are polynomial-time algorithms yielding constant-factor approximations.
One might attempt to tighten the quality bounds we derived.
Further, there might be efficient exact or approximate algorithms for certain types of problem instances, e.g., satisfying additional assumptions regarding feature-set quality or the parameters~$k$, $a$, and $\tau$.
Finally, while we placed alternative feature selection in class~$\mathcal{XP}$ (cf.~Proposition~\ref{prop:afs:complexity-simultaneuos-xp}), one might prove membership or hardness for more specific parameterized complexity classes.

\paragraph{Related problem formulations}

We only focused on the optimization problem of alternative feature selection until now.
Another interesting question is how many alternatives exist for a given $n$, $k$, and $\tau$, regardless of their quality.
Also, given the number of alternatives as well, it would be interesting to have an exact or approximate estimate for the number of valid solutions for alternative feature selection, i.e., sets of feature sets.
While both these estimates are straightforward for $\tau = 1$, allowing arbitrary~$\tau$ poses a larger challenge.
Finally, one could re-formulate alternative feature selection similar to \textsc{Bin Covering} (cf.~Section~\ref{sec:afs:appendix:complexity:related-work}) and analyze this problem in detail.

\subsection{Greedy Depth Search for the Univariate Objective}
\label{sec:afs:appendix:greedy-depth}

In this section, we propose another heuristic search method for univariate feature qualities (cf.~Equation~\ref{eq:afs:univariate-filter} and Section~\ref{sec:afs:appendix:univariate-complete-optimization-problem}), complementing the methods discussed in Section~\ref{sec:afs:approach:univariate-heuristics}.
In particular, the new method \emph{Greedy Depth Search} is a sequential search method that generalizes \emph{Greedy Replacement Search} and allows to obtain more than $\frac{n - k}{\lceil \tau \cdot k \rceil}$ alternatives.

\paragraph{Algorithm}

\begin{algorithm}[tp]
	\DontPrintSemicolon
	\KwIn{Univariate feature qualities~$q_j$ with $j \in \{1, \dots, n\}$, \newline
		Feature-set size~$k$, \newline
		Number of alternatives~$a$, \newline
		Dissimilarity threshold~$\tau$}
	\KwOut{List of feature-selection decision vectors~$s^{(\cdot)}$}
	\BlankLine
	$indices \leftarrow$ sort\_indices($q$, order=descending) \tcp*{Order by qualities} \label{al:afs:greedy-depth:line:sorting}
	$feature\_positions \leftarrow \{0\}^k$ \tcp*{Indices of indices of features} \label{al:afs:greedy-depth:line:position-init-start}
	\For(\tcp*[f]{Start with top $k$ features}){$p \leftarrow 1$ \KwTo $k$}{
		$feature\_positions[p] \leftarrow p$ \tcp*{Ordered by qualities as well} \label{al:afs:greedy-depth:line:position-init-end}
	}
	$i \leftarrow 0$\ \tcp*{Number of current alternative}
	$has\_next\_solution \leftarrow$ \textbf{true} \;
	\While{$i \leq a$ \textbf{and} $has\_next\_solution$}{ \label{al:afs:greedy-depth:line:main-loop-start} \label{al:afs:greedy-depth:line:stop}
		$s^{(i)} \leftarrow \{0\}^n$ \; \label{al:afs:greedy-depth:line:selection-start}
		\For(\tcp*[f]{Select $k$ features, indexed by quality}){$p \leftarrow 1$ \KwTo $k$}{
			$j \leftarrow indices[feature\_positions[p]]$ \;
			$s^{(i)}_j \leftarrow 1$\;
		} \label{al:afs:greedy-depth:line:selection-end}
		\If{is\_valid\_alternative($s^{(i)}$, $\{s^{(0)}, \dots, s^{(i-1)}\}$)}{ \label{al:afs:greedy-depth:line:check-valid}
			$i \leftarrow i + 1$ \tcp*{Else, $s^{(i)}$ overwritten in next iteration}
		}
		$p \leftarrow k$ \tcp*{Update feature positions, starting with last} \label{al:afs:greedy-depth:line:position-update-start}
		\While{$p \geq 1$}{
			$position \leftarrow feature\_positions[p]$ \;
			\If(\tcp*[f]{Position can be increased}){$position < n + p - k$}{ \label{al:afs:greedy-depth:line:check-position-update}
				\For(\tcp*[f]{Also update later positions}){$\Delta_p \leftarrow 0$ \KwTo $k - p$}{ \label{al:afs:greedy-depth:line:quality-decrease-start}
					$feature\_positions[p + \Delta_p] \leftarrow position + \Delta_p + 1$ \; \label{al:afs:greedy-depth:line:quality-decrease-end}
				}
				$p \leftarrow -1$ \tcp*{Position update finished}
			}
			\Else(\tcp*[f]{Position cannot be increased}){
				$p \leftarrow p - 1$ \tcp*[f]{Also update at least one prior position} \label{al:afs:greedy-depth:line:continue-prior-position}
			}
		}
		\If(\tcp*[f]{Updating positions further would violate $n$}){$p = 0$}{ \label{al:afs:greedy-depth:line:position-update-impossible}
			$has\_next\_solution \leftarrow$ \textbf{false} \; \label{al:afs:greedy-depth:line:main-loop-end} \label{al:afs:greedy-depth:line:position-update-end}
		}
	}
	\Return{$s^{(0)}, \dots, s^{(i)}$}
	\caption{\emph{Greedy Depth Search} for alternative feature sets.}
	\label{al:afs:greedy-depth}
\end{algorithm}

Algorithm~\ref{al:afs:greedy-depth} outlines \emph{Greedy Depth Search}.
As in the other two heuristics, we start by sorting the features decreasingly according to their qualities~$q_j$ (Line~\ref{al:afs:greedy-depth:line:sorting}).
However, instead of keeping the same $\lfloor (1 - \tau) \cdot k \rfloor$~features in each alternative and only replacing the remaining ones, we now allow all features to be replaced.
In particular, we may exhaustively iterate over all feature sets, depending on the number of alternatives~$a$.
Thus, we maintain not only one feature position as before but a length-$k$ array of the feature positions for the current feature set (Lines~\ref{al:afs:greedy-depth:line:position-init-start}--\ref{al:afs:greedy-depth:line:position-init-end}).
This array represents feature indices regarding the sorted qualities and is sorted increasingly, which prevents evaluating the same feature set, only with different feature order, multiple times.

In the main loop of the algorithm, we find alternatives sequentially
(Lines~\ref{al:afs:greedy-depth:line:main-loop-start}--\ref{al:afs:greedy-depth:line:main-loop-end}).
For each potential alternative, we select the features based on the position array (Lines~\ref{al:afs:greedy-depth:line:selection-start}--\ref{al:afs:greedy-depth:line:selection-end}).
We check the resulting feature set against the constraints for alternatives (Line~\ref{al:afs:greedy-depth:line:check-valid}) and only store it if it is valid.
This check was unnecessary in the other two heuristics, which only formed valid alternatives by design.

Next, we update the feature positions for the next potential alternative (Lines~\ref{al:afs:greedy-depth:line:position-update-start}--\ref{al:afs:greedy-depth:line:position-update-end}).
First, we try to replace the lowest-quality feature in the current feature set by advancing one position in the sorted qualities.
This step may not be possible, as the feature set may already contain the feature with the overall lowest quality, i.e., position~$n$ in the array of sorted qualities (Line~\ref{al:afs:greedy-depth:line:check-position-update}).
In this case, we try to replace the second-lowest-quality feature in the current feature set by advancing its position.
If this action is impossible as well, we iterate further over positions in the current feature set by increasing quality (Line~\ref{al:afs:greedy-depth:line:continue-prior-position}).
Once we find a feature position that we can increase, we also advance all subsequent, i.e., lower-quality, positions accordingly.
Hence, the feature positions remain sorted by decreasing quality (Lines~\ref{al:afs:greedy-depth:line:quality-decrease-start}--\ref{al:afs:greedy-depth:line:quality-decrease-end}).

We repeat the main loop until we reach the desired number of alternatives~$a$ or until we cannot update any feature position without exceeding the number of features~$n$, i.e., we cannot form another alternative (Lines~\ref{al:afs:greedy-depth:line:stop} and~\ref{al:afs:greedy-depth:line:position-update-impossible}).
%
\begin{example}[Algorithm of \emph{Greedy Depth Search}]
	Consider $n=6$~features with univariate feature qualities $q = (9,8,7,3,2,1)$, feature-set size~$k=4$, number of alternatives~$a=1$, and dissimilarity threshold~$\tau = 0.5$, which permits an overlap of two features between sets here.
	Note that the features are already ordered by quality here, i.e., $indices = (1,2,3,4,5,6)$ (Line~\ref{al:afs:greedy-depth:line:sorting}).
	Next, the algorithm initializes $feature\_positions = (1,2,3,4)$ (Line~\ref{al:afs:greedy-depth:line:position-init-start}--\ref{al:afs:greedy-depth:line:position-init-end}).
	$s^{(0)}$ contains these $k$~features, i.e., $s^{(0)} = (1,1,1,1,0,0)$.
	Given that there are no other alternatives yet, this feature set is valid (Line~\ref{al:afs:greedy-depth:line:check-valid})) and the algorithm moves on to $i=1$.
	
	For forming $s^{(1)}$, the position-update step (Lines~\ref{al:afs:greedy-depth:line:position-update-start}--\ref{al:afs:greedy-depth:line:position-update-end}) first tries to only replace the lowest-quality feature in the alternative, i.e., $feature\_positions = (1,2,3,5)$ and $feature\_positions = (1,2,3,6)$.
	However, neither of these feature sets constitutes a valid alternative regarding~$s^{(0)}$.
	Thus, the algorithm attempts to replace the feature with the second-lowest quality as well, evaluating $feature\_positions = (1,2,4,5)$ and $feature\_positions = (1,2,4,6)$.
	However, the overlap with $s^{(0)}$ is still too large.
	The next value is $feature\_positions = (1,2,5,6)$, which yields the valid alternative $s^{(1)} = (1,1,0,0,1,1)$.
	
	\emph{Greedy Replacement Search} would terminate now since the options for replacing the $\lceil \tau \cdot k \rceil = 2$ lowest-quality features are exhausted.
	In contrast, \emph{Greedy Depth Search} attempts to replace the third-lowest-quality feature, starting with $feature\_positions = (1,3,4,5)$.
	This feature set is not a valid alternative, and neither are the subsequent sets with $feature\_positions = (1,3,4,6)$, $feature\_positions = (1,3,5,6)$, etc.
	After more iterations, the algorithm also replaces the highest-quality feature, starting with $feature\_positions = (2,3,4,5)$.
	Eventually, the algorithm reaches $feature\_positions = (3,4,5,6)$, which yields the valid alternative $s^{(2)} = (0,0,1,1,1,1)$.
	After obtaining $s^{(2)}$, there is no valid update of the feature positions left (Line~\ref{al:afs:greedy-depth:line:position-update-impossible}).
	Thus, the algorithm terminates.
	\label{ex:afs:greedy-depth:algorithm}
\end{example}

\paragraph{Complexity}

The runtime behavior differs from the other two heuristics.
In particular, \emph{Greedy Replacement Search} has the same runtime cost between subsequent alternatives since it directly creates valid alternatives by design.
In contrast, \emph{Greedy Depth Search} iterates over all possible feature sets, and the runtime between valid alternatives may vary.
For each values of $feature\_positions$, the algorithm creates a feature selection in $O(k \cdot n)$ (Lines~\ref{al:afs:greedy-depth:line:selection-start}--\ref{al:afs:greedy-depth:line:selection-end}), checks constraints in $O(a \cdot n)$ (Line~\ref{al:afs:greedy-depth:line:check-valid}), and updates the position array in~$O(k^2)$ (Lines~\ref{al:afs:greedy-depth:line:position-update-start}--\ref{al:afs:greedy-depth:line:position-update-end}).
However, there are $O(n^k)$ potential $feature\_positions$, and \emph{Greedy Depth Search} may exhaustively iterate over them.
This cost is comparable to exhaustive conventional feature selection (cf.~Proposition~\ref{prop:afs:complexity-exhaustive-conventional}) and exhaustive sequential search (cf.~Proposition~\ref{prop:afs:complexity-exhaustive-sequential}).
Unlike the latter, the search does not restart for each alternative, i.e., it only considers each feature set once instead of $a+1$~times.

On the positive side, \emph{Greedy Depth Search} can yield more alternatives than \emph{Greedy Replacement Search} with its $O(n^2)$ cost or \emph{Greedy Balancing Search} with its $O(a \cdot n^2)$ cost.
Nevertheless, in scenarios where the latter two are applicable, i.e., $k + a \cdot \lceil \tau \cdot k \rceil \leq n$, they have a lower cost than \emph{Greedy Depth Search}.
In particular, \emph{Greedy Depth Search} needs $O(n^{\lceil \tau \cdot k \rceil})$~iterations to cover the options for replacing the worst $\lceil \tau \cdot k \rceil$ features in size-$k$ feature sets, which is the search space of the other two heuristics.
In particular, the cost disadvantage relative to the other two heuristics grows with the dissimilarity threshold~$\tau$.
As a remedy, one may use \emph{Greedy Replacement Search} for as many alternatives as possible and then continue with \emph{Greedy Depth Search}, initializing the $feature\_positions$ (Line~\ref{al:afs:greedy-depth:line:position-init-start}--\ref{al:afs:greedy-depth:line:position-init-end}) based on the results of the former heuristic.

\paragraph{Quality}

\emph{Greedy Depth Search} initially yields the same solutions as \emph{Greedy Replacement Search}.
Thus, \emph{Greedy Depth Search} also yields a constant-factor approximation of the optimal solution in case $k + a \cdot \lceil \tau \cdot k \rceil \leq n$ (cf.~Proposition~\ref{prop:afs:approximation-greedy-replacement}).
The quality analysis becomes more involved for further alternatives since these do not contain all top $\lfloor (1 - \tau) \cdot k \rfloor$~features anymore, on which our proof of Proposition~\ref{prop:afs:approximation-greedy-replacement} builds.
Thus, we leave this analysis open for future work.
The quality of alternatives may not even be monotonically decreasing anymore, as the following example shows:
%
\begin{example}[Non-monotonic quality of \emph{Greedy Depth Search}]
	Consider $n=4$~features with univariate feature qualities $q = (9,8,7,1)$, feature-set size~$k=2$, number of alternatives~$a=3$, and dissimilarity threshold~$\tau = 0.5$, which permits an overlap of one feature between sets here.
	\emph{Greedy Depth Search} yields the the selection $s^{(0)} = (1,1,0,0)$, $s^{(1)} = (1,0,1,0)$, $s^{(2)} = (1,0,0,1)$, and $s^{(3)} = (0,1,1,0)$, with the corresponding feature-set qualities~17, 16, 10, and 15.
	\label{ex:afs:greedy-depth:non-monotonic}
\end{example}

\paragraph{Limitations}

Like \emph{Greedy Balancing Search} and \emph{Greedy Replacement Search}, \emph{Greedy Depth Search} assumes univariate feature qualities and may be worse than exact search.
As a sequential procedure, it does not balance the alternatives' qualities.
It may yield more alternatives than the former two heuristics but has a higher and more variable runtime.

\subsection{Evaluation}
\label{sec:afs:appendix:evaluation}

In this section, we evaluate experimental results not covered in Section~\ref{sec:afs:evaluation}.
In particular, we cover three experimental dimensions not stemming from the search for alternatives:
datasets (cf.~Section~\ref{sec:afs:appendix:evaluation:datasets}), feature-set-quality metrics (cf.~Section~\ref{sec:afs:appendix:evaluation:metrics}), and feature-selection methods (cf.~Section~\ref{sec:afs:appendix:evaluation:feature-selection}).

\subsubsection{Datasets}
\label{sec:afs:appendix:evaluation:datasets}

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 30 15 15, clip]{plots/afs-impact-dataset-k-train-objective.pdf}
		\caption{Training-set objective value.}
		\label{fig:afs:impact-dataset-k-train-objective}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 30 15 15, clip]{plots/afs-impact-dataset-k-decision-tree-test-mcc.pdf}
		\caption{Test-set prediction performance.}
		\label{fig:afs:impact-dataset-k-decision-tree-test-mcc}
	\end{subfigure}
	\caption{
		Feature-set quality in datasets over feature-set size~$k$ relative to dimensionality~$n$, by feature-set size~$k$ and evaluation metric.
		Results from the original feature sets of solver-based sequential search with \emph{MI} as the feature-selection method.
	}
	\label{fig:afs:impact-dataset-k-quality}
\end{figure}

Naturally, feature-set quality depends on the dataset, and several effects could occur.
For example, the distribution of feature-set quality in a dataset may be relatively uniform or relatively skewed.
Further, datasets with more features~$n$ give way to more alternative feature sets.
At the same time, the feature quality can be spread over more features than for lower-dimensional datasets, making it harder to compose a small high-quality feature set.
Indeed, our experiments show a broad variation of feature-set quality over the datasets.
Figure~\ref{fig:afs:impact-dataset-k-quality} depicts the relationship between datasets and the quality of the original, i.e., unconstrained, feature set in solver-based sequential search.
To account for the varying dataset dimensionality, we put the ratio between feature-set size~$k$ and dimensionality~$n$ on the x-axis, which measures relative feature-set sizes.
As Figure~\ref{fig:afs:impact-dataset-k-train-objective} displays, the objective of the univariate feature-selection method \emph{MI} approximately increases linearly with~$k/n$.
However, there still is variation exclusively caused by the dataset rather than its dimensionality.
Further, the quality of a prediction model, i.e., decision trees, does not exhibit any trend but varies strongly between datasets, as Figure~\ref{fig:afs:impact-dataset-k-decision-tree-test-mcc} visualizes.
This variation justifies our normalization of feature-set quality when analyzing alternatives in Sections~\ref{sec:afs:evaluation:num-alternatives} and~\ref{sec:afs:evaluation:tau}.

\subsubsection{Feature-Set Quality Metrics}
\label{sec:afs:appendix:evaluation:metrics}

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 15 10 15, clip]{plots/afs-evaluation-metrics-overfitting.pdf}
		\caption{
			Training-test difference in feature-set quality by feature-selection method.
			Y-axis is truncated to improve readability.
		}
		\label{fig:afs:evaluation-metrics-overfitting}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 15 10 15, clip]{plots/afs-evaluation-metrics-correlation.pdf}
		\caption{Correlation between evaluation metrics, averaged over datasets, cross-valida\-tion folds, and feature-selection methods.}
		\label{fig:afs:evaluation-metrics-correlation}
	\end{subfigure}
	\caption{
		Feature-set quality by evaluation metric.
		Results from all solver-based search runs.
	}
	\label{fig:afs:evaluation-metrics}
\end{figure}

\paragraph{Prediction models and overfitting}

As one can expect, random forests have a higher average prediction performance than decision trees.
Further, both model types exhibit overfitting, i.e., there is a gap between training-set and test-set performance.
In particular, over all feature sets from solver-based search runs, both model types have a mean training-set MCC around 0.85-0.86 (median: 1.0).
In contrast, decision trees have a mean MCC of 0.47 (median: 0.53) on the test set, while random forests have a slightly higher mean MCC of 0.52 (median: 0.61).
I.e., prediction performance is significantly worse on the test set than on the training set.
The existence of overfitting makes sense as we do not regularize, i.e., limit the growth of the trees or prune them after training.

As another comparison, Figure~\ref{fig:afs:evaluation-metrics-overfitting} shows the distribution of the difference between training and test feature-set quality, again over all feature sets from solver-based search runs.
Once more, we observe that training feature-set quality is usually higher, i.e., the difference shown in the figure is greater than zero.
However, this phenomenon does not invalidate our analysis of how feature-set quality develops over alternatives.
The optimization objective~$Q$, which Figure~\ref{fig:afs:evaluation-metrics-overfitting} also depicts, shows overfitting for all feature-selection methods as well, though to a lesser extent than prediction performance.
Thus, Section~\ref{sec:afs:evaluation} considers the training and test set for the objective value, but only the test set for prediction performance.

\paragraph{Correlation between evaluation metrics}

Figure~\ref{fig:afs:evaluation-metrics-correlation} shows the Spearman correlation between different evaluation metrics over all feature sets from solver-based search runs:
First, we compute the correlation between metrics for each combination of dataset, cross-validation fold, and feature-selection method.
Second, we average the correlation values over these three experimental dimensions.
This two-step procedure accounts for the different objectives of feature-selection methods and the normalization of quality per dataset and cross-validation fold in some objectives (cf.~Section~\ref{sec:afs:experimental-design:approaches:feature-selection}).
The plot shows that the performance of decision trees and random forests is highly correlated.
Thus, we only report MCC of decision trees in Section~\ref{sec:afs:evaluation}, which are the simpler model type and always consider all features during training rather than randomly sampling them.

Figure~\ref{fig:afs:evaluation-metrics-correlation} also shows that the correlation between training and test feature-set quality over all solver-based search runs is only moderate for the optimization objective~$Q$ and weak for prediction performance in terms of MCC.
This result might be caused by overfitting, whose strength may depend on the experimental settings.
Further, the correlation between optimization objective~$Q$ and prediction MCC is only weak to moderate as well.
I.e., the objective of feature selection is only partially indicative of prediction performance since the former might use a simplified quality criterion.
Among the five feature-selection methods, \emph{Greedy Wrapper} has the highest correlation between training-set objective value and test-set MCC, with a value of 0.48.
Since this feature-selection method uses prediction performance in its objective, a comparatively high correlation is expected.
The correlation still is far from perfect since the search procedure of \emph{Greedy Wrapper} evaluates feature sets with a validation split of the training set.
MCC on this holdout set may not perfectly correspond to MCC on the test set, which is not used in the search.
On the other end of the spectrum, \emph{mRMR} exhibits a correlation of -0.05 between training-set objective value and test-set MCC.
This filter method penalizes the correlation between features in its objective.
However, redundant features may not hurt prediction performance in decision trees, even if they do not improve it.

\subsubsection{Feature-Selection Methods}
\label{sec:afs:appendix:evaluation:feature-selection}

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 35 5 15, clip]{plots/afs-impact-fs-method-k-decision-tree-test-mcc.pdf}
		\caption{Test-set prediction performance.}
		\label{fig:afs:impact-fs-method-k-decision-tree-test-mcc}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=15 35 5 15, clip]{plots/afs-impact-fs-method-k-metric-diff.pdf}
		\caption{
			Difference in feature-set quality between $k=10$ and $k=5$ by evaluation metric.
			Y-axis is truncated to improve readability.
		}
		\label{fig:afs:impact-fs-method-k-metric-diff}
	\end{subfigure}
	\caption{
		Feature-set quality by feature-selection method and feature-set size~$k$.
		Results from the original feature sets of solver-based sequential search.
	}
	\label{fig:afs:impact-fs-method-k-quality}
\end{figure}

\paragraph{Prediction performance}

As the five feature-selection methods employ different objective functions~$Q$, comparing absolute objective values between them does not make sense.
However, we can analyze the prediction performance of the obtained feature sets.
Figure~\ref{fig:afs:impact-fs-method-k-decision-tree-test-mcc} compares a decision tree's test-set MCC on the original feature sets of solver-based sequential search between feature-selection methods.
On average, \emph{Model Gain} is the best feature-selection method:
The mean test-set MCC of decision trees is 0.53 for \emph{Model Gain}, 0.49 for \emph{Greedy Wrapper}, 0.47 for \emph{MI}, 0.46 for \emph{mRMR}, 0.43 for \emph{FCBF}.
In particular, the univariate, model-free method \emph{MI} keeps up surprisingly well with more sophisticated methods.
Thus, the analyses of alternative feature sets in Section~\ref{sec:afs:evaluation} focus on \emph{MI} while still discussing the remaining feature-selection methods.
The overall best feature-selection method, \emph{Model Gain}, uses the same objective function as \emph{MI} but obtains its feature qualities from a prediction model rather than a bivariate dependency measure, which might boost its performance. 

While \emph{Greedy Wrapper} uses actual prediction performance to assess feature-set quality, its heuristic nature might prevent better results:
This method only evaluates a fraction of all feature sets, while the other feature-selection methods optimize globally.
In particular, \emph{Greedy Wrapper} performed 629 iterations on average (median: 561) to determine the original feature sets of solver-based sequential search.
However, the number of possible feature sets is much higher, e.g., already $2^{15}=32768$ for the lowest-dimensional datasets in our evaluation (cf.~Table~\ref{tab:afs:datasets}).

\emph{FCBF}'s results may be taken with a grain of salt:
Over all solver-based search runs, 89\% of the feature sets for \emph{FCBF} were infeasible, i.e., no solution satisfied the constraints.
In contrast, this figure only is 18\% for \emph{MI}.
Even the original feature set in solver-based sequential search is infeasible in 71\% of the cases for \emph{FCBF} but never for the other feature-selection methods.
In particular, the combination of feature-correlation constraints in our formulation of \emph{FCBF} (cf.~Equation~\ref{eq:afs:fcbf}) with a feature-set-cardinality constraint, i.e., enforcing a feature-set size~$k$, may make the problem infeasible, especially if~$k$ gets larger.

\paragraph{Influence of feature-set size~$k$}

As expected, larger feature sets usually exhibit a higher feature-set quality than smaller feature sets in our experiments.
However, the increase in quality with $k$ is not proportional, and there might even be a decrease.
As Figure~\ref{fig:afs:impact-fs-method-k-metric-diff} shows for the original feature sets of solver-based sequential search, \emph{MI} and \emph{Model Gain} exhibit an increase of the training-set objective value~$Q_\text{train}$ from~$k=5$ to~$k=10$, i.e., the difference depicted in Figure~\ref{fig:afs:impact-fs-method-k-metric-diff} is positive.
As these objectives are monotonic in the set of selected features, a decrease in the training-set objective value is impossible.
In contrast, the heuristic \emph{Greedy Wrapper} does not necessarily benefit from more features.
The latter insight also applies to \emph{mRMR}, which normalizes its objective with the number of selected features and penalizes feature redundancy.
For \emph{FBCF}, the fraction of feasible feature sets changes considerably from $k=5$ to $k=10$, so one cannot directly compare the overall quality between these two settings.
As Figure~\ref{fig:afs:impact-fs-method-k-metric-diff} also displays, the benefit of larger feature sets is even less clear for prediction performance.
In particular, all feature-selection methods except \emph{FCBF} show a median difference in test-set MCC close to zero when comparing $k=5$ to $k=10$.
Thus, Section~\ref{sec:afs:evaluation} focuses on smaller feature sets, i.e., $k=5$.

\renewcommand*{\bibfont}{\small} % use a smaller font for bib than for main text
\printbibliography

\end{document}
