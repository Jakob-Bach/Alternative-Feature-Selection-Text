\documentclass{article}

\title{Finding Optimal Solutions for Alternative Feature Selection}
\author{Jakob Bach}

\usepackage[style=ieee, backend=bibtex]{biblatex}
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % theorems, definitions etc.
\usepackage{enumitem} % nicely formatted enumerations
\usepackage{graphicx} % plots
\usepackage{subcaption} % figures with multiple sub-figures and sub-captions
\usepackage{hyperref} % links and URLs
\addbibresource{references.bib}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
Feature-selection techniques are popular to obtain small, interpretable, yet highly accurate prediction models.
Existing feature-selection techniques typically yield either a fixed set of features or a ranking of features.
However, obtaining just one feature set might not be sufficient in some cases.
For example, users might be interested in finding different feature sets with similar prediction performance, offering alternative explanations of the data.
In this article, we formalize alternative feature selection with optimization problems.
Next, we propose several approaches to solve these problems.
In particular, we integrate various kinds of feature-selection techniques.
Finally, we evaluate these approaches in a study with 100 classification datasets.
In our experiments, we find that most datasets give way to finding alternative feature sets with similar prediction performance.
\end{abstract}

\textbf{Keywords:} feature selection, alternatives, constraints, explainability

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

\paragraph{Problem statement}

\paragraph{Related work}

In machine learning, finding alternative solution has already been addressed extensively in clustering~\cite{bae2006coala}.
However, there is a lack of such approaches and corresponding experimental evaluation for feature selection.
The concept of statistically equivalent feature subsets~\cite{lagani2017feature} groups features such that one obtains alternatives for each individual feature.
However, there might not be such an alternative for some features.
Also, our approaches consider alternatives to feature sets as a whole.
In the field of explainable AI, counterfactual explanations have massively gained popularity in the last few years~\cite{verma2020counterfactual, stepin2021survey}.
Such explanations involve data objects with similar feature values, but a different prediction.
In contrast, we target at different feature sets with similar prediction performance.

\paragraph{Contributions}

Our contribution is threefold.
First, we define optimization problems that formalize alternative feature selection.
In particular, we consider finding multiple alternatives sequentially and simultaneously.
Second, we propose approaches to solve these optimization problems.
To that end, we discuss how to integrate different kinds of feature-selection techniques.
Depending on the feature-selection technique, we present an exact or a heuristic solution approach.
Third, we evaluate our approaches for alternative feature selection with comprehensive experiments.
We use xxx classification dataset from PMLB\cite{olson2017pmlb, romano2021pmlb}.
For feature selection, we consider x, y, and z.
We focus on the question if one can find alternative feature sets with similar prediction quality as the original ones.

\paragraph{Results}

We find that most datasets allow finding alternative feature sets with similar prediction quality.
This presents the user with alternative explanations for prediction performance.
For the different feature-selection techniques, we note \dots
Comparing sequential and simultaneous search for alternatives, we find \dots
We publish our code\footnote{\url{https://github.com/Jakob-Bach/AFS}} and our experimental data\footnote{temporary link: \url{https://bwsyncandshare.kit.edu/s/xxx}; will be moved to a public repository after review}.

\paragraph{Outline}

Section~\ref{sec:related-work} reviews related work.
Section~\ref{sec:fundamentals} explains fundamentals of feature selection.
Section~\ref{sec:problems} defines the optimization problems of alternative feature selection.
Section~\ref{sec:approaches} discusses approaches to find alternative feature sets.
Section~\ref{sec:experimental-design} describes our experimental design.
Section~\ref{sec:evaluation} presents the experimental results.
Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related-work}

COALA \cite{bae2006coala}
Alternate Clustering \cite{bailey2014alternative}
alternative features for clustering \cite{tao2012novel}
features relevant global and in clusters \cite{guan2011unified}
integer problem to find alternative clusters \cite{bae2010clustering}
penalize overlap \cite{mueller2009relevant}

GMD \cite{trittenbach2019dimension}
Edouard \cite{fouche2021efficient}
combining subspaces \cite{nguyen20134s}

subgroup set discovery \cite{leeuwen2012diverse}

statistical equivalent subsets \cite{lagani2017feature, borboudakis2021extending, tsamardinos2003towards, dougherty2006number}
feature clustering \cite{mueller2021feature}
combining feature selection results \cite{woznica2012model}
genetic algo multi feature sets \cite{siddiqi2020genetic}

counterfactual surveys \cite{verma2020counterfactual, stepin2021survey}
MIP for counterfactuals \cite{mohammadi2021scaling}
SMT for counterfactuals \cite{karimi2020model}

constraint data mining \cite{grossi2017survey}

\section{Fundamentals}
\label{sec:fundamentals}

In this section, we introduce basic notation and review different methods to measure quality of feature sets.

\subsection{Notation}
\label{sec:fundamentals:notation}

Let $X \in \mathbb{R}^{m \times n}$ be a dataset represented as a matrix.
Each row is a data object, and each column is a feature.
Let $F = \{f_1, \dots, f_n\}$ denote the set of feature names.
We assume categorical features have already been made numeric, e.g., via one-hot encoding.
Further, let $y \in \mathbb{R}^m$ represent the prediction target.
The actual domain of the target might also be smaller, e.g., $\{0,1\}$ for binary classification.

With feature selection, one makes a binary decision $s_j \in \{0,1\}$ for each feature, i.e., either selects it or not.
The vector $s \in \{0,1\}^n$ combines all these selection decisions.
The selected feature set is $F_s = \{f_j \mid s_j=1\}$.
Let function $Q(s,X,y)$ returns the quality of a such feature set.
Without loss of generality, we assume this function should be maximized.

\subsection{Measuring Feature (Set) Quality}
\label{sec:fundamentals:quality}

There are different ways to evaluate feature-set quality $Q(s,X,y)$.
Note that we only give a short overview here, and focus on techniques that we use in our evaluation.
See \cite{chandrashekar2014survey,li2017feature} for comprehensive surveys of feature selection.
A typical categorization of feature selection is into filter, wrapper, and embedded methods~\cite{guyon2003introduction}.

\paragraph{Filter methods}

Filter methods evaluate feature sets without training a prediction model.
Univariate filters assess each feature on its own, often assigning a numeric score ot each feature, while multivariate filters evaluate feature sets.
Examples for univariate filters are the absolute Pearson correlation or the mutual information between a feature and the prediction target.
Such approaches ignore potential interaction between features, e.g., if they are redundant to each other.
Multivariate methods often combine a measure of feature relevance with a measure of feature redundancy.
Examples for such an approach include CFS \cite{hall1999correlation}, FCBF \cite{yu2003feature}, and mRMR \cite{peng2005feature}.
Another interesting filter method is Relief~\cite{kira1992feature}, for which multiple extensions exist.
While Relief assigns quality to individual features rather than feature sets, it still uses other features indirectly via nearest-neighbor computations between data objects.

\paragraph{Wrapper methods}

Wrapper methods~\cite{kohavi1997wrappers} employ a search strategy over feature sets and evaluate each of these feature set by training a prediction model.
Various black-box optimization techniques can serve as search strategy, e.g., genetic algorithms.

\paragraph{Embedded methods}

Embedded methods train prediction models with built-in feature selection, e.g., decision trees or random forests~\cite{breiman2001random}.

\paragraph{Post-hoc feature importance}

Apart from traditional feature selection, there are various approaches that assess feature importance after training a model.
These methods range range from local explanation methods like LIME~\cite{ribeiro2016should} or SHAP~\cite{lundberg2017unified} to global importance methods like permutation importance~\cite{breiman2001random} or SAGE~\cite{covert2020understanding}.
In particular, assessing feature importance plays a crucial role in the emerging fields of ML interpretability~\cite{carvalho2019machine}.
Mathematically, having a post-hoc importance score for each feature is not different from the output of univariate filter methods.
Thus, our approach for finding alternative feature sets works for outputs of such feature-importance methods as well.

\section{The Optimization Problems of Alternative Feature Selection}
\label{sec:problems}

In this section, we formalize alternative feature selection as optimization problems.
As a first step, we define when an individual feature set is an alternative to another one.
Second, we extend this notion to multiple alternatives, considering sequential as well as simultaneous search procedures for alternatives.

\subsection{Individual Alternatives}

We consider a feature set to be a valid alternative to another feature set if it differs sufficiently.
Mathematically, we express this with a set-distance measure.
A simple and well-known set-distance measure is the Jaccard distance.
We use this distance in our article, but one could choose another measure~\cite{egghe2009new} as well.
Given two feature sets $F_1$, $F_2$, the Jaccard distance is
%
\begin{equation}
	d_{Jacc}(F_1,F_2) = 1 - \frac{|F_1 \cap F_2|}{|F_1 \cup F_2|} = 1 - \frac{|F_1 \cap F_2|}{|F_1| + |F_2| - |F_1 \cap F_2|}
\end{equation}
%
\begin{definition}
	A feature set $F_2$ a valid alternative to feature set $F_1$ (and vice versa) for a distance threshold $\tau \in [0,1]$ if $d_{Jacc}(F_1,F_2) \geq \tau$.
\end{definition}
%
Depending on the preferences of the user, different values of $\tau$ might be appropriate.
This depends on how alternative the new feature set should be.
Also, a feature set that differs strongly might also yield large changes in prediction performance.
Thus, we leave $\tau$ as a parameter of our approach.
If the choice of $\tau$ is unclear a priori, users can try out different values and compare results.
Setting $\tau$ is made easier by the fact that the Jaccard distance is normalized to $[0,1]$.
A value of $0$ allows the alternative feature set to be identical to the original feature set.
A value of $1$ requires the two feature sets to have no overlap.
If the feature sets sizes $|F_1|$ and $|F_2|$ are known, one can also specify the maximum number of overlapping features $|F_1 \cap F_2|$ instead of $\tau$, because one can compute $\tau$ from $|F_1 \cap F_2|$.

Computing feature-set sizes based on the binary feature-selection vector $s$ is straightforward:
%
\begin{align}
	|F_s| =& \sum_{j=1}^n s_j\\
	|F_{s_1} \cap F_{s_2}| =& \sum_{j=1}^n s_{1,j} \cdot s_{2,j} = \sum_{j=1}^n s_{1,j} \land s_{2,j}
\end{align}
%
\subsection{Multiple Alternatives}

If the user desires multiple alternative feature sets rather than just one, we can compute these alternatives either sequentially or simultaneously.

\subsubsection{Sequential Alternatives}
%
\begin{definition}
	A feature set $F_2$ a valid alternative to a set of feature sets $\mathbb{F}$ (and vice versa) for a distance threshold $\tau \in [0,1]$ if $\forall F_1 \in \mathbb{F}: d_{Jacc}(F_1,F_2) \geq \tau$.
\end{definition}
%
\subsubsection{Simultaneous Alternatives}
%
\begin{definition}
	A set of feature sets $\mathbb{F}$ contains pairwise valid alternatives for a distance threshold $\tau \in [0,1]$ if $\forall F_1 \in \mathbb{F}, F_2 \in \mathbb{F}, F_1 \neq F_2: d_{Jacc}(F_1,F_2) \geq \tau$.
\end{definition}
%
\section{Approaches for Alternative Feature Selection}
\label{sec:approaches}

In this section, we introduce our approaches to find alternative feature sets.
In particular, we discuss how to address the optimization problems from Section~\ref{sec:problems} for different kinds of feature-set quality measures from Section~\ref{sec:fundamentals:quality}.

write something general about constraints

can be combined with domain constraints \cite{groves2015toward}, group constraints \cite{yuan2006model}, cost constraints \cite{paclik2002feature}
SMT \cite{barrett2018satisfiability}

focus on alternatives of sets as a whole - not 1-of-x for individual features (which requires user to define feature groups first, is similar to statistically equivalent signatures)

\paragraph{White-box optimization}

univariate filter, importance scores, multivariate filters that can be encoded
however, note that importance scores loose information (e.g., SHAP scores rate features in context of other features, which white-box approach might simply drop, so theoretically a re-computation is necessary)

\paragraph{Black-box optimization}

wrapper
- use approaches from software engineering (e.g., fix invalid solutions in search heuristic)
- baseline: let solver enumerate all valid solutions (e.g., for fixed number of iterations)
- even with search heuristics, might use solver to start with valid solution, since many searches slightly modify existing solutions, and goal here is to find a solution that significantly differs from existing one

\paragraph{Embedding alternatives}

embedded FS
no general solution -> need to push notion of alternativeness into model training, i.e., would get a rather customized solution

\section{Experimental Design}
\label{sec:experimental-design}

In this section, we describe our experimental design.
First, we pose the research questions.
Next, we elaborate on different components of the design.

\subsection{Research Questions}

\subsection{Approaches}

\paragraph{Feature selection}

\paragraph{Finding alternatives}

\paragraph{Prediction models}

As prediction models, we use decision trees~\cite{breiman1984classification} as well as random forests with 100 trees \cite{breiman2001random}.
Both types of models ae popular and allow to learn complex, non-linear dependencies from the data.
We leave the hyperparameters of the models at their defaults.

\subsection{Evaluation Metrics}

We conduct 10-fold cross-validation.
Not only model training, but also the search for alternative feature sets is limited to the training data.
In contrast, when we report prediction performance, we describe results on the test data only.

number of interesting alternatives: valid alternative and quality within a threshold of the original solution

\subsection{Datasets}

PMLB \cite{olson2017pmlb, romano2021pmlb}

\subsection{Implementation}

We implement our experimental pipeline in Python.
For machine learning, we use \emph{scikit-learn}~\cite{pedregosa2011scikit-learn}.
To express and solve the constraint optimization problems, we use \emph{Z3}~\cite{deMoura2008z3}.

\section{Evaluation}
\label{sec:evaluation}

\section{Conclusions and Future Work}
\label{sec:conclusion}

\subsection{Conclusions}

\subsection{Future Work}

\printbibliography

\appendix

\section{Appendix}
\label{sec:appendix}

\end{document}
