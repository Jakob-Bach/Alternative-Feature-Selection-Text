\documentclass{article}

\title{Alternative Feature Selection}
\author{Jakob Bach}

\usepackage[style=ieee, backend=bibtex]{biblatex}
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % theorems, definitions etc.
\usepackage{enumitem} % nicely formatted enumerations
\usepackage{graphicx} % plots
\usepackage{subcaption} % figures with multiple sub-figures and sub-captions
\usepackage{hyperref} % links and URLs
\addbibresource{references.bib}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

\paragraph{Problem statement}

\paragraph{Related work}

\paragraph{Contributions}

\paragraph{Results}

We publish our code\footnote{\url{https://github.com/Jakob-Bach/AFS}} and our experimental data\footnote{temporary link: \url{https://bwsyncandshare.kit.edu/s/xxx}; will be moved to a public repository after review}.

\paragraph{Outline}

Section~\ref{sec:related-work} reviews related work.
Section~\ref{sec:fundamentals} explains fundamentals of feature selection.
Section~\ref{sec:approach} introduces our approach for alternative feature sets.
Section~\ref{sec:experimental-design} describes our experimental design.
Section~\ref{sec:evaluation} presents the experimental results.
Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related-work}

COALA \cite{bae2006coala}
Alternate Clustering \cite{bailey2014alternative}
alternative features for clustering \cite{tao2012novel}
features relevant global and in clusters \cite{guan2011unified}
integer problem to find alternative clusters \cite{bae2010clustering}
penalize overlap \cite{mueller2009relevant}

GMD \cite{trittenbach2019dimension}
Edouard \cite{fouche2021efficient}
combining subspaces \cite{nguyen20134s}

subgroup set discovery \cite{leeuwen2012diverse}

statistical equivalent subsets \cite{lagani2017feature, borboudakis2021extending, tsamardinos2003towards, dougherty2006number}
feature clustering \cite{mueller2021feature}
combining feature selection results \cite{woznica2012model}
genetic algo multi feature sets \cite{siddiqi2020genetic}

counterfactual surveys \cite{verma2020counterfactual, stepin2021survey}
MIP for counterfactuals \cite{mohammadi2021scaling}
SMT for counterfactuals \cite{karimi2020model}

constraint data mining \cite{grossi2017survey}

\section{Fundamentals}
\label{sec:fundamentals}

\subsection{Notation}

Let $X \in \mathbb{R}^{m \times n}$ be a dataset represented as a matrix.
Each row is a data object, and each column is a feature.
Let $F = \{f_1, \dots, f_n\}$ denote the set of feature names.
We assume categorical features have already been made numeric, e.g., via one-hot encoding.
Further, let $y \in \mathbb{R}^m$ represent the prediction target.
The actual domain of the target might also be smaller, e.g., $\{0,1\}$ for binary classification.

With feature selection, one makes a binary decision $s_j \in \{0,1\}$ for each feature, i.e., either selects it or not.
The vector $s \in \{0,1\}^n$ combines all these selection decisions.
The selected feature set is $F_s = \{f_j|~s_j=1\}$.
Let function $Q(s,X,y)$ returns the quality of a such feature set.
Without loss of generality, we assume this function should be maximized.

\subsection{Measuring Feature (Set) Quality}

There are different ways to evaluate feature-set quality $Q(s,X,y)$.
Note that we only give a short overview here, and focus on techniques that we use in our evaluation.
See \cite{chandrashekar2014survey,li2017feature} for comprehensive surveys of feature selection.
A typical categorization of feature selection is into filter, wrapper, and embedded methods~\cite{guyon2003introduction}.

\paragraph{Filter methods}

Filter methods evaluate feature sets without training a prediction model.
Univariate filters assess each feature on its own, often assigning a numeric score ot each feature, while multivariate filters evaluate feature sets.
Examples for univariate filters are the absolute Pearson correlation or the mutual information between a feature and the prediction target.
Such approaches ignore potential interaction between features, e.g., if they are redundant to each other.
Multivariate methods often combine a measure of feature relevance with a measure of feature redundancy.
Examples for such an approach include CFS \cite{hall1999correlation}, FCBF \cite{yu2003feature}, and mRMR \cite{peng2005feature}.
Another interesting filter method is Relief~\cite{kira1992feature}, for which multiple extensions exist.
While Relief assigns quality to individual features rather than feature sets, it still uses other features indirectly via nearest-neighbor computations between data objects.

\paragraph{Wrapper methods}

Wrapper methods~\cite{kohavi1997wrappers} employ a search strategy over feature sets and evaluate each of these feature set by training a prediction model.
Various black-box optimization techniques can serve as search strategy, e.g., genetic algorithms.

\paragraph{Embedded methods}

Embedded methods train prediction models with built-in feature selection, e.g., decision trees or random forests~\cite{breiman2001random}.

\paragraph{Post-hoc feature importance}

Apart from traditional feature selection, there are various approaches that assess feature importance after training a model.
These methods range from classical global methods like permutation importance~\cite{breiman2001random} to local explanation methods like LIME \cite{ribeiro2016should} or SHAP \cite{lundberg2017unified}.
In particular, assessing feature importance plays a crucial role in the emerging fields of ML interpretability~\cite{carvalho2019machine}.
Mathematically, having a post-hoc importance score for each feature is not different from the output of univariate filter methods.
Thus, our approach for finding alternative feature sets works for outputs of such feature-importance methods as well.

\section{Alternative Feature Selection (AFS) -- Approach}
\label{sec:approach}

\subsection{Quantifying Alternatives}

We consider a feature set to be a valid alternative to another feature set if it differs sufficiently.
Mathematically, we express this with a set-distance measure.
A simple and well-known set-distance measure is the Jaccard distance.
We use this distance in our article, but one could choose another measure~\cite{egghe2009new} as well.
Given two feature sets $F_1$, $F_2$, the Jaccard distance is
%
\begin{equation}
	d_{Jacc}(F_1,F_2) = 1 - \frac{|F_1 \cap F_2|}{|F_1 \cup F_2|} = 1 - \frac{|F_1 \cap F_2|}{|F_1| + |F_2| - |F_1 \cap F_2|}
\end{equation}
%
\begin{definition}
	A feature set $F_2$ a valid alternative to $F_1$ (and vice versa) if $d_{Jacc}(F_1,F_2) \geq \tau$ for a distance threshold $\tau \in [0,1]$.
\end{definition}
%
Depending on the preferences of the user, different values of $\tau$ might be appropriate.
This depends on how alternative the new feature set should be.
Also, a feature set that differs strongly might also yield large changes in prediction performance.
Thus, we leave $\tau$ as a parameter of our approach.
If the choice of $\tau$ is unclear a priori, users can try out different values and compare results.
Setting $\tau$ is made easier by the fact that the Jaccard distance is normalized to $[0,1]$.
A value of $0$ allows the alternative feature set to be identical to the original feature set.
A value of $1$ requires the two feature sets to have no overlap.
If the feature sets sizes $|F_1|$ and $|F_2|$ are known, one can also specify the maximum number of overlapping features $|F_1 \cap F_2|$ instead of $\tau$, because one can compute $\tau$ from $|F_1 \cap F_2|$.

Computing feature-set sizes based on the binary feature-selection vector $s$ is straightforward:
%
\begin{align}
	|F_s| =& \sum_{j=1}^n s_j\\
	|F_{s_1} \cap F_{s_2}| =& \sum_{j=1}^n s_{1,j} \cdot s_{2,j} = \sum_{j=1}^n s_{1,j} \land s_{2,j}
\end{align}
%
\subsection{Finding Alternatives}

can be combined with domain constraints \cite{groves2015toward}, group constraints \cite{yuan2006model}, cost constraints \cite{paclik2002feature}

focus on alternatives of sets as a whole - not 1-of-x for individual features (which requires user to define feature groups first, is similar to statistically equivalent signatures)

\section{Experimental Design}
\label{sec:experimental-design}

\subsection{Research Questions}

\subsection{Approaches}

feature selection
finding alternatives
prediction

\subsection{Evaluation Metrics}

\subsection{Datasets}

PMLB \cite{olson2017pmlb, romano2021pmlb}

\subsection{Implementation}

We implement our experimental pipeline in Python.
For machine learning, we use \emph{scikit-learn}~\cite{pedregosa2011scikit-learn}.
To express and solve the constraint optimization problems, we use \emph{Z3}~\cite{deMoura2008z3}.

\section{Evaluation}
\label{sec:evaluation}

\section{Conclusions and Future Work}
\label{sec:conclusion}

\subsection{Conclusions}

\subsection{Future Work}

\printbibliography

\appendix

\section{Appendix}
\label{sec:appendix}

\end{document}
