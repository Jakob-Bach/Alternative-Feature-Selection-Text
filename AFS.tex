\documentclass{article}

\title{
	Finding Optimal Diverse Feature Sets\\
	via Alternative Feature Selection
}
\author{
	Jakob Bach~\orcidlink{0000-0003-0301-2798}\\
	\small Karlsruhe Institute of Technology (KIT), Germany\\
	\small \href{mailto:jakob.bach@kit.edu}{jakob.bach@kit.edu}
}
\date{} % don't display a date

\usepackage[style=numeric, backend=bibtex]{biblatex}
\usepackage[ruled,linesnumbered,vlined]{algorithm2e} % pseudo-code
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % theorems, definitions etc.
\usepackage{graphicx} % plots
\usepackage{orcidlink} % ORCID icon
\usepackage{subcaption} % figures with multiple sub-figures and sub-captions
\usepackage{hyperref} % links and URLs

\addbibresource{references.bib}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
Feature-selection methods are popular to obtain small, interpretable, yet highly accurate prediction models.
Existing feature-selection methods typically yield only one feature set, which might not be sufficient in some cases.
For example, users might be interested in finding different feature sets with similar prediction quality, offering alternative explanations of the data.
In this article, we formalize alternative feature selection as an optimization problem.
Next, we propose several general approaches to solve this problem.
In particular, we show how to integrate various categories of existing feature-selection methods.
Finally, we evaluate these approaches in a study with 30 classification datasets.
In our experiments, we can determine alternative feature sets with similar prediction quality as the original ones.
\end{abstract}
%
\textbf{Keywords:} feature selection, alternatives, constraints, mixed-integer programming, explainability, interpretability

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

Feature-selection methods are ubiquitous for a variety of reasons.
By reducing dataset dimensionality, they lower the computational and memory requirements of prediction models.
Next, models might generalize better after removing irrelevant and spurious predictors.
Finally, prediction models might become simpler and more comprehensive~\cite{li2017feature}, improving interpretability.

Traditional feature-selection methods mostly return only one feature set~\cite{borboudakis2021extending}.
These methods optimize a criterion of feature-set quality, e.g., prediction performance.
However, besides the optimal feature set, there might be other, differently composed feature sets with similar quality.
For the user, these alternative feature sets might be interesting for multiple reasons.
First, some originally selected features might be costly to obtain, so the user would prefer cheaper alternatives.
Second, some originally selected features might contain sensitive information that should not be used in predictions.
Third, the user might be interested in explanations for the prediction, rather than just making good predictions.
In such a situation, knowing alternatives allows the formulation of multiple hypotheses and broadens the understanding.

\paragraph{Problem statement}

This article addresses the problem of alternative feature selection, which we informally define as follows:
%
\begin{definition}[Alternative feature selection (informal)]
	Given an original feature set, find a sufficiently different feature set that optimizes feature-set quality.
	\label{def:alternative-feature-selection}
\end{definition}
%
We will provide formal definitions later.
Ideally, the alternative feature set should have a similar quality as the original one.
However, depending on how different the alternative feature set should be, one might have to compromise on quality.
E.g., if there are only a few highly predictive features and most of them are part of the original feature set, the alternative feature set might have significantly lower prediction quality.
We analyze this effect in our article.
Also, we consider finding multiple alternatives rather than just one.

Two points are essential for alternative feature selection, which we both address in this article.
First, one needs to formalize and quantify what an alternative feature set is.
Second, one needs an approach to find alternative feature sets efficiently.
Ideally, the approach should be general, i.e., cover a broad range of existing feature-selection methods.

\paragraph{Related work}

Finding alternative solutions has already been addressed extensively in the field of clustering~\cite{bailey2014alternative}.
However, there is a lack of such approaches for feature selection.
Only a few feature-selection methods target at obtaining multiple, diverse feature sets~\cite{borboudakis2021extending, siddiqi2020genetic}.
Techniques for ensemble feature selection~\cite{saeys2008robust, seijo2017ensemble} and statistically equivalent feature subsets~\cite{lagani2017feature} produce multiple feature sets but do not focus on optimal alternatives.
In the field of explainable AI, counterfactual explanations have massively gained popularity in the last few years~\cite{stepin2021survey, verma2020counterfactual}.
Such explanations involve data objects with similar feature values but a different prediction outcome.
In contrast, we target at sets with different features but similar quality, e.g., prediction performance.

\paragraph{Contributions}

Our contribution is threefold.
First, we formalize alternative feature selection as an optimization problem.
In particular, we foster the search for alternatives via constraints on feature sets.
This formulation also allows integrating other constraints on feature sets~\cite{bach2022empirical, groves2015toward}.
Second, we propose heuristic and exact approaches to solve this optimization problem.
To that end, we discuss how to integrate different categories of feature-selection methods in the objective function.
Our notion of alternatives is general and thus not tailored to specific feature-selection methods.
Third, we evaluate our approaches for alternative feature selection with comprehensive experiments.
We use 30 classification dataset from the Penn Machine Learning Benchmarks (PMLB)~\cite{olson2017pmlb, romano2021pmlb} and four feature-selection methods.
We focus on the question if one can find alternative feature sets with similar feature-set quality as the original ones.

\paragraph{Results}

Our experiments show that finding alternative feature sets with similar quality is possible indeed.
This outcome encourages using alternative feature sets as a tool for alternative explanations of predictions.
As expected, feature-set quality tends to decrease with the number of alternatives.
The exact decrease depends on the dataset and how the feature-set quality is distributed in it.
Further, we note that the quality of alternative feature sets significantly depends on the dissimilarity threshold for being alternative.
Thereby, this threshold allows the user to exercise control over alternatives and make use-case-specific choices.
We publish our code\footnote{\url{https://github.com/Jakob-Bach/Alternative-Feature-Selection}} and our experimental data\footnote{\url{https://www.dropbox.com/sh/3bmeoihgozmvfg3/AAD9AcRddqRVlu7tps6FxIKIa?dl=0}} online. % TODO

\paragraph{Outline}

Section~\ref{sec:fundamentals} explains fundamentals of feature selection.
Section~\ref{sec:approach} defines the problem of alternative feature selection and discusses solution approaches.
Section~\ref{sec:related-work} reviews related work.
Section~\ref{sec:experimental-design} describes our experimental design and Section~\ref{sec:evaluation} presents the experimental results.
Section~\ref{sec:conclusion} concludes.

\section{Fundamentals}
\label{sec:fundamentals}

In this section, we introduce basic notation and review different methods to measure the quality of feature sets.

\subsection{Notation}
\label{sec:fundamentals:notation}

Let $X \in \mathbb{R}^{m \times n}$ be a dataset represented as a matrix.
Each row is a data object, and each column is a feature.
Let $F = \{f_1, \dots, f_n\}$ denote the set of feature names.
We assume categorical features have already been made numeric, e.g., via one-hot encoding.
Let $X_{\cdot{}j} \in \mathbb{R}^m$ denote the vector representation of the $j$-th feature.
Further, let $y \in \mathbb{R}^m$ represent the prediction target.
The actual domain of the target might also be smaller, e.g., $\{0,1\}$ for binary classification.

With feature selection, one makes a binary decision $s_j \in \{0,1\}$ for each feature, i.e., either selects it or not.
The vector $s \in \{0,1\}^n$ combines all these selection decisions.
The selected feature set is $F_s = \{f_j \mid s_j=1\}$.
Let the function $Q(s,X,y)$ return the quality of such a feature set.
Without loss of generality, we assume this function should be maximized.

\subsection{Measuring Feature (Set) Quality}
\label{sec:fundamentals:quality}

There are different ways to evaluate feature-set quality $Q(s,X,y)$.
Note that we only give a short overview here; see~\cite{chandrashekar2014survey,li2017feature} for comprehensive surveys of feature selection.
A traditional categorization of feature-selection methods distinguishes between filter, wrapper, and embedded methods~\cite{guyon2003introduction}.

\paragraph{Filter methods}

Filter methods evaluate feature sets without training a prediction model.
Univariate filters assess each feature independently, often assigning a score to each feature, while multivariate filters evaluate feature sets.
Examples for univariate filters are the absolute Pearson correlation or the mutual information between a feature and the prediction target.
Such methods ignore potential interaction between features, e.g., if they are redundant to each other.
In contrast, multivariate methods often combine a measure of feature relevance with a measure of feature redundancy.
Examples for such methods include CFS~\cite{hall1999correlation, hall2000correlation}, FCBF~\cite{yu2003feature}, and mRMR~\cite{peng2005feature}.
Another popular filter method is Relief~\cite{kira1992feature, robnik1997adaptation}, which assigns quality to individual features rather than feature sets but still uses other features indirectly in nearest-neighbor computations.

\paragraph{Wrapper methods}

Wrapper methods~\cite{kohavi1997wrappers} employ a generic search strategy over feature sets, e.g., genetic algorithms.
Feature-set quality is a black-box function:
Wrappers evaluate candidate feature sets from the search by training prediction models with them and measuring prediction quality.

\paragraph{Embedded methods}

Embedded methods train prediction models with built-in feature selection, e.g., decision trees~\cite{breiman1984classification} or random forests~\cite{breiman2001random}.
Thus, the criterion for feature-set quality is model-specific.
For example, tree-based models often use information gain or the Gini index to select features during training.

\paragraph{Post-hoc feature-importance methods}

Apart from traditional feature selection, there are various methods that assess feature importance after training a model.
These methods range range from local explanation methods like LIME~\cite{ribeiro2016should} or SHAP~\cite{lundberg2017unified} to global importance methods like permutation importance~\cite{breiman2001random} or SAGE~\cite{covert2020understanding}.
In particular, assessing feature importance plays a crucial role in the emerging field of ML interpretability~\cite{carvalho2019machine}.

\section{Alternative Feature Selection}
\label{sec:approach}

In this section, we present the problem and approaches for alternative feature selection.
First, we define the structure of the optimization problem, i.e., objective and constraints.
Second, we formalize the notion of alternatives via constraints.
Third, we discuss different objective functions, corresponding to different feature-set quality measures from Section~\ref{sec:fundamentals:quality}.
In particular, we describe how to solve the resulting optimization problem.

\subsection{Optimization Problem}
\label{sec:approach:problem}

In alternative feature selection, there are two goals.
First, the quality of an alternative feature set should be high.
Second, an alternative feature set should differ from one or more existing feature set(s).
There are different ways to combine these two goals in an optimization problem:

First, one can consider both goals as objectives, obtaining an unconstrained multi-objective problem.
Second, one can treat feature-set quality as objective and enforce alternatives with constraints.
Third, one can consider being alternative as objective and constrain feature-set quality, e.g., with a lower bound.
Fourth, one can define constraints for both, feature-set quality and being alternative, searching for feasible solutions instead of optimizing.
Depending on the use case, any of these four formulations might be appropriate.

Following the informal Definition~\ref{def:alternative-feature-selection} from the introduction, we stick to the second formulation, i.e., optimizing feature-set quality subject to being alternative.
This formulation has the advantage of keeping the original objective function of feature selection.
Consequently, one does not need to specify a range or a threshold on feature-set quality.
Instead, the user can control how alternative the new feature set should be.
This yields the following optimization problem:
%
\begin{equation}
	\begin{split}
		\max_s &\quad Q(s,X,y) \\
		\text{subject to:} &\quad F_s~\text{being alternative}
	\end{split}
	\label{eq:afs}
\end{equation}
%
We discuss different constraints for \emph{being alternative} and different objective functions $Q(s,X,y)$ in the following.

\subsection{Constraints -- Defining Alternatives}
\label{sec:approach:constraints}

In this section, we formalize alternative feature sets.
This part of our work is independent of the feature-selection method.
First, we discuss the base case where an individual feature set is an alternative to another one.
Second, we extend this notion to multiple alternatives, considering sequential as well as simultaneous search procedures for alternatives.

\subsubsection{Single Alternatives}
\label{sec:approach:constraints:single}

We consider a feature set to be an alternative to another feature set if it differs sufficiently.
Mathematically, we express this with a set-dissimilarity measure~\cite{egghe2009new}.
These measures typically assess how strongly two sets overlap and set this in relation to the size of the sets.
For example, a simple and well-known set-dissimilarity measure is the Jaccard distance.
Given two feature sets $F_1$, $F_2$, the Jaccard distance is defined as
%
\begin{equation}
	d_{\text{Jacc}}(F_1,F_2) = 1 - \frac{|F_1 \cap F_2|}{|F_1 \cup F_2|} = 1 - \frac{|F_1 \cap F_2|}{|F_1| + |F_2| - |F_1 \cap F_2|}
	\label{eq:jaccard}
\end{equation}
Another possible dissimilarity measure follows from the Dice coefficient:
%
\begin{equation}
	d_{\text{Dice}}(F_1,F_2) = 1 - \frac{2 \cdot |F_1 \cap F_2|}{|F_1| + |F_2|}
	\label{eq:dice}
\end{equation}
%
We leverage such a set-dissimilarity measure for the following definition:
%
\begin{definition}[Single alternative]
	A feature set $F_2$ is an alternative to a feature set $F_1$ (and vice versa) for a dissimilarity threshold $\tau \in \mathbb{R}_{\geq 0}$ if $d(F_1,F_2) \geq \tau$.
	\label{def:single-alternative}
\end{definition}
%
$\tau$ controls how alternative the new feature set should be and depends on the preferences of the user.
In particular, a feature set that differs strongly might cause a significant drop in feature-set quality.
Thus, we leave $\tau$ as a parameter.
If the choice of $\tau$ is unclear a priori, users can try out different values and compare results.
In case the set-dissimilarity measure $d(\cdot)$ is normalized to $[0,1]$, like the Dice dissimilarity or Jaccard distance, the interpretation of $\tau$ is user-friendly:
Setting $\tau=0$ allows the alternative feature set to be identical to the original one, while $\tau=1$ requires the two feature sets to have no overlap.

If the feature-set sizes $|F_1|$ and $|F_2|$ are known, a threshold $\tau$ corresponds to a particular maximum number of overlapping features $|F_1 \cap F_2|$.
This follows from re-arranging Equation~\ref{eq:jaccard} and Equation~\ref{eq:dice} in combination with Definition~\ref{def:single-alternative}:
%
\begin{equation}
	\begin{aligned}
		d_{\text{Jacc}}(F_1,F_2) = 1 - \frac{|F_1 \cap F_2|}{|F_1| + |F_2| - |F_1 \cap F_2|} &\geq \tau \\
		\Leftrightarrow |F_1 \cap F_2| &\leq \frac{1 - \tau}{2 - \tau} \cdot (|F_1| + |F_2|)
		\end{aligned}
	\label{eq:jaccard-rearranged}
\end{equation}
%
The latter formulation also avoids having the feature-set sizes, and thereby the decision variables for feature selection, in a quotient.
However, the effect of $\tau$ on the maximum overlap size $|F_1 \cap F_2|$ is non-linear for the Jaccard distance in Equation~\ref{eq:jaccard-rearranged}.
Thus, we use the Dice dissimilarity in this article, where the effect of $\tau$ on the maximum overlap size $|F_1 \cap F_2|$ is linear:
%
\begin{equation}
	\begin{aligned}
		d_{\text{Dice}}(F_1,F_2) = 1 - \frac{2 \cdot |F_1 \cap F_2|}{|F_1| + |F_2|} &\geq \tau \\
		\Leftrightarrow |F_1 \cap F_2| &\leq \frac{1 - \tau}{2} \cdot (|F_1| + |F_2|)
	\end{aligned}
	\label{eq:dice-rearranged}
\end{equation}
%
If $|F_1| = |F_2|$, this measure is also identical to several further overlap measures.
In that case, $\tau$ directly controls which fraction of features in one set needs to differ from the other set, and vice versa:
%
\begin{equation}
	\begin{aligned}
		\text{If}~|F_1| = |F_2|: \quad d_{\text{Dice}}(F_1,F_2) &\geq \tau \\
		\Leftrightarrow |F_1 \cap F_2| &\leq (1 - \tau) \cdot |F_1| = (1 - \tau) \cdot |F_2|
	\end{aligned}
\end{equation}
%
Up to now, we have used feature sets instead of the binary feature-selection vector $s$.
However, expressing the feature-set sizes in terms of $s$ is straightforward:
%
\begin{equation}
	\begin{aligned}
		|F_s| =& \sum_{j=1}^n s_j \\
		|F_{s_1} \cap F_{s_2}| =& \sum_{j=1}^n s_{1,j} \cdot s_{2,j}
	\end{aligned}
	\label{eq:feature-set-size}
\end{equation}
%
Combining Equation~\ref{eq:jaccard-rearranged} or Equation~\ref{eq:dice-rearranged} with Equation~\ref{eq:feature-set-size} yields an inequality only involving sums and products of the binary decision variables and constant values.
In fact, one can replace the product $s_{1,j} \cdot s_{2,j}$ with linear constraints by introducing an auxiliary variable $t_j$~\cite{mosek2022modeling}:
%
\begin{equation}
	\begin{aligned}
		t_j \leq& s_{1,j} \\
		t_j \leq& s_{2,j} \\
		1 + t_j \geq& s_{1,j} + s_{2,j} \\
		t_j \in& \{0,1\}
	\end{aligned}
	\label{eq:product-linear}
\end{equation}
%
If there is an existing feature set, i.e., we either know $s_1$ or $s_2$, Equation~\ref{eq:feature-set-size} already is linear without Equation~\ref{eq:product-linear}.

Overall, one can express alternative feature sets following Definition~\ref{def:single-alternative} with 0-1 integer linear constraints.
This simple constraint type allows using a broad range of solvers, given a suitable objective function, which we discuss later.

\subsubsection{Multiple Alternatives}
\label{sec:approach:constraints:multiple}

If the user desires multiple alternative feature sets rather than only one, we can compute these alternatives either sequentially or simultaneously.

\paragraph{Sequential alternatives}

In the sequential case, the user gets several alternatives iteratively, with one alternative feature set per iteration.
We constrain this new feature set to be an alternative to all previously found feature sets:
%
\begin{definition}[Sequential alternative]
	A feature set $F_2$ is an alternative to a set of feature sets $\mathbb{F}$ (and vice versa) for a dissimilarity threshold $\tau \in \mathbb{R}_{\geq 0}$ if $\forall F_1 \in \mathbb{F}: d(F_1,F_2) \geq \tau$.
	\label{def:sequential-alternative}
\end{definition}
%
This definition duplicates the constraint from the base case and instantiates it for multiple existing feature sets.
One could also use a less strict constraint, e.g., requiring only the average dissimilarity to all existing feature sets to pass the threshold.
We do not consider this case in our article.

The objective function remains the same as in the base case, i.e., we optimize the quality of the new feature set~$F_2$.
Thus, the number of variables in the optimization problem remains constant, independent of the number of alternatives.
As we are alway comparing a new, variable feature set to existing feature sets, we also do not need to introduce interaction variables as in Equation~\ref{eq:product-linear}.
Each alternative only adds one new constraint to the optimization problem.
Thus, we expect the runtime of sequential search to scale well with the number of alternatives.
Further runtime gains might be possible if the solver keeps a state between iterations and can warm-start.

As the solution space becomes narrower over iterations, feature-set quality can drop with each further alternative.
After each iteration, the user can decide if the feature-set quality is too low or if another alternative should be found.

\paragraph{Simultaneous alternatives}

In the simultaneous case, we directly obtain multiple alternatives.
The user needs to decide on the number of feature sets $\mathbb{F}$ beforehand.
We formulate pairwise dissimilarity constraints:
%
\begin{definition}[Simultaneous alternatives]
	A set of feature sets $\mathbb{F}$ contains simultaneous alternatives for a dissimilarity threshold $\tau \in \mathbb{R}_{\geq 0}$ if $\forall F_1 \in \mathbb{F}, F_2 \in \mathbb{F}, F_1 \neq F_2: d(F_1,F_2) \geq \tau$.
	\label{def:simultaneous-alternative}
\end{definition}
%
Again, one could resort to less strict constraints, e.g., based on the average dissimilarity between alternatives.

In contrast to the sequential case, we need to introduce further decision variables and modify the objective function here, as we optimize multiple feature sets at once.
In this paper, we consider the summed quality of all feature sets as objective.

Runtime-wise, we expect simultaneous search to scale worse with the number of alternatives than sequential search.
The number of decision variables increases linearly with the number of alternatives while the number of constraints grows quadratically.
Also, for each feature and each pair of alternatives, we need to introduce an interaction variable as in Equation~\ref{eq:product-linear} if we want to obtain a linear problem.

In contrast to the greedy procedure of sequential search, simultaneous search optimizes alternatives globally.
Thus, the simultaneous procedure should yield same or higher average feature-set quality for the same number of alternatives.
Also, we expect the qualities of the alternatives to be more evenly distributed, as opposed to the dropping quality over the course of the sequential procedure.

\subsection{Objective Functions -- Finding Alternatives}
\label{sec:approach:objectives}

In this section, we present approaches for finding alternative feature sets.
We discuss how to solve the optimization problem from Section~\ref{sec:approach:problem} for the different categories of feature-set quality measures from Section~\ref{sec:fundamentals:quality}.
In particular, we categorize solution approaches as white-box optimization, black-box optimization, and embedding alternatives.

\subsubsection{White-Box Optimization}
\label{sec:approach:objectives:white-box}

If the feature-set quality function~$Q(s,X,y)$ is sufficiently simple, one can tackle alternative feature selection with a suitable white-box solver.
In Section~\ref{sec:approach:constraints}, we already showed that our notion of alternative feature sets results in 0-1 integer linear constraints.
In this section, we present white-box optimization objectives for different feature-selection methods.

\paragraph{Univariate filter feature selection}

For univariate filter feature selection, the objective function is linear.
Thus, the optimization problem of alternative feature selection becomes a 0-1 integer linear problem.
In particular, univariate filter methods decompose the quality of a feature set into the quality of the individual features:
%
\begin{equation}
	Q_{\text{uni}}(s,X,y) = \sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j
	\label{eq:univariate-filter}
\end{equation}
%
Here, $q(\cdot)$ typically is a bivariate dependency measure, e.g., mutual information~\cite{kraskov2004estimating} or absolute value of Pearson correlation, to quantify the relationship between a feature and the prediction target.

\paragraph{Post-hoc feature importance}

From the technical perspective, one can also insert values of post-hoc feature-importance scores for $q(\cdot)$ in Equation~\ref{eq:univariate-filter}.
For example, one can pre-compute permutation importance~\cite{breiman2001random} or SAGE scores~\cite{covert2020understanding} for each feature.
However, such post-hoc importance scores often evaluate the usefulness of features in the presence of other features.
Thus, the feature-independence assumption underlying Equation~\ref{eq:univariate-filter} does not hold.
In theory, one would have to re-calculate feature importance for each feature set.
However, such a procedure makes a white-box approach infeasible.
In practice, one can still use Equation~\ref{eq:univariate-filter} with importance scores only computed on the full dataset.
One only needs to be aware that such an approach might not represent feature interactions, e.g., feature importances in particular subsets, faithfully.

\paragraph{Multivariate filter feature selection}

Several multivariate filter methods allow for a simple white-box formulation as well, though not necessarily a linear one.
In the following, we describe FCBF~\cite{yu2003feature}, which we use in our experiments.
Appendix~\ref{sec:appendix:multivariate-filter-objectives}~discusses CFS~\cite{hall1999correlation, hall2000correlation}, mRMR~\cite{peng2005feature}, and Relief~\cite{kira1992feature, robnik1997adaptation}.

The Fast Correlation-Based Filter (FCBF)~\cite{yu2003feature} bases on the notion of predominance:
It strives to select features that have at least a certain correlation to the prediction target but a lower correlation to all other features.
While the original FCBF algorithm uses a heuristic search, we propose a formulation as a constrained optimization problem:
%
\begin{equation}
	\begin{aligned}
		\max_s &\quad Q_{\text{FCBF}}(s,X,y) = \sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j \\
		\text{subject to:} &\quad \forall (j_1,j_2) \in \{1, \dots, n\}^2, j_1 \neq j_2: \\
		&\quad q(X_{\cdot{}j_1}, X_{\cdot{}j_2}) \cdot s_{j_1} \cdot s_{j_2} < q(X_{\cdot{}j_1},y)
	\end{aligned}
	\label{eq:fcbf}
\end{equation}
%
In particular, we drop the original FCBF's threshold parameter on feature-target correlation and maximize the latter instead, as in the univariate filter case.
To limit the size of the resulting feature set, one can put a constraint on the feature set size.
Also, if one still wants to definitely prevent selection of features with low correlation to the prediction target, one can filter out these features before optimization.
With the constraint in Equation~\ref{eq:fcbf}, we enforce that if two feature are selected, their correlation has to be lower than each feature's target correlation.
The optimization problem is linear if one appropriately handles the product term between decision variables.

\subsubsection{Black-Box Optimization}
\label{sec:approach:objectives:black-box}

If feature-set quality has not simple closed-form expression, one needs to treat it as a black-box function when searching for alternative feature sets.
This situation applies to the category of wrapper feature selection.
One can optimize such black-box functions using search heuristics, systematically iterating over candidate feature sets.
However, search heuristics often assume an unconstrained search space.
For example, they might propose a candidate feature set that is not alternative enough.
We see multiple ways to address this challenge:

\paragraph{Enumerating feature sets}

One can relinquish the idea of a search heuristic and just enumerate all feature sets that fulfill the constraints for being alternative.
This most naive idea is iterating over all feature sets and sorting out those not alternative enough.
A bit more focused is using a solver to enumerate all valid alternatives.
While such an approach is technically simple, it might very inefficient, as there can be a huge number of alternative feature sets.

\paragraph{Sampling feature sets}

Instead of considering all possible alternatives, one can also sample a limited number of them.
Again, the naive way is sampling from all feature sets, but removing those samples that are not alternative enough.
If the number of valid alternatives is low to the total number of feature sets, this approach might need a lot of samples.
In fact, uniform sampling from a constrained space is a computationally hard problem, believed to be harder than only determining if a valid solution exists or not~\cite{ermon2012uniform}.

\paragraph{Multi-objective optimization}

If one considers alternative feature selection as a multi-objective problem, as mentioned in Section~\ref{sec:approach:problem}, there are no hard constraints anymore.
Given a suitable multi-objective search procedure, the decision on trading off being alternative against feature-set quality might be postponed till after the search.

\paragraph{Adapting search}

One can adapt a search heuristic to consider the constraints for being alternative.
One idea is to prevent the search heuristic from producing feature sets that violate the constraints, or at least making the latter less likely, e.g., be penalizing the objective function accordingly.
Another idea is to `repair' feature sets proposed by the search that are not alternative enough.
For example, one can replace a feature set violating the constraint with the most similar feature set satisfying the constraints.
Such solver-assisted search approaches are common in search procedure for valid configuration in software feature models~\cite{white2010automated,henard2015combining,guo2018preserve}.

As a simple wrapper approach for our experiments, we use a greedy hill-climbing strategy, as displayed in Algorithm~\ref{al:greedy-wrapper}.
Compared to standard hill-climbing~\cite{kohavi1997wrappers}, our search procedure only evaluates feature sets that satisfy the constraints for alternatives.
First, the algorithm uses a solver to find one solution that is alternative enough, given the current constraints.
Thus, it has a valid starting point and can always return a solution, unless there are no valid solutions at all.
Next, it tries `swapping' one feature, i.e., selecting the feature if it was deselected or deselecting it if it was selected.
The algorithm calls the solver again to find a solution~$s'$ containing this change and satisfying the other constraints.
If such a solution~$s'$ exists and its quality~$Q(s',X,y)$ improves the current solution, the algorithm continues from the new solution.
Else, it attempts to swap the next feature.
The algorithm terminates if no swap of a feature leads to an improvement or a fixed amount of iterations $max\_iters$ is reached.
We define the iteration count as the number of calls to the solver, i.e., attempts to generate feature sets, as this step might be costly.
This also is an upper bound on the number of prediction models trained.
However, not all solver calls might yield a valid feature set.

\begin{algorithm}[htb]
	\DontPrintSemicolon
	\KwIn{Dataset $X$, Prediction target $y$, \newline
		Feature-set quality function $Q(\cdot)$, \newline
		Constraints for alternatives $Cons$, \newline
		Maximum number of iterations $max\_iters$}
	\KwOut{Feature-selection decision vector $s$}
	\BlankLine
	$s \leftarrow \text{Solve}(Cons)$ \tcp*{Initial alternative}
	$iters \leftarrow 1$ \tcp*{Number of iterations = solver calls}
	\If(\tcp*[f]{No valid alternative}){$s = \emptyset$}{
		\Return{$\emptyset$}
	}
	$j \leftarrow 1$ \tcp*{Index of feature to be swapped}
	\While{$iters < max\_iters$ \textbf{and} $j \leq |s|$}{
		$s' \leftarrow \text{Solve}(Cons \cup \{\neg s_j\})$ \tcp*{Try swapping one feature}
		$iters \leftarrow iters + 1$\;
		\If(\tcp*[f]{Swap if improved}){$s' \neq \emptyset$ \textbf{and} $Q(s',X,y) > Q(s,X,y)$}{
			$s \leftarrow s'$\;
			$j \leftarrow 1$\;
		}
		\Else(\tcp*[f]{Try next feature}){
			$j \leftarrow j + 1$\;
		}
	}
	\Return{$s$}
	\caption{Constraint-aware greedy wrapper feature selection.}
	\label{al:greedy-wrapper}
\end{algorithm}

\subsubsection{Embedding Alternatives}
\label{sec:approach:objectives:embedding}

If feature selection is embedded into training a prediction model, there is no general approach for finding alternative feature sets.
Instead, one would need to embed the search for alternatives into training as well.
For example, one could prevent decision trees from splitting on a feature if the resulting feature set was too similar to a given feature set.
We leave the formulation of such specific approaches open for future work.

\section{Related Work}
\label{sec:related-work}

In this section, we review literature from areas related to alternative feature selection, within the field of feature selection itself, closely related fields, alternative clustering, and counterfactual explanations.
To the best of our knowledge, searching for optimal alternative feature sets is novel.
However, there is literature on optimal alternatives outside the field of feature selection.
Also, there are works on finding multiple, diverse feature sets.

\subsection{Feature Selection}

\paragraph{Traditional feature selection}

Most feature-selection methods only yield one solution~\cite{borboudakis2021extending}, though there are some exceptions.
Nevertheless, none of the following approaches explicitly searches for optimal alternatives.
\cite{siddiqi2020genetic}~adapts a genetic algorithm to ensure diversity of feature sets in the population.
In particular, they modify the fitness criterion to penalize overlap of feature sets in each iteration.
\cite{emmanouilidis1999selecting}~uses multi-objective genetic algorithms to obtain prediction models of different complexity, using feature sets of different sizes.
\cite{mueller2021feature}~clusters features and forms all combinations by picking one feature from each cluster.
They do this to reduce the number of features, not as a guided search for alternatives.

\paragraph{Ensemble feature selection}

Ensemble feature selection~\cite{saeys2008robust, seijo2017ensemble} combines feature-selection results, e.g., obtained by different feature-selection methods or on different samples of the data.
Fostering diverse feature sets might be a sub-goal to improve prediction performance, but is only an intermediate step.
This focus differs from our goal of finding optimal alternative feature sets.

\cite{woznica2012model}~uses k-medoid clustering and frequent-itemset mining to reduce a set of feature-selection results, obtained on bootstrap samples of the data, to a smaller, yet diverse subset.
\cite{liu2019subspace}~builds an ensemble prediction model from classifiers trained on different feature sets and uses an evolutionary algorithm that ensures diversity of these feature sets.
\cite{guru2018alternative}~selects features separately for each class and then combines the results.

\paragraph{Statistically equivalent feature sets}

Approaches for statistically equivalent feature sets~\cite{borboudakis2021extending, lagani2017feature} use statistical tests to determine which features or feature sets are equivalent for predictions.
For example, a feature may be independent from the prediction target given another feature.
A search algorithm conducts multiple such tests and either outputs equivalent feature sets or a feature grouping that allows forming such non-redundant feature sets.

Our notion of alternative feature sets differs from equivalent feature sets.
In particular, it is not straightforward to build optimal alternatives from such equivalent feature sets.
Depending on how the statistical test are configured, there can be an arbitrary number of equivalent feature sets, without an explicit quality-based ordering.
Instead, we always provide a fixed number of alternatives, unless the problem is infeasible.
Also, our alternatives need not be equivalent in terms of their quality, but should be optimal under constraints.
Our dissimilarity threshold allows controlling overlap between feature sets instead of eliminating all feature redundancies.
Nevertheless, one can also add constraints to our optimization problem to exclude predefined redundant selections.

\paragraph{Constrained feature selection}

There already is work on considering various kinds of constraints in feature selection, e.g., for feature cost~\cite{paclik2002feature}, feature groups~\cite{yuan2006model}, or domain knowledge~\cite{bach2022empirical, groves2015toward}.
These approaches are orthogonal to our work, as such constraints can be added to our optimization problem as well.

\subsection{Fields Related to Feature Selection}

\paragraph{Subgroup set discovery}

\cite{leeuwen2012diverse} presents six strategies to foster diversity in subgroup set discovery, which searches for interesting regions in the data space rather than purely selecting features.
They integrate these strategies into beam search, i.e, a heuristic search procedure, while we also consider exact solutions.

\paragraph{Subspace clustering and subspace search}

Finding multiple useful feature set plays a role in subspace clustering~\cite{guan2011unified, hu2018subspace, mueller2009relevant} and subspace search~\cite{fouche2021efficient, nguyen20134s, trittenbach2019dimension}.
These approaches strive to improve the results of data-mining algorithms by using subspaces, i.e., feature sets, rather than the full space, i.e., all features.
While some subspace approaches only consider features utility in individual subspaces, others explicitly try to remove redundancy between subspaces~\cite{mueller2009relevant, nguyen20134s} or foster subspace diversity~\cite{fouche2021efficient, trittenbach2019dimension}, which bears some resemblance to alternative feature selection.
Nevertheless, these scenarios are fundamentally different to ours, e.g., we are in a supervised-learning setting.

\subsection{Alternative Clustering}

Finding alternative solutions has been addressed extensively in the field of clustering.
\cite{bailey2014alternative} gives a taxonomy and describes algorithms for alternative clustering.
Our problem definition in Sections~\ref{sec:approach:problem} and~\ref{sec:approach:constraints} is, on a high level, inspired by the one in~\cite{bailey2014alternative}:
Find one or multiple solutions that maximize quality while minimizing similarity.
\cite{bailey2014alternative} also distinguishes between singular and multiple alternatives, found sequentially or simultaneously.
Nevertheless, the concrete problem definition for a clustering scenario and our feature-selection scenario is different.
Besides constraint-based search for alternatives, \cite{bailey2014alternative} also discuss other solution paradigms, e.g., using feature selection and data transformation~\cite{tao2012novel}.

Two examples of alternative clustering are \emph{COALA}~\cite{bae2006coala} and \emph{MAXIMUS}~\cite{bae2010clustering}.
COALA~\cite{bae2006coala} imposes \emph{cannot-link constraints} on pairs of data objects:
Data objects from the same cluster should be assigned to different clusters in the alternative clustering.
If observing the constraints is infeasible or violates a quality threshold, the algorithms proceeds without constraints.
MAXIMUS~\cite{bae2010clustering} employs an integer program to formulate dissimilarity between clusterings, building on the value distributions in clusters, and uses a constrained clustering procedure.

\subsection{Counterfactual Explanations}

Counterfactual explanations~\cite{stepin2021survey, verma2020counterfactual} uncover solutions for alternative predictions.
The search for counterfactuals can be formulated as an optimization problem, e.g., an integer problem~\cite{mohammadi2021scaling} or Satisfiability Modulo Theories (SMT)~\cite{karimi2020model}.
Finding diverse counterfactuals~\cite{mothilal2020explaining} is a topic as well.
However, there are crucial differences between counterfactuals and alternative feature sets:
Counterfactual explanations target at predictions on single data objects rather than the global predictive quality of features.
Also, counterfactuals want to alter the prediction outcome by changing feature values little, while alternative feature sets should keep the prediction quality while changing the selection of features significantly.

\section{Experimental Design}
\label{sec:experimental-design}

In this section, we describe our experimental design.
We give a brief overview of its goal and components before elaborating on the components in detail.

\subsection{Overview}
\label{sec:experimental-design:overview}

We conduct experiments with 30 binary-classification datasets.
In our evaluation, we focus on the trade-off between feature-set quality and obtaining alternative feature sets.
We compare four feature-selection methods, representing different notions of feature-set quality.
Also, we train prediction models with the resulting feature sets and analyze prediction performance.
To find multiple alternative feature sets, we consider a simultaneous as well as a sequential approach.
We systematically vary the number of alternatives and the dissimilarity threshold for being alternative.

\subsection{Methods}
\label{sec:experimental-design:approaches}

Our experimental design comprises multiple approaches for feature selection, defining alternatives, and making predictions.

\subsubsection{Feature Selection (Objective Functions)}
\label{sec:experimental-design:approaches:feature-selection}

We search for alternatives under different notions of feature-set quality as objective function.
To have a diverse set of feature-selection methods, we consider the different categories from Section~\ref{sec:fundamentals:quality}.
We choose four well-known feature-selection methods that are either parameter-free or easy to parameterize.
See Section~\ref{sec:approach:objectives} for details on the objective function of each method.
One method (\emph{Greedy Wrapper}) requires black-box optimization, while the other three methods have white-box objectives.
As explained in Section~\ref{sec:approach:objectives:embedding}, we do not include an embedded feature-selection method.

For each feature-selection method, we set the number of selected features to $k \in \{5,10\}$.
These values yield small feature sets, fostering interpretability.
We enforce the desired $k$ with a simple constraint in our optimization problems.

\paragraph{MI}

For univariate filtering, we use Equation~\ref{eq:univariate-filter} with mutual information~\cite{kraskov2004estimating} as the dependency measure~$q(\cdot)$.
This measure allows to capture arbitrary dependencies rather than, say, just linear correlations.
To improve comparability between datasets and values of $k$, we normalize the qualities such that selecting all features yields a quality of 1 and selecting no feature yields a quality of 0.

\paragraph{FCBF}

As a multivariate filter method, we use our constrained formulation of FCBF~\cite{yu2003feature} according to Equation~\ref{eq:fcbf}.
As bivariate dependency measure within FCBF, we rely on mutual information again, normalized to sum up to 1.

\paragraph{Greedy Wrapper}

As a wrapper method, we employ the greedy hill-climbing strategy from Algorithm~\ref{al:greedy-wrapper} with $max\_iters$ set to 1000.
To evaluate feature-set quality within the wrapper, we choose a decision tree as the prediction model and Matthews correlation coefficient (MCC)~\cite{matthews1975comparison} as the performance metric.
To test generalization performance, we apply a stratified 80:20 holdout split:
$Q(s,X,y)$ corresponds to the MCC on the 20\% validation part of the data.

\paragraph{Model Gain}

As post-hoc importance measure, we take model-based feature importance provided by \emph{scikit-learn}.
Again, we use a decision tree as the model.
There, importance expresses a feature's contribution towards optimizing the split criterion of the tree, for which we choose information gain, i.e., mutual information.
These importances are normalized to sum up to 1 by default.
We plug the importances into Equation~\ref{eq:univariate-filter}, i.e., treat the importances like univariate filter scores.
Note that the interpretation is different, though.
In univariate filters, the scores consider features independently.
Here, the scores originate from trees trained with all features rather than assessing features in isolation.

\subsubsection{Alternatives (Constraints)}
\label{sec:experimental-design:approaches:alternatives}

We employ \emph{sequential} as well as \emph{simultaneous} search for alternatives, using the corresponding definitions from Section~\ref{sec:approach:constraints:multiple}.
We evaluate 1 to 10 alternatives for sequential search, while we examine 1 to 5 alternatives for simultaneous search.
This difference stems from the increased runtime for simultaneous search, caused by the larger size of the optimization problem, i.e., more decision variables and more constraints.
For the dissimilarity threshold $\tau$, we analyze all possible sizes of the overlap, or rather the difference, between feature sets.
Thus, for $k=5$, we consider values of $\tau$ from 0.2 to 1 with a step size of 0.2, corresponding to an overlap of four to zero features.
For $k=10$ we consider values of $\tau$ from 0.1 to 1 with a step size of 0.1.
Naturally, we exclude $\tau = 0$, which would allow returning duplicate feature sets.

As solver runtime may vary considerably even for optimization problems of the same size, we set a timeout of 60~s on each solver run.
If the solver finishes before the timeout, it either finds an optimal solution or proves no solution exists.
These two outcomes apply to 94\% of the feature sets in our evaluation.
In the remaining cases, the solver either found a feasible but potentially suboptimal solution or found no solution though one might exist.

%As a simple \emph{baseline} compared to optimization, we use iterative solving to obtain arbitrary feature sets that satisfy the constraints for being alternative.
%In each iteration, the solver has to return a valid feature set that it did not return in a previous iteration.
%To obtain a large sample, we conduct 100 iterations.
%As we neither know nor influence how the solver chooses these solutions, we consider them to be random.
%However, depending on how the solver obtains them, they might not be uniformly random from the space of valid feature sets.

\subsubsection{Prediction}
\label{sec:experimental-design:approaches:prediction}

As prediction models, we use decision trees~\cite{breiman1984classification} as well as random forests with 100 trees \cite{breiman2001random}.
Both types of models allow learning of complex, non-linear dependencies from the data.
We leave the hyperparameters of the models mostly at their defaults, apart from two changes:
First, we choose information gain instead of Gini impurity as split criterion, to be consistent to our filter feature-selection methods.
Second, we specify a random seed for reproducibility reasons.

Note that tree models also carry out feature selection themselves, i.e., they are embedded approaches.
Thus, they might not use all features that we provide to them.
However, this is no problem for our study.
We are interested in which performance the models achieve if they are limited to small alternative feature sets, not how they use each feature from an alternative.

\subsection{Evaluation Metrics}
\label{sec:experimental-design:evaluation}

We use two kinds of metrics for feature-set quality.
We focus on on how these metrics evolve for different configurations of alternative feature selection, cf. Section~\ref{sec:experimental-design:approaches:alternatives}.
First, we evaluate the objective functions~$Q(s,X,y)$ of the feature-selection methods, which guided the search for feature sets.
Second, we train predictions models with the found feature sets.
We report prediction performance in terms of the Matthews correlation coefficient (MCC), which is insensitive to class imbalance.
This coefficient reaches its maximum of 1 for perfect predictions and is 0 for random guessing.

To analyze how well feature-selection and prediction models generalize, we conduct stratified 10-fold cross-validation.
Not only model training but also the search for alternative feature sets is limited to the training data.

\subsection{Datasets}
\label{sec:experimental-design:datasets}

We evaluate alternative feature selection on a variety of datasets from the Penn Machine Learning Benchmarks (PMLB)~\cite{olson2017pmlb,romano2021pmlb}.
To focus on one machine-learning task type, we only consider binary-classification datasets.
However, alternative feature selection is possible for regression and multi-class problems as well.
We filter out datasets containing less than 100 data objects.
Also, we filter out datasets with less than 15 features to leave some room for alternatives.
Next, we remove one dataset with 1000 features, which would dominate the overall runtime of the experiments.
Finally, we manually remove datasets that seem to be duplicated or modified versions of other datasets from the benchmark.

In consequence, we obtain 30 datasets with 106 to 9822 data objects and 15 to 168 features.
The datasets contain no missing values.
Categorical features have an ordinal encoding by default.

\subsection{Implementation}
\label{sec:experimental-design:implementation}

We implement our experimental pipeline in Python.
For machine learning, we use the package \emph{scikit-learn}~\cite{pedregosa2011scikit-learn}.
To express and solve the constraint optimization problems, we use the solver \emph{CBC} via the package \emph{OR-Tools}~\cite{perron2022or-tools}.

\section{Evaluation}
\label{sec:evaluation}

In this section, we evaluate our experiments.
In particular, we discuss the results for the different dimensions of our experimental design:
datasets, prediction models, feature-selection methods, and approaches to search for alternatives.

\subsection{Datasets}

Naturally, feature-set quality depends on the datasets used, and several effects could occur.
For example, the distribution of feature-set quality in the datasets might be relatively uniform or relatively skewed.
Datasets with more features $n$ give way to more alternative feature sets.
At the same time, the (normalized) feature quality $q(X_{\cdot{}j},y)$ can be spread over more features than for smaller datasets, making it harder to compose a small high-quality feature set.

Indeed, our experiments show a broad variation of feature-set quality over datasets.
Further, we do not observe a clear relationship between dataset dimensionality~$n$ and feature-set quality.

\subsection{Prediction Models}

\begin{figure}[htb]
	\centering
	\includegraphics[width=0.7\textwidth, trim=15 15 15 15, clip]{plots/evaluation-metrics-correlation.pdf}
	\caption{Correlation between evaluation metrics over all experimental settings.}
	\label{fig:evaluation-metrics-correlation}
\end{figure}

As one can expect, the average prediction performance of random forests is higher than that of decision trees.
Also, overfitting occurs for both model types, i.e., there is a gap between training-set and test-set prediction performance.
This observation makes sense as we do not regularize, i.e., limit the growth of, the trees or prune them after training.
However, this will not invalidate our analysis of how prediction performance develops over alternatives.
The optimization objective $Q$ also shows overfitting for all feature-selection methods, though to a lesser extent than prediction performance.

Figure~\ref{fig:evaluation-metrics-correlation} shows the Spearman correlation between different evaluation metrics.
As the performance of decision trees and random forests is highly correlated on training set as well as test set, we only focus on one prediction model in the following:
We choose decision trees, as they always consider all features during training, while random forests involve random sampling of features.

Figure~\ref{fig:evaluation-metrics-correlation} also shows that the correlation between training-set quality and test-set quality is moderate, but not high, for the optimization objective~$Q$ as well as prediction performance in terms of MCC.
This might be caused by different degrees of overfitting, depending on the experimental settings.
Further, the correlation between optimization objective~$Q$ and prediction MCC is only weak to moderate.
I.e., the objective of feature selection is only partially indicative of prediction performance since the former might use a simplified quality criterion.

\subsection{Feature-Selection Methods}

As the different feature-selection methods have different objective functions~$Q$, it does not make sense to compare objective values between feature-selection methods.
Regarding the optimization status, we note that 89\% of feature sets for \emph{FCBF} could not be determined, as the solver timed out or the optimization problem was infeasible.
In contrast, this figure only is 19\% for MI.
In particular,the feature-correlation constraints in our formulation of \emph{FCBF} (c.f.~Equation~\ref{eq:fcbf}) seemingly made it hard to find valid feature sets.
This point needs to be considered when applying this \emph{FCBF} feature selector.

Regarding test-set prediction performance, \emph{Model Gain} beats \emph{MI} and \emph{Greedy Wrapper} on average.
The median test-set MCC for decision trees is 0.57 for \emph{Model Gain}, 0.54 for \emph{MI}, and 0.51 for \emph{Greedy Wrapper}.
Unsurprisingly, model-based importance yields better feature sets than univariate, model-free feature scoring with \emph{MI}.
The relatively bad performance of \emph{Greedy Wrapper} might result from its heuristic nature:
The wrapper search can only evaluate a fraction of all feasible feature sets and might get stuck in local optima, while the remaining feature-selection methods optimize globally.

The feature-set size $k$ shows the expected effect:
Larger feature sets, i.e., $k=10$, exhibit higher feature-set quality than smaller feature sets, i.e., $k=5$.
However, doubling the number of features does not double the prediction performance.
Over all experimental settings, the median test-set MCC of decision trees is 0.50 for $k=5$ and 0.60 for $k=10$.
This displays that small feature sets can already achieve a large share of the possible prediction performance.
For the optimization objective $Q$, the increase from $k=5$ to $k=10$ is less than proportional with~$k$ as well.

\subsection{Searching Alternatives}

\subsubsection{Search Method}

\begin{figure}[htb]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=5 25 10 10, clip]{plots/impact-search-stddev-objective.pdf}
		\caption{Standard deviation of objective.}
		\label{fig:impact-search-stddev-objective}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=5 25 10 10, clip]{plots/impact-search-mean-objective.pdf}
		\caption{Mean objective.}
		\label{fig:impact-search-mean-objective}
	\end{subfigure}
	\caption{Training-set objective value $Q$ in experimental runs of sequential or simultaneous search.
	Outliers removed for readability reasons.}
	\label{fig:impact-search-objective}
\end{figure}

As we expected, the training-set objective value $Q$ of feature sets within a particular search run varies more for sequential search than for simultaneous search.
Figure~\ref{fig:impact-search-stddev-objective} visualizes this observation.
Also, the variance slightly increases with the number of alternatives for sequential search, but decreases for simultaneous search.
I.e., simultaneous search finds more quality-homogeneous feature sets, in particular, if many alternatives are desired.
However, on the test set, the variance of $Q$ also increases with the number of alternatives for simultaneous search.
This effect might be a result of overfitting.
Regarding test-set prediction performance, simultaneous search does not even show a lower variance than sequential search anymore.

Surprisingly, the average feature-set quality of simultaneous search is not higher than for sequential search, neither regarding objective value $Q$ nor prediction performance.
Figure~\ref{fig:impact-search-mean-objective} shows this behavior for the mean training-set objective of search runs.
Depending on the concrete search setting, either of the two search types might yield better-performing feature sets.
An explanation for this phenomenon is that search results can be sub-optimal.
For \emph{Greedy Wrapper}, the search is heuristic per se.
Additionally, the exact optimization of the other three feature-selection methods can time out.
As simultaneous search is harder than sequential search, it is affected stronger by this phenomenon.
In particular, 10\% of the feature sets from simultaneous search were feasible but potentially sub-optimal, but no feature sets for sequential search.
This figure strongly depends on the number of alternatives:
For up to three alternatives in simultaneous search, less than 1\% of feature sets are potentially sub-optimal but 4\% for four alternatives and 32\% for five alternatives.
Based on these results, we focus on the sequential search in the following.

\subsubsection{Number of Alternatives}

\begin{figure}[htb]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=5 25 10 5, clip]{plots/impact-num-alternatives-objective-max.pdf}
		\caption{Max-normalization.}
		\label{fig:impact-num-alternatives-objective-max}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=5 25 10 5, clip]{plots/impact-num-alternatives-objective-min-max.pdf}
		\caption{Min-max-normalization.}
		\label{fig:impact-num-alternatives-objective-min-max}
	\end{subfigure}
	\caption{Objective value $Q$, normalized per experimental setting, in sequential search, using \emph{MI} as feature selector.
	Outliers removed for readability reasons.}
	\label{fig:impact-num-alternatives-objective}
\end{figure}

For sequential search, the training-set objective value $Q$ naturally decreases with the number of alternatives, at least for the feature-selection criteria optimized exactly.
Figure~\ref{fig:impact-num-alternatives-objective} illustrates this for \emph{MI}-based feature selection.
The other feature-selection methods, including the heuristic \emph{Greedy Wrapper}, exhibit similar effects.
As feature-set quality varies between datasets and feature-set sizes~$k$, we apply normalization:
In Figure~\ref{fig:impact-num-alternatives-objective-max}, we have max-normalized the objective value for each search of alternatives, i.e., the highest objective value in the search run is scaled to~1 and the other feature-set qualities are scaled accordingly.
This figure shows that there might be multiple alternatives of similar quality, as the median feature-set quality remains relatively stable over the number of alternatives, and is close to the maximum of 1.
Figure~\ref{fig:impact-num-alternatives-objective-min-max} uses min-max normalization, i.e., the worst of the alternatives gets~0 as objective.
This figure highlights that the training-set objective value decreases most from the original feature set to the first alternative but less beyond.

Additionally, Figure~\ref{fig:impact-num-alternatives-objective} shows that the test-set objective value also drops most from the original feature set to the first alternative.
However, the decrease in objective value over the alternatives is less prominent than on the training set.
In particular, alternatives can even have a higher test-set objective value than the original feature set, due to overfitting of the optimization procedure.
Similar findings hold for test-set prediction performance.
Thus, the alternative feature sets fulfill their purpose of being different solutions with similar quality.

Be aware that these observations refer to the quality of the found feature sets.
However, the more alternatives are desired, the more likely an infeasible optimization problem is.
For example, the \emph{MI} feature selector in sequential search always finds an original feature set.
However, the problem is infeasible in 2\% of the cases for the first alternative, 14\% for the second alternative, 24\% for the fifth alternative, and 30\% for the tenth alternative.
Thus, while the quality of feature-sets remains relatively stable with increased number of alternatives, valid alternatives might simply not exist.
These figures also depend on the dissimilarity threshold $\tau$, which we analyze next.

\subsubsection{Dissimilarity Threshold \texorpdfstring{$\tau$}{}} % \texorpdfstring prevents warning "Token not allowed in a PDF string"

\begin{figure}[htb]
	\centering
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=5 15 30 10, clip]{plots/impact-num-alternatives-train-objective-tau.pdf}
		\caption{Training set.}
		\label{fig:impact-num-alternatives-train-objective-tau}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth, trim=5 15 30 10, clip]{plots/impact-num-alternatives-test-objective-tau.pdf}
		\caption{Test set.}
		\label{fig:impact-num-alternatives-test-objective-tau}
	\end{subfigure}
	\caption{Median objective value $Q$, max-normalized per experimental setting, in sequential search, using \emph{MI} as feature selector and $k=10$.}
	\label{fig:impact-num-alternatives-objective-tau}
\end{figure}

As Figure~\ref{fig:impact-num-alternatives-objective-tau} shows for \emph{MI} as feature selector, the decrease of the objective value $Q$ over the number of alternatives can strongly depend on the dissimilarity threshold $\tau$.
For a low dissimilarity threshold, e.g., $\tau=0.1$, the objective value barely drops over the number of alternatives.
In contrast, the objective decrease significantly for a high dissimilarity threshold, e.g., $\tau=1$.
This phenomenon also holds for test-set objective and test-set prediction performance, though the decrease with $\tau$ is less prominent.
Also, the fraction of infeasible optimization problems, i.e., lack of valid alternative feature sets, increases with $\tau$:
For a higher dissimilarity threshold, the likelihood is higher that there is no feature set that is alternative enough.

While the previous observation were made for \emph{MI} as feature selector, they do not hold universally.
Besides \emph{MI}, the results for \emph{Model Gain} strongly depend on \emph{tau} as well.
In contrast, \emph{FCBF} and \emph{Greedy Wrapper} exhibit less influence of $\tau$ on feature-set quality.
For \emph{Greedy Wrapper}, this can be explained by its heuristic search procedure.
For \emph{FCBF}, the fraction of infeasible solutions is much higher than for \emph{MI} in general, so adding further constraints has less impact.

\section{Conclusions and Future Work}
\label{sec:conclusion}

\subsection{Conclusions}

Obtaining interpretable solutions is a highly active area of research in machine learning.
One way to foster interpretability is by selecting a small set of features for predictions.
Traditional feature-selection methods yield \emph{one} feature set with high quality, e.g., regarding prediction performance.
However, users might be interested in obtaining multiple, sufficiently different feature sets that, at the same time, have high quality.
Such alternative feature set might provide alternative explanations for predictions from the data.

In this paper, we defined alternative feature selection as an optimization problem.
We formalized alternatives via constraints, which are independent from the feature-selection method and can be combined with other constraints on feature sets, e.g., based on domain knowledge.
Further, we presented approaches to solve this optimization problem for different categories of feature selectors.
Finally, we ran an evaluation with 30 classification datasets and four feature-selection methods.
We compared two search strategies for alternatives and varied the threshold for being alternative.

In our experiments, we found that one can indeed obtain alternative feature sets with similar quality as the original ones.
This worked with simultaneous search as well as sequential search for alternatives.
The latter procedure was significantly faster and allows stopping the search once the user does not want any more alternatives.
As expected, the quality of alternatives drops the more alternatives one desires, and the more alternatives should differ from existing feature sets.
The ultimate decision on which trade-off between feature-set quality and alternatives is acceptable lies with the user.
Thus, we recommend evaluating different dissimilarity thresholds for alternatives to find a suitable value.

\subsection{Future Work}

While our work introduced alternative feature selection, one might vary several points of our approach.
For example, one can combine the search for alternatives with other feature-selection methods.
For embedded feature selection, one would need to customize our search routines and push them into model training.
Further, one can vary the notion of alternatives, e.g., the set-dissimilarity measure, definition for multiple alternatives, or soft constraints instead of hard constraints.
Finally, while our evaluation was domain-independent, the benefit of alternative feature sets for explainable predictions should be analyzed in domain-specific case studies.

\appendix

\section{Appendix}
\label{sec:appendix}

\subsection{Objectives for Multivariate Filter Feature Selectors}
\label{sec:appendix:multivariate-filter-objectives}

While Section~\ref{sec:approach:objectives:white-box} already addressed FCBF, we discuss the objective functions for CFS~\cite{hall1999correlation, hall2000correlation}, mRMR~\cite{peng2005feature}, and Relief~\cite{kira1992feature, robnik1997adaptation} here.

\paragraph{CFS}

Correlation-based Feature Selection (CFS)~\cite{hall1999correlation, hall2000correlation} considers both relevance and redundancy of features.
Relevance is the correlation between features and prediction target, similar to the univariate filter.
Redundancy is the correlation between features, acting as a normalization term.
Using a bivariate dependency measure $q$ to quantify correlation, the objective is as follows:
%
\begin{equation}
	Q_{\text{CFS}}(s,X,y) = \frac{\sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j}{\sqrt{\sum_{j=1}^{n} s_j + \sum_{j_1=1}^{n} \sum_{\substack{j_2=1 \\ j_2 \neq j_1}}^{n} q(X_{\cdot{}j_1}, X_{\cdot{}j_2}) \cdot s_{j_1} \cdot s_{j_2}}}
	\label{eq:cfs}
\end{equation}
%
This objective is non-linear in the decision variables~$s$ since it involves a fraction and multiplications between variables.
However, one can linearize the objective with the help of additional variables and constraints~\cite{nguyen2010comparison}.

\paragraph{mRMR}

Minimal Redundancy Maximum Relevance (mRMR)~\cite{peng2005feature} follows a similar principle as CFS, but uses the difference instead of the ratio between a relevance term and a redundancy term:
%
\begin{equation}
	Q_{\text{mRMR}}(s,X,y) = \frac{\sum_{j=1}^{n} q(X_{\cdot{}j},y) \cdot s_j}{\sum_{j=1}^{n} s_j} - \frac{\sum_{j_1=1}^{n} \sum_{j_2=1}^{n} q(X_{\cdot{}j_1}, X_{\cdot{}j_2}) \cdot s_{j_1} \cdot s_{j_2}}{(\sum_{j=1}^{n} s_j)^2}
	\label{eq:mrmr}
\end{equation}
%
If one knows the feature-set size, the denominators are constant and thus, Equation~\ref{eq:mrmr} is linear if one replaces the product terms with Equation~\ref{eq:product-linear}.
Without this replacement, the objective leads to a quadratic-programming problem~\cite{nguyen2014effective, rodriguez2010quadratic}, though there is a linearization as well~\cite{nguyen2009optimizing, nguyen2010comparison}.

\paragraph{Relief}

Relief~\cite{kira1992feature, robnik1997adaptation} assigns a score to each feature by sampling data objects and considering the difference in feature values compared to their nearest neighbors.
The idea is that data objects with a similar value of the prediction target should have similar feature values.
In contrast, data objects that differ in their prediction target should differ in their feature values as well.
We consider Relief to be multivariate as the nearest-neighbor computations involve all features instead of considering features independently.
However, the resulting scores for each feature can directly be put into the univariate-filter objective from Equation~\ref{eq:univariate-filter}.
Further, one can use Relief scores in CFS to consider feature redundancy~\cite{hall1999correlation, hall2000correlation}, which the default Relief does not.

\renewcommand*{\bibfont}{\small} % use a smaller font for bib than for main text
\printbibliography

\end{document}
