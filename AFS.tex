\documentclass{article}

\title{Finding Optimal Solutions for Alternative Feature Selection}
\author{Jakob Bach}

\usepackage[style=ieee, backend=bibtex]{biblatex}
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % theorems, definitions etc.
\usepackage{enumitem} % nicely formatted enumerations
\usepackage{graphicx} % plots
\usepackage{subcaption} % figures with multiple sub-figures and sub-captions
\usepackage{hyperref} % links and URLs
\addbibresource{references.bib}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
Feature-selection techniques are popular to obtain small, interpretable, yet highly accurate prediction models.
Existing feature-selection techniques typically yield either a fixed set of features or a ranking of features.
However, obtaining just one feature set might not be sufficient in some cases.
For example, users might be interested in finding different feature sets with similar prediction quality, offering alternative explanations of the data.
In this article, we formalize alternative feature selection as optimization problem first.
Next, we propose several approaches to solve these problems.
In particular, we integrate various kinds of existing feature-selection techniques.
Finally, we evaluate these approaches in a study with 100 classification datasets.
In our experiments, we find that most datasets give way to finding alternative feature sets with similar prediction quality as the optimal ones.
\end{abstract}

\textbf{Keywords:} feature selection, alternatives, constraints, explainability

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

Feature-selection techniques are ubiquitous for a variety of reasons.
By reducing the dimensionality of the dataset, they lower computational and memory requirements of prediction models.
Next, models might generalize better if irrelevant and spurious predictors are removed.
Finally, prediction models might be smaller and more comprehensive due to feature selection~\cite{li2017feature}, improving interpretability.

Traditional feature-selection techniques either select a feature set or assign a score to each feature.
One can derive a feature set from the latter by only selecting the top-ranked features.
In any case, existing techniques focus on optimizing a criterion of feature-set quality, e.g., prediction performance.
However, besides the optimal feature set, there might be multiple, differently composed feature sets with similar quality.
For the user, these other feature sets might be interesting alternatives for multiple reasons.
First, some of the originally selected features might be costly to obtain, so the user would prefer cheaper alternatives, while maintaining prediction quality.
Second, some of the originally selected features might contain sensitive information that should not be used in predictions.
Third, the user might be interested in explaining the prediction target, rather than just making good predictions.
In such a situation, knowing alternatives allows for formulating multiple hypotheses and broadens the understanding.

\paragraph{Problem statement}

This article addresses the problem of alternative feature selection, which we informally define as follows:
Given an original feature set, find a sufficiently different feature set that optimizes feature-set quality at the same time.
In particular, if the original feature set had high quality, the alternative feature set should have a similar quality.
Depending on how different the alternative feature set should be, one might have to compromise on quality.
E.g., if there are only a few highly predictive features and most of them are part of the original feature set, the alternative feature set might have significantly lower prediction quality.

Two points are essential for alternative feature selection.
First, one needs to formalize and quantify what an alternative feature sets is.
Second, one needs an approach to efficiently find alternative feature sets.
We address both these points in this article.
Also, we consider finding multiple alternatives rather than just one.

\paragraph{Related work}

In machine learning, finding alternative solutions has already been addressed extensively in clustering~\cite{bae2006coala}.
However, there is a lack of such approaches and corresponding experimental evaluation for feature selection.
The concept of statistically equivalent feature subsets~\cite{lagani2017feature} groups features such that one obtains alternatives for each individual feature.
However, there might not be such an alternative for some features.
Also, our approaches consider alternatives to feature sets as a whole.
In the field of explainable AI, counterfactual explanations have massively gained popularity in the last few years~\cite{verma2020counterfactual, stepin2021survey}.
Such explanations involve data objects with similar feature values, but a different prediction.
In contrast, we target at different feature sets with similar feature-set quality, e.g., prediction performance.

\paragraph{Contributions}

Our contribution is threefold.
First, we define an optimization problem that formalizes alternative feature selection.
In particular, we consider finding multiple alternatives sequentially and simultaneously.
Second, we propose approaches to solve this optimization problem.
To that end, we discuss how to integrate different kinds of feature-selection techniques.
Depending on the feature-selection technique, we present an exact or a heuristic solution approach.
Third, we evaluate our approaches for alternative feature selection with comprehensive experiments.
We use xxx classification dataset from PMLB\cite{olson2017pmlb, romano2021pmlb}.
For feature selection, we consider x, y, and z.
We focus on the question if one can find alternative feature sets with similar feature-set quality as the original ones.

\paragraph{Results}

We find that most datasets allow finding alternative feature sets with similar quality as the optimal ones.
This presents the user with alternative explanations for high-quality predictions.
For the different feature-selection techniques, we note \dots
Comparing sequential and simultaneous search for alternatives, we find \dots
We publish our code\footnote{\url{https://github.com/Jakob-Bach/AFS}} and our experimental data\footnote{temporary link: \url{https://bwsyncandshare.kit.edu/s/xxx}; will be moved to a public repository after review}.

\paragraph{Outline}

Section~\ref{sec:fundamentals} explains fundamentals of feature selection.
Section~\ref{sec:problem} defines the optimization problem of alternative feature selection.
Section~\ref{sec:approaches} discusses approaches to find alternative feature sets.
Section~\ref{sec:related-work} reviews related work.
Section~\ref{sec:experimental-design} describes our experimental design.
Section~\ref{sec:evaluation} presents the experimental results.
Section~\ref{sec:conclusion} concludes.

\section{Fundamentals}
\label{sec:fundamentals}

In this section, we introduce basic notation and review different methods to measure quality of feature sets.

\subsection{Notation}
\label{sec:fundamentals:notation}

Let $X \in \mathbb{R}^{m \times n}$ be a dataset represented as a matrix.
Each row is a data object, and each column is a feature.
Let $F = \{f_1, \dots, f_n\}$ denote the set of feature names.
We assume categorical features have already been made numeric, e.g., via one-hot encoding.
Further, let $y \in \mathbb{R}^m$ represent the prediction target.
The actual domain of the target might also be smaller, e.g., $\{0,1\}$ for binary classification.

With feature selection, one makes a binary decision $s_j \in \{0,1\}$ for each feature, i.e., either selects it or not.
The vector $s \in \{0,1\}^n$ combines all these selection decisions.
The selected feature set is $F_s = \{f_j \mid s_j=1\}$.
Let the function $Q(s,X,y)$ return the quality of a such feature set.
Without loss of generality, we assume this function should be maximized.

\subsection{Measuring Feature (Set) Quality}
\label{sec:fundamentals:quality}

There are different ways to evaluate feature-set quality $Q(s,X,y)$.
Note that we only give a short overview here, and focus on techniques that we use in our evaluation.
See \cite{chandrashekar2014survey,li2017feature} for comprehensive surveys of feature selection.
A typical categorization of feature selection is into filter, wrapper, and embedded methods~\cite{guyon2003introduction}.

\paragraph{Filter methods}

Filter methods evaluate feature sets without training a prediction model.
Univariate filters assess each feature on its own, often assigning a numeric score ot each feature, while multivariate filters evaluate feature sets.
Examples for univariate filters are the absolute Pearson correlation or the mutual information between a feature and the prediction target.
Such approaches ignore potential interaction between features, e.g., if they are redundant to each other.
Multivariate methods often combine a measure of feature relevance with a measure of feature redundancy.
Examples for such an approach include CFS \cite{hall1999correlation}, FCBF \cite{yu2003feature}, and mRMR \cite{peng2005feature}.
Another interesting filter method is Relief~\cite{kira1992feature}, for which multiple extensions exist.
While Relief assigns quality to individual features rather than feature sets, it still uses other features indirectly via nearest-neighbor computations between data objects.

\paragraph{Wrapper methods}

Wrapper methods~\cite{kohavi1997wrappers} employ a search strategy over feature sets and evaluate each of these feature set by training a prediction model.
Various black-box optimization techniques can serve as search strategy, e.g., genetic algorithms.

\paragraph{Embedded methods}

Embedded methods train prediction models with built-in feature selection, e.g., decision trees or random forests~\cite{breiman2001random}.

\paragraph{Post-hoc feature importance}

Apart from traditional feature selection, there are various approaches that assess feature importance after training a model.
These methods range range from local explanation methods like LIME~\cite{ribeiro2016should} or SHAP~\cite{lundberg2017unified} to global importance methods like permutation importance~\cite{breiman2001random} or SAGE~\cite{covert2020understanding}.
In particular, assessing feature importance plays a crucial role in the emerging fields of ML interpretability~\cite{carvalho2019machine}.
Mathematically, having a post-hoc importance score for each feature is not different from the output of univariate filter methods.
Thus, our approach for finding alternative feature sets works for outputs of such feature-importance methods as well.

\section{The Optimization Problem of Alternative Feature Selection}
\label{sec:problem}

In this section, we formalize alternative feature selection as an optimization problem.
As a first step, we define the structure of the problem, i.e., objective and constraints.
Second, we discuss the base case where an individual feature set is an alternative to another one.
Third, we extend this notion to multiple alternatives, considering sequential as well as simultaneous search procedures for alternatives.

\subsection{Problem Structure}

In alternative feature selection, there are two goals.
First, the quality of the alternative feature set should be high.
Second, the next alternative feature sets should be different to an existing feature set.
There are different ways to combine these two goals in an optimization problem:

First, one can consider both goals as objectives, obtaining an unconstrained multi-objective problem.
Second, one can treat feature-set quality as objective and being alternative as one or multiple constraints.
Third, one can consider being alternative as objective and introduce a constraint on feature-set quality, e.g., a lower bound.
Fourth, one can define constraints for both feature-set quality and being alternative, searching for any feasible solution instead of optimizing.
Depending on the use case, any of these four formulations might be appropriate.

In accordance with our problem statement in Section~\ref{sec:introduction}, we stick to the second formulation, i.e., optimizing feature-set quality subject to being alternative.
This option has the advantage of keeping the original objective function of feature selection.
Consequently, one does not need to specify a range or a threshold on quality.
Instead, the user can decide how alternative the feature set should be.
This yields the following optimization problem:
%
\begin{align}
	\max_s &\quad Q(s,X,y) \nonumber \\
	\text{subject to:} &\quad F_s~\text{being alternative}
\end{align}
%
In the following, we formalize \emph{being alternative}.

\subsection{Single Alternatives}

We consider a feature set to be an alternative to another feature set if it differs sufficiently.
Mathematically, we express this with a set-distance measure.
A simple and well-known set-distance measure is the Jaccard distance.
We use this distance in our article, but one could choose another measure~\cite{egghe2009new} as well.
Given two feature sets $F_1$, $F_2$, the Jaccard distance is
%
\begin{equation}
	d_{Jacc}(F_1,F_2) = 1 - \frac{|F_1 \cap F_2|}{|F_1 \cup F_2|} = 1 - \frac{|F_1 \cap F_2|}{|F_1| + |F_2| - |F_1 \cap F_2|}
	\label{eq:jaccard}
\end{equation}
%
We leverage this distance measure for the following definition:
%
\begin{definition}
	A feature set $F_2$ is an alternative to a feature set $F_1$ (and vice versa) for a distance threshold $\tau \in [0,1]$ if $d_{Jacc}(F_1,F_2) \geq \tau$.
	\label{def:single-alternative}
\end{definition}
%
Depending on the preferences of the user, different values of $\tau$ might be appropriate.
This depends on how alternative the new feature set should be.
Also, a feature set that differs strongly might also yield large changes in feature-set quality.
Thus, we leave $\tau$ as a parameter of our approach.
If the choice of $\tau$ is unclear a priori, users can try out different values and compare results.
Setting $\tau$ is made easier by the fact that the Jaccard distance is normalized to $[0,1]$.
A value of $0$ allows the alternative feature set to be identical to the original feature set.
A value of $1$ requires the two feature sets to have no overlap.
If the feature sets sizes $|F_1|$ and $|F_2|$ are known, one can also specify the maximum number of overlapping features $|F_1 \cap F_2|$ instead of $\tau$ by re-arranging Equation~\ref{eq:jaccard} and adapting Definition~\ref{def:single-alternative} accordingly:
%
\begin{align}
	& & d_{Jacc}(F_1,F_2) = 1 - \frac{|F_1 \cap F_2|}{|F_1| + |F_2| - |F_1 \cap F_2|} &\geq \tau \nonumber \\
	&\Leftrightarrow & |F_1 \cap F_2| &\leq \frac{1 - \tau}{2 - \tau} \cdot (|F_1| + |F_2|)
	\label{eq:jaccard-rearranged}
\end{align}
%
Expressing the feature-set sizes in terms of the binary feature-selection vector $s$ is straightforward as well:
%
\begin{align}
	|F_s| =& \sum_{j=1}^n s_j \nonumber \\
	|F_{s_1} \cap F_{s_2}| =& \sum_{j=1}^n s_{1,j} \cdot s_{2,j} = \sum_{j=1}^n s_{1,j} \land s_{2,j}
	\label{eq:feature-set-size}
\end{align}
%
\subsection{Multiple Alternatives}

If the user desires multiple alternative feature sets rather than just one, one can compute these alternatives either sequentially or simultaneously.

\subsubsection{Sequential Alternatives}

In the sequential case, the user gets several alternatives iteratively.
In each iteration, we determine one alternative feature set.
We constrain the new feature set to be alternative to all previously found feature sets:
%
\begin{definition}
	A feature set $F_2$ an alternative to a set of feature sets $\mathbb{F}$ (and vice versa) for a distance threshold $\tau \in [0,1]$ if $\forall F_1 \in \mathbb{F}: d_{Jacc}(F_1,F_2) \geq \tau$.
	\label{def:sequential-alternative}
\end{definition}
%
The objective function remains the same as in the base case, i.e., we optimize the quality of the new feature set~$F_2$.
As the solution space becomes narrower over iterations, feature-set quality can drop with each further alternative.
The user can decide after each iteration if feature-set quality is too low or if another alternative should be found.
Instead of Definition~\ref{def:sequential-alternative}, one could also use a less strict constraint, e.g., requiring only the average distance to all existing feature sets to pass the threshold.

\subsubsection{Simultaneous Alternatives}

In the simultaneous case, we directly obtain several alternatives.
This entails pairwise distance constraints:
%
\begin{definition}
	A set of feature sets $\mathbb{F}$ contains pairwise alternatives for a distance threshold $\tau \in [0,1]$ if $\forall F_1 \in \mathbb{F}, F_2 \in \mathbb{F}, F_1 \neq F_2: d_{Jacc}(F_1,F_2) \geq \tau$.
	\label{def:simultaneous-alternative}
\end{definition}
%
Again, one could resort to less strict constraints, e.g., based on the average distance between alternatives.
In contrast to the sequential case, we need to modify the objective function here, as we optimize multiple feature sets at once.
In this paper, we consider the average quality of all alternatives as objective.
The number of alternatives is a parameter for the user.

In contrast to the greedy procedure of the sequential approach, the simultaneous approach optimizes alternatives globally.
Thus, for the same number of alternatives, the simultaneous approach should be better in terms of average feature-set quality.
Also, we expect the qualities of the alternatives to be more evenly distributed.

\section{Approaches for Alternative Feature Selection}
\label{sec:approaches}

In this section, we introduce our approaches to find alternative feature sets.
In particular, we discuss how to solve the optimization problems from Section~\ref{sec:problem} for different kinds of feature-set quality measures from Section~\ref{sec:fundamentals:quality}.

SMT \cite{barrett2018satisfiability}

focus on alternatives of sets as a whole - not 1-of-x for individual features (which requires user to define feature groups first, is similar to statistically equivalent signatures)

\paragraph{White-box optimization}

univariate filter, importance scores, multivariate filters that can be encoded
however, note that importance scores loose information (e.g., SHAP scores rate features in context of other features, which white-box approach might simply drop, so theoretically a re-computation is necessary)

\paragraph{Black-box optimization}

wrapper
- use approaches from software engineering (e.g., fix invalid solutions in search heuristic)
- baseline: let solver enumerate all valid solutions (e.g., for fixed number of iterations)
- even with search heuristics, might use solver to start with valid solution, since many searches slightly modify existing solutions, and goal here is to find a solution that significantly differs from existing one

\paragraph{Embedding alternatives}

embedded FS
no general solution -> need to push notion of alternativeness into model training, i.e., would get a rather customized solution

\section{Related Work}
\label{sec:related-work}

COALA \cite{bae2006coala}
Alternate Clustering \cite{bailey2014alternative}
alternative features for clustering \cite{tao2012novel}
features relevant global and in clusters \cite{guan2011unified}
integer problem to find alternative clusters \cite{bae2010clustering}
penalize overlap \cite{mueller2009relevant}

GMD \cite{trittenbach2019dimension}
Edouard \cite{fouche2021efficient}
combining subspaces \cite{nguyen20134s}

subgroup set discovery \cite{leeuwen2012diverse}

statistical equivalent subsets \cite{lagani2017feature, borboudakis2021extending, tsamardinos2003towards, dougherty2006number} - 1-of-x alternatives for individual features, while we find feature sets that are alternatives as a whole

feature clustering \cite{mueller2021feature}
combining feature selection results \cite{woznica2012model}
genetic algo multi feature sets \cite{siddiqi2020genetic}

counterfactual surveys \cite{verma2020counterfactual, stepin2021survey}
MIP for counterfactuals \cite{mohammadi2021scaling}
SMT for counterfactuals \cite{karimi2020model}

constraint data mining \cite{grossi2017survey}
There is existing work on considering specific kinds of constraints in feature selection, e.g. cost constraints \cite{paclik2002feature}, group constraints \cite{yuan2006model}, or constraints from domain knowledge \cite{groves2015toward}.
These approaches are orthogonal to our work, as such constraints can be added to our optimization problem as well.

\section{Experimental Design}
\label{sec:experimental-design}

In this section, we describe our experimental design.
First, we pose the research questions.
Next, we elaborate on different components of the design.

\subsection{Research Questions}

\subsection{Approaches}

\paragraph{Feature selection}

\paragraph{Finding alternatives}

\paragraph{Prediction models}

As prediction models, we use decision trees~\cite{breiman1984classification} as well as random forests with 100 trees \cite{breiman2001random}.
Both types of models ae popular and allow to learn complex, non-linear dependencies from the data.
We leave the hyperparameters of the models at their defaults.

\subsection{Evaluation Metrics}

We conduct 10-fold cross-validation.
Not only model training, but also the search for alternative feature sets is limited to the training data.
In contrast, when we report prediction performance, we describe results on the test data only.

number of interesting alternatives: valid alternative and quality within a threshold of the original solution

\subsection{Datasets}

PMLB \cite{olson2017pmlb, romano2021pmlb}

\subsection{Implementation}

We implement our experimental pipeline in Python.
For machine learning, we use \emph{scikit-learn}~\cite{pedregosa2011scikit-learn}.
To express and solve the constraint optimization problems, we use \emph{Z3}~\cite{deMoura2008z3}.

\section{Evaluation}
\label{sec:evaluation}

\section{Conclusions and Future Work}
\label{sec:conclusion}

\subsection{Conclusions}

\subsection{Future Work}

\printbibliography

\appendix

\section{Appendix}
\label{sec:appendix}

\end{document}
