\documentclass{article}

\title{Alternative Feature Selection}
\author{Jakob Bach}

\usepackage[style=ieee, backend=bibtex]{biblatex}
\usepackage{amsmath} % mathematical symbols
\usepackage{amssymb} % mathematical symbols
\usepackage{amsthm} % theorems, definitions etc.
\usepackage{enumitem} % nicely formatted enumerations
\usepackage{graphicx} % plots
\usepackage{subcaption} % figures with multiple sub-figures and sub-captions
\usepackage{hyperref} % links and URLs
\addbibresource{references.bib}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

\maketitle

\begin{abstract}
\end{abstract}

\section{Introduction}
\label{sec:introduction}

\paragraph{Motivation}

\paragraph{Problem statement}

\paragraph{Related work}

\paragraph{Contributions}

\paragraph{Results}

We publish our code\footnote{\url{https://github.com/Jakob-Bach/AFS}} and our experimental data\footnote{temporary link: \url{https://bwsyncandshare.kit.edu/s/xxx}; will be moved to a public repository after review}.

\paragraph{Outline}

Section~\ref{sec:related-work} reviews related work.
Section~\ref{sec:fundamentals} explains fundamentals of feature selection.
Section~\ref{sec:approach} introduces our approach for alternative feature sets.
Section~\ref{sec:experimental-design} describes our experimental design.
Section~\ref{sec:evaluation} presents the experimental results.
Section~\ref{sec:conclusion} concludes.

\section{Related Work}
\label{sec:related-work}

COALA \cite{bae2006coala}
Alternate Clustering \cite{bailey2014alternative}
alternative features for clustering \cite{tao2012novel}
features relevant global and in clusters \cite{guan2011unified}
integer problem to find alternative clusters \cite{bae2010clustering}
penalize overlap \cite{mueller2009relevant}

GMD \cite{trittenbach2019dimension}
Edouard \cite{fouche2021efficient}
combining subspaces \cite{nguyen20134s}

subgroup set discovery \cite{leeuwen2012diverse}

statistical equivalent subsets \cite{lagani2017feature, borboudakis2021extending, tsamardinos2003towards, dougherty2006number}
feature clustering \cite{mueller2021feature}
combining feature selection results \cite{woznica2012model}
genetic algo multi feature sets \cite{siddiqi2020genetic}

counterfactual surveys \cite{verma2020counterfactual, stepin2021survey}
MIP for counterfactuals \cite{mohammadi2021scaling}
SMT for counterfactuals \cite{karimi2020model}

constraint data mining \cite{grossi2017survey}

\section{Fundamentals}
\label{sec:fundamentals}

In this section, we introduce basic notation and review different methods to measure quality of feature sets.

\subsection{Notation}
\label{sec:fundamentals:notation}

Let $X \in \mathbb{R}^{m \times n}$ be a dataset represented as a matrix.
Each row is a data object, and each column is a feature.
Let $F = \{f_1, \dots, f_n\}$ denote the set of feature names.
We assume categorical features have already been made numeric, e.g., via one-hot encoding.
Further, let $y \in \mathbb{R}^m$ represent the prediction target.
The actual domain of the target might also be smaller, e.g., $\{0,1\}$ for binary classification.

With feature selection, one makes a binary decision $s_j \in \{0,1\}$ for each feature, i.e., either selects it or not.
The vector $s \in \{0,1\}^n$ combines all these selection decisions.
The selected feature set is $F_s = \{f_j|~s_j=1\}$.
Let function $Q(s,X,y)$ returns the quality of a such feature set.
Without loss of generality, we assume this function should be maximized.

\subsection{Measuring Feature (Set) Quality}
\label{sec:fundamentals:quality}

There are different ways to evaluate feature-set quality $Q(s,X,y)$.
Note that we only give a short overview here, and focus on techniques that we use in our evaluation.
See \cite{chandrashekar2014survey,li2017feature} for comprehensive surveys of feature selection.
A typical categorization of feature selection is into filter, wrapper, and embedded methods~\cite{guyon2003introduction}.

\paragraph{Filter methods}

Filter methods evaluate feature sets without training a prediction model.
Univariate filters assess each feature on its own, often assigning a numeric score ot each feature, while multivariate filters evaluate feature sets.
Examples for univariate filters are the absolute Pearson correlation or the mutual information between a feature and the prediction target.
Such approaches ignore potential interaction between features, e.g., if they are redundant to each other.
Multivariate methods often combine a measure of feature relevance with a measure of feature redundancy.
Examples for such an approach include CFS \cite{hall1999correlation}, FCBF \cite{yu2003feature}, and mRMR \cite{peng2005feature}.
Another interesting filter method is Relief~\cite{kira1992feature}, for which multiple extensions exist.
While Relief assigns quality to individual features rather than feature sets, it still uses other features indirectly via nearest-neighbor computations between data objects.

\paragraph{Wrapper methods}

Wrapper methods~\cite{kohavi1997wrappers} employ a search strategy over feature sets and evaluate each of these feature set by training a prediction model.
Various black-box optimization techniques can serve as search strategy, e.g., genetic algorithms.

\paragraph{Embedded methods}

Embedded methods train prediction models with built-in feature selection, e.g., decision trees or random forests~\cite{breiman2001random}.

\paragraph{Post-hoc feature importance}

Apart from traditional feature selection, there are various approaches that assess feature importance after training a model.
These methods range range from local explanation methods like LIME~\cite{ribeiro2016should} or SHAP~\cite{lundberg2017unified} to global importance methods like permutation importance~\cite{breiman2001random} or SAGE~\cite{covert2020understanding}.
In particular, assessing feature importance plays a crucial role in the emerging fields of ML interpretability~\cite{carvalho2019machine}.
Mathematically, having a post-hoc importance score for each feature is not different from the output of univariate filter methods.
Thus, our approach for finding alternative feature sets works for outputs of such feature-importance methods as well.

\section{AFS -- Approach}
\label{sec:approach}

In this section, we introduce our approach for alternative feature selection.
First, we define how to quantify alternative feature sets.
Second, we present approaches to find such alternatives for different feature-set quality measures.

\subsection{Quantifying Alternatives}
\label{sec:approach:quantifying}

We consider a feature set to be a valid alternative to another feature set if it differs sufficiently.
Mathematically, we express this with a set-distance measure.
A simple and well-known set-distance measure is the Jaccard distance.
We use this distance in our article, but one could choose another measure~\cite{egghe2009new} as well.
Given two feature sets $F_1$, $F_2$, the Jaccard distance is
%
\begin{equation}
	d_{Jacc}(F_1,F_2) = 1 - \frac{|F_1 \cap F_2|}{|F_1 \cup F_2|} = 1 - \frac{|F_1 \cap F_2|}{|F_1| + |F_2| - |F_1 \cap F_2|}
\end{equation}
%
\begin{definition}
	A feature set $F_2$ a valid alternative to feature set $F_1$ (and vice versa) for a distance threshold $\tau \in [0,1]$ if $d_{Jacc}(F_1,F_2) \geq \tau$.
\end{definition}
%
Depending on the preferences of the user, different values of $\tau$ might be appropriate.
This depends on how alternative the new feature set should be.
Also, a feature set that differs strongly might also yield large changes in prediction performance.
Thus, we leave $\tau$ as a parameter of our approach.
If the choice of $\tau$ is unclear a priori, users can try out different values and compare results.
Setting $\tau$ is made easier by the fact that the Jaccard distance is normalized to $[0,1]$.
A value of $0$ allows the alternative feature set to be identical to the original feature set.
A value of $1$ requires the two feature sets to have no overlap.
If the feature sets sizes $|F_1|$ and $|F_2|$ are known, one can also specify the maximum number of overlapping features $|F_1 \cap F_2|$ instead of $\tau$, because one can compute $\tau$ from $|F_1 \cap F_2|$.

Computing feature-set sizes based on the binary feature-selection vector $s$ is straightforward:
%
\begin{align}
	|F_s| =& \sum_{j=1}^n s_j\\
	|F_{s_1} \cap F_{s_2}| =& \sum_{j=1}^n s_{1,j} \cdot s_{2,j} = \sum_{j=1}^n s_{1,j} \land s_{2,j}
\end{align}
%
\subsection{Finding Alternatives}
\label{sec:approach:finding}

In this section, we discuss how to find alternatives for the different types of feature-set quality measures from Section~\ref{sec:fundamentals:quality}.
Next, we describe how to obtain multiple alternatives for one feature set.

\subsubsection{Handling Different Measures for Feature (Set) Quality}

write something general about constraints

can be combined with domain constraints \cite{groves2015toward}, group constraints \cite{yuan2006model}, cost constraints \cite{paclik2002feature}
SMT \cite{barrett2018satisfiability}

focus on alternatives of sets as a whole - not 1-of-x for individual features (which requires user to define feature groups first, is similar to statistically equivalent signatures)

\paragraph{White-box optimization}

univariate filter, importance scores, multivariate filters that can be encoded
however, note that importance scores loose information (e.g., SHAP scores rate features in context of other features, which white-box approach might simply drop, so theoretically a re-computation is necessary)

\paragraph{Black-box optimization}

wrapper
- use approaches from software engineering (e.g., fix invalid solutions in search heuristic)
- baseline: let solver enumerate all valid solutions (e.g., for fixed number of iterations)
- even with search heuristics, might use solver to start with valid solution, since many searches slightly modify existing solutions, and goal here is to find a solution that significantly differs from existing one

\paragraph{Embedding alternatives}

embedded FS
no general solution -> need to push notion of alternativeness into model training, i.e., would get a rather customized solution

\subsubsection{Handling Multiple Alternatives}

If the user desires multiple alternative feature sets rather than just one, we can compute these alternatives either sequentially or simultaneously.

\paragraph{Sequential Approach}
%
\begin{definition}
	A feature set $F_2$ a valid alternative to a set of feature sets $\mathbb{F}$ (and vice versa) for a distance threshold $\tau \in [0,1]$ if $\forall F_1 \in \mathbb{F}: d_{Jacc}(F_1,F_2) \geq \tau$.
\end{definition}
%
\paragraph{Simultaneous Approach}
%
\begin{definition}
	A set of feature sets $\mathbb{F}$ contains pairwise valid alternatives for a distance threshold $\tau \in [0,1]$ if $\forall F_1 \in \mathbb{F}, F_2 \in \mathbb{F}, F_1 \neq F_2: d_{Jacc}(F_1,F_2) \geq \tau$.
\end{definition}
%
\section{Experimental Design}
\label{sec:experimental-design}

In this section, we describe our experimental design.
First, we pose the research questions.
Next, we elaborate on different components of the design.

\subsection{Research Questions}

\subsection{Approaches}

\paragraph{Feature selection}

\paragraph{Finding alternatives}

\paragraph{Prediction models}

As prediction models, we use decision trees~\cite{breiman1984classification} as well as random forests with 100 trees \cite{breiman2001random}.
Both types of models ae popular and allow to learn complex, non-linear dependencies from the data.
We leave the hyperparameters of the models at their defaults.

\subsection{Evaluation Metrics}

We conduct 10-fold cross-validation.
Not only model training, but also the search for alternative feature sets is limited to the training data.
In contrast, when we report prediction performance, we describe results on the test data only.

\subsection{Datasets}

PMLB \cite{olson2017pmlb, romano2021pmlb}

\subsection{Implementation}

We implement our experimental pipeline in Python.
For machine learning, we use \emph{scikit-learn}~\cite{pedregosa2011scikit-learn}.
To express and solve the constraint optimization problems, we use \emph{Z3}~\cite{deMoura2008z3}.

\section{Evaluation}
\label{sec:evaluation}

\section{Conclusions and Future Work}
\label{sec:conclusion}

\subsection{Conclusions}

\subsection{Future Work}

\printbibliography

\appendix

\section{Appendix}
\label{sec:appendix}

\end{document}
